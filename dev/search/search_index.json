{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"","title":"Home"},{"location":"docs-index/","text":"Welcome to Knative \u00b6 The Knative project provides a set of Kubernetes components that introduce event-driven and serverless capabilities for Kubernetes clusters. Knative APIs build on existing Kubernetes APIs, so that Knative resources are compatible with other Kubernetes-native resources, and can be managed by cluster administrators using existing Kubernetes tools. Common languages and frameworks that include Kubernetes-friendly tooling work smoothly with Knative to reduce the time spent solving common deployment issues, such as: Deploying a container Routing and managing traffic with blue/green deployment Scaling automatically and sizing workloads based on demand Binding running services to eventing ecosystems There are two core Knative components that can be installed and used together or independently to provide different functions: Knative Serving : Easily manage stateless services on Kubernetes by reducing the developer effort required for autoscaling, networking, and rollouts. Knative Eventing : Easily route events between on-cluster and off-cluster components by exposing event routing as configuration rather than embedded in code. These components are delivered as Kubernetes custom resource definitions (CRDs), which can be configured by a cluster administrator to provide default settings for developer-created applications and event workflow components. Note : Earlier versions of Knative included a build component. That component has since evolved into the separate Tekton Pipelines project. Getting started \u00b6 Installing Knative Getting started with app deployment Getting started with serving Getting started with eventing Configuration and networking \u00b6 Using a custom domain Assigning a static IP address for Knative on Google Kubernetes Engine Configuring HTTPS with a custom certificate Configuring high availability Samples and demos \u00b6 Autoscaling Binding running services to eventing ecosystems REST API sample All samples for serving All samples for eventing Debugging \u00b6 Debugging application issues","title":"Welcome to Knative"},{"location":"docs-index/#welcome-to-knative","text":"The Knative project provides a set of Kubernetes components that introduce event-driven and serverless capabilities for Kubernetes clusters. Knative APIs build on existing Kubernetes APIs, so that Knative resources are compatible with other Kubernetes-native resources, and can be managed by cluster administrators using existing Kubernetes tools. Common languages and frameworks that include Kubernetes-friendly tooling work smoothly with Knative to reduce the time spent solving common deployment issues, such as: Deploying a container Routing and managing traffic with blue/green deployment Scaling automatically and sizing workloads based on demand Binding running services to eventing ecosystems There are two core Knative components that can be installed and used together or independently to provide different functions: Knative Serving : Easily manage stateless services on Kubernetes by reducing the developer effort required for autoscaling, networking, and rollouts. Knative Eventing : Easily route events between on-cluster and off-cluster components by exposing event routing as configuration rather than embedded in code. These components are delivered as Kubernetes custom resource definitions (CRDs), which can be configured by a cluster administrator to provide default settings for developer-created applications and event workflow components. Note : Earlier versions of Knative included a build component. That component has since evolved into the separate Tekton Pipelines project.","title":"Welcome to Knative"},{"location":"docs-index/#getting-started","text":"Installing Knative Getting started with app deployment Getting started with serving Getting started with eventing","title":"Getting started"},{"location":"docs-index/#configuration-and-networking","text":"Using a custom domain Assigning a static IP address for Knative on Google Kubernetes Engine Configuring HTTPS with a custom certificate Configuring high availability","title":"Configuration and networking"},{"location":"docs-index/#samples-and-demos","text":"Autoscaling Binding running services to eventing ecosystems REST API sample All samples for serving All samples for eventing","title":"Samples and demos"},{"location":"docs-index/#debugging","text":"Debugging application issues","title":"Debugging"},{"location":"samples/","text":"Knative code samples \u00b6 Find and use Knative code samples to help you get up and running with common use cases. Code samples include content from the Knative team and community members. Browse all code samples to find other languages and use cases that might align closer with your goals. Knative owned and maintained \u00b6 View the set of Knative code samples that are actively tested and maintained: Eventing and Eventing Sources code samples Serving code samples Community owned and maintained \u00b6 View code samples that are contributed and maintained by the community . External code samples \u00b6 A list of links to Knative code samples that live outside of https://knative.dev : Image processing using Knative Eventing, Cloud Run on GKE (Knative Serving implementation) and Google Cloud Vision API A potpourri of Knative Eventing Examples Knfun - a complete Knative example of three functions using Twitter and Watson API that use kn to deploy and manage functions Knative Eventing (Cloud Events) example using spring-boot and spring-cloud-streams + Kafka Image processing pipeline using Knative Eventing on GKE, Google Cloud Vision API and ImageSharp library BigQuery processing pipeline using Knative Eventing on GKE, Cloud Scheduler, BigQuery, mathplotlib and SendGrid Please add links to your externally hosted Knative code sample.","title":"Knative code samples"},{"location":"samples/#knative-code-samples","text":"Find and use Knative code samples to help you get up and running with common use cases. Code samples include content from the Knative team and community members. Browse all code samples to find other languages and use cases that might align closer with your goals.","title":"Knative code samples"},{"location":"samples/#knative-owned-and-maintained","text":"View the set of Knative code samples that are actively tested and maintained: Eventing and Eventing Sources code samples Serving code samples","title":"Knative owned and maintained"},{"location":"samples/#community-owned-and-maintained","text":"View code samples that are contributed and maintained by the community .","title":"Community owned and maintained"},{"location":"samples/#external-code-samples","text":"A list of links to Knative code samples that live outside of https://knative.dev : Image processing using Knative Eventing, Cloud Run on GKE (Knative Serving implementation) and Google Cloud Vision API A potpourri of Knative Eventing Examples Knfun - a complete Knative example of three functions using Twitter and Watson API that use kn to deploy and manage functions Knative Eventing (Cloud Events) example using spring-boot and spring-cloud-streams + Kafka Image processing pipeline using Knative Eventing on GKE, Google Cloud Vision API and ImageSharp library BigQuery processing pipeline using Knative Eventing on GKE, Cloud Scheduler, BigQuery, mathplotlib and SendGrid Please add links to your externally hosted Knative code sample.","title":"External code samples"},{"location":"smoketest/","text":"Macro Rendering Error \u00b6 TemplateSyntaxError : unexpected '<' Traceback (most recent call last): File \"/usr/local/lib/python3.9/site-packages/mkdocs_macros/plugin.py\", line 441, in render md_template = self.env.from_string(markdown) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 941, in from_string return cls.from_code(self, self.compile(source), globals, None) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 638, in compile self.handle_exception(source=source_hint) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 832, in handle_exception reraise(*rewrite_traceback_stack(source=source)) File \"/usr/local/lib/python3.9/site-packages/jinja2/_compat.py\", line 28, in reraise raise value.with_traceback(tb) File \"<unknown>\", line 20, in template jinja2.exceptions.TemplateSyntaxError: unexpected '<'","title":"Spacer Title"},{"location":"smoketest/#macro-rendering-error","text":"TemplateSyntaxError : unexpected '<' Traceback (most recent call last): File \"/usr/local/lib/python3.9/site-packages/mkdocs_macros/plugin.py\", line 441, in render md_template = self.env.from_string(markdown) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 941, in from_string return cls.from_code(self, self.compile(source), globals, None) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 638, in compile self.handle_exception(source=source_hint) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 832, in handle_exception reraise(*rewrite_traceback_stack(source=source)) File \"/usr/local/lib/python3.9/site-packages/jinja2/_compat.py\", line 28, in reraise raise value.with_traceback(tb) File \"<unknown>\", line 20, in template jinja2.exceptions.TemplateSyntaxError: unexpected '<'","title":"Macro Rendering Error"},{"location":"client/","text":"CLI tools \u00b6 The following CLI tools are supported for use with Knative. kubectl \u00b6 You can use kubectl to apply the YAML files required to install Knative components, and also to create Knative resources, such as services and event sources using YAML. See Install and Set Up kubectl . kn \u00b6 kn provides a quick and easy interface for creating Knative resources such as services and event sources, without the need to create or modify YAML files directly. kn also simplifies completion of otherwise complex procedures such as autoscaling and traffic splitting. NOTE: kn cannot be used to install Knative components such as Serving or Eventing. See Installing kn . Connecting CLI tools to your cluster \u00b6 After you have installed kubectl or kn , these tools will search for the kubeconfig file of your cluster in the default location of $HOME/.kube/config , and will use this file to connect to the cluster. A kubeconfig file is usually automatically created when you create a Kubernetes cluster. For more information about kubeconfig files, see Organizing Cluster Access Using kubeconfig Files . Using kubeconfig files with your platform \u00b6 Instructions for using kubeconfig files are available for the following platforms: Amazon EKS Google GKE IBM IKS Red Hat OpenShift Cloud Platform Starting minikube writes this file automatically, or provides an appropriate context in an existing configuration file.","title":"CLI tools"},{"location":"client/#cli-tools","text":"The following CLI tools are supported for use with Knative.","title":"CLI tools"},{"location":"client/#kubectl","text":"You can use kubectl to apply the YAML files required to install Knative components, and also to create Knative resources, such as services and event sources using YAML. See Install and Set Up kubectl .","title":"kubectl"},{"location":"client/#kn","text":"kn provides a quick and easy interface for creating Knative resources such as services and event sources, without the need to create or modify YAML files directly. kn also simplifies completion of otherwise complex procedures such as autoscaling and traffic splitting. NOTE: kn cannot be used to install Knative components such as Serving or Eventing. See Installing kn .","title":"kn"},{"location":"client/#connecting-cli-tools-to-your-cluster","text":"After you have installed kubectl or kn , these tools will search for the kubeconfig file of your cluster in the default location of $HOME/.kube/config , and will use this file to connect to the cluster. A kubeconfig file is usually automatically created when you create a Kubernetes cluster. For more information about kubeconfig files, see Organizing Cluster Access Using kubeconfig Files .","title":"Connecting CLI tools to your cluster"},{"location":"client/#using-kubeconfig-files-with-your-platform","text":"Instructions for using kubeconfig files are available for the following platforms: Amazon EKS Google GKE IBM IKS Red Hat OpenShift Cloud Platform Starting minikube writes this file automatically, or provides an appropriate context in an existing configuration file.","title":"Using kubeconfig files with your platform"},{"location":"client/install-kn/","text":"Setting up kn \u00b6 This guide provides details about how you can set up the Knative kn CLI. Install kn using brew \u00b6 For macOS, you can install kn by using Homebrew . brew install knative/client/kn Install kn using a binary \u00b6 You can install kn by downloading the executable binary for your system and placing it in the system path. A link to the latest stable binary release is available on the kn release page . Install kn using the nightly-built binary \u00b6 Nightly-built executable binaries are available for users who want to install the latest pre-release build of kn . WARNING: Nightly-built executable binaries include features which may not be included in the latest Knative release and are not considered to be stable. Links to the latest nightly-built executable binaries are available here: macOS Linux Windows Install kn using Go \u00b6 Check out the kn client repository: git clone https://github.com/knative/client.git cd client/ Build an executable binary: hack/build.sh -f Move kn into your system path, and verify that kn commands are working properly. For example: kn version Running kn using container images \u00b6 WARNING: Nightly container images include features which may not be included in the latest Knative release and are not considered to be stable. Links to images are available here: Latest release Nightly container image You can run kn from a container image. For example: docker run --rm -v \"$HOME/.kube/config:/root/.kube/config\" gcr.io/knative-releases/knative.dev/client/cmd/kn:latest service list NOTE: Running kn from a container image does not place the binary on a permanent path. This procedure must be repeated each time you want to use kn . Using kn with Tekton \u00b6 See the Tekton documentation .","title":"Install kn"},{"location":"client/install-kn/#setting-up-kn","text":"This guide provides details about how you can set up the Knative kn CLI.","title":"Setting up kn"},{"location":"client/install-kn/#install-kn-using-brew","text":"For macOS, you can install kn by using Homebrew . brew install knative/client/kn","title":"Install kn using brew"},{"location":"client/install-kn/#install-kn-using-a-binary","text":"You can install kn by downloading the executable binary for your system and placing it in the system path. A link to the latest stable binary release is available on the kn release page .","title":"Install kn using a binary"},{"location":"client/install-kn/#install-kn-using-the-nightly-built-binary","text":"Nightly-built executable binaries are available for users who want to install the latest pre-release build of kn . WARNING: Nightly-built executable binaries include features which may not be included in the latest Knative release and are not considered to be stable. Links to the latest nightly-built executable binaries are available here: macOS Linux Windows","title":"Install kn using the nightly-built binary"},{"location":"client/install-kn/#install-kn-using-go","text":"Check out the kn client repository: git clone https://github.com/knative/client.git cd client/ Build an executable binary: hack/build.sh -f Move kn into your system path, and verify that kn commands are working properly. For example: kn version","title":"Install kn using Go"},{"location":"client/install-kn/#running-kn-using-container-images","text":"WARNING: Nightly container images include features which may not be included in the latest Knative release and are not considered to be stable. Links to images are available here: Latest release Nightly container image You can run kn from a container image. For example: docker run --rm -v \"$HOME/.kube/config:/root/.kube/config\" gcr.io/knative-releases/knative.dev/client/cmd/kn:latest service list NOTE: Running kn from a container image does not place the binary on a permanent path. This procedure must be repeated each time you want to use kn .","title":"Running kn using container images"},{"location":"client/install-kn/#using-kn-with-tekton","text":"See the Tekton documentation .","title":"Using kn with Tekton"},{"location":"eventing/","text":"Knative Eventing is a system that is designed to address a common need for cloud native development and provides composable primitives to enable late-binding event sources and event consumers. Functionality \u00b6 Knative Eventing supports multiple modes of usage. The following scenarios are well-supported by the existing components; since the system is modular, it's also possible to combine the components in novel ways. I just want to publish events, I don't care who consumes them. Send events to a Broker as an HTTP POST. Sink binding can be useful to decouple the destination configuration from your application. I just want to consume events like X, I don't care how they are published. Use a Trigger to consume events from a Broker based on CloudEvents attributes. Your application will receive the events as an HTTP POST. I want to transform events through a series of steps. Use Channels and Subscriptions to define complex message-passing topologies. For simple pipelines, the Sequence automates construction of Channels and Subscriptions between each stage. Knative also supports some additional patterns such as Parallel fanout of events, and routing response events from both Channels and Brokers. Design overview \u00b6 Knative Eventing is designed around the following goals: The Knative Eventing resources are loosely coupled. These resources can be developed and deployed independently on, and across a variety of platforms (for example Kubernetes, VMs, SaaS or FaaS). Event producers and event consumers are independent. Any producer (or source), can generate events before there are active event consumers that are listening. Any event consumer can express interest in an event or class of events, before there are producers that are creating those events. Other services can be connected to the Eventing system. These services can perform the following functions: Create new applications without modifying the event producer or event consumer. Select and target specific subsets of the events from their producers. Ensure cross-service interoperability. Knative Eventing is consistent with the CloudEvents specification that is developed by the CNCF Serverless WG . Event consumers \u00b6 To enable delivery to multiple types of Services, Knative Eventing defines two generic interfaces that can be implemented by multiple Kubernetes resources: Addressable objects are able to receive and acknowledge an event delivered over HTTP to an address defined in their status.address.url field. As a special case, the core Kubernetes Service object also fulfils the Addressable interface. Callable objects are able to receive an event delivered over HTTP and transform the event, returning 0 or 1 new events in the HTTP response. These returned events may be further processed in the same way that events from an external event source are processed. Event sources \u00b6 To learn about using event sources, see the event sources documentation. Event brokers and triggers \u00b6 Broker and Trigger objects make it easy to filter events based on event attributes. A Broker provides a bucket of events which can be selected by attribute. It receives events and forwards them to subscribers defined by one or more matching Triggers. Since a Broker implements Addressable, event senders can submit events to the Broker by POSTing the event to the Broker's status.address.url . A Trigger describes a filter on event attributes which should be delivered to an Addressable. You can create as many Triggers as necessary. For most use cases, a single bucket (Broker) per namespace is sufficient, but there are serveral use cases where multiple buckets (Brokers) can simplify architecture. For example, separate Brokers for events containing Personally Identifiable Information (PII) and non-PII events can simplify audit and access control rules. Event registry \u00b6 Knative Eventing defines an EventType object to make it easier for consumers to discover the types of events they can consume from Brokers. The registry consists of a collection of event types. The event types stored in the registry contain (all) the required information for a consumer to create a Trigger without resorting to some other out-of-band mechanism. To learn how to use the registry, see the Event Registry documentation . Event channels and subscriptions \u00b6 Knative Eventing also defines an event forwarding and persistence layer, called a Channel . Each channel is a separate Kubernetes Custom Resource . Events are delivered to Services or forwarded to other channels (possibly of a different type) using Subscriptions . This allows message delivery in a cluster to vary based on requirements, so that some events might be handled by an in-memory implementation while others would be persisted using Apache Kafka or NATS Streaming. See the List of Channel implementations . Higher Level eventing constructs \u00b6 There are cases where you may want to utilize a set of co-operating functions together and for those use cases, Knative Eventing provides two additional resources: Sequence provides a way to define an in-order list of functions. Parallel provides a way to define a list of branches for events. Future design goals \u00b6 The focus for the next Eventing release will be to enable easy implementation of event sources. Sources manage registration and delivery of events from external systems using Kubernetes Custom Resources . Learn more about Eventing development in the Eventing work group . Installation \u00b6 Follow the instructions to install on the platform of your choice . Getting Started \u00b6 Install Knative Run samples Default Channels provide a way to choose the persistence strategy for Channels across the cluster.","title":"Overview"},{"location":"eventing/#functionality","text":"Knative Eventing supports multiple modes of usage. The following scenarios are well-supported by the existing components; since the system is modular, it's also possible to combine the components in novel ways. I just want to publish events, I don't care who consumes them. Send events to a Broker as an HTTP POST. Sink binding can be useful to decouple the destination configuration from your application. I just want to consume events like X, I don't care how they are published. Use a Trigger to consume events from a Broker based on CloudEvents attributes. Your application will receive the events as an HTTP POST. I want to transform events through a series of steps. Use Channels and Subscriptions to define complex message-passing topologies. For simple pipelines, the Sequence automates construction of Channels and Subscriptions between each stage. Knative also supports some additional patterns such as Parallel fanout of events, and routing response events from both Channels and Brokers.","title":"Functionality"},{"location":"eventing/#design-overview","text":"Knative Eventing is designed around the following goals: The Knative Eventing resources are loosely coupled. These resources can be developed and deployed independently on, and across a variety of platforms (for example Kubernetes, VMs, SaaS or FaaS). Event producers and event consumers are independent. Any producer (or source), can generate events before there are active event consumers that are listening. Any event consumer can express interest in an event or class of events, before there are producers that are creating those events. Other services can be connected to the Eventing system. These services can perform the following functions: Create new applications without modifying the event producer or event consumer. Select and target specific subsets of the events from their producers. Ensure cross-service interoperability. Knative Eventing is consistent with the CloudEvents specification that is developed by the CNCF Serverless WG .","title":"Design overview"},{"location":"eventing/#event-consumers","text":"To enable delivery to multiple types of Services, Knative Eventing defines two generic interfaces that can be implemented by multiple Kubernetes resources: Addressable objects are able to receive and acknowledge an event delivered over HTTP to an address defined in their status.address.url field. As a special case, the core Kubernetes Service object also fulfils the Addressable interface. Callable objects are able to receive an event delivered over HTTP and transform the event, returning 0 or 1 new events in the HTTP response. These returned events may be further processed in the same way that events from an external event source are processed.","title":"Event consumers"},{"location":"eventing/#event-sources","text":"To learn about using event sources, see the event sources documentation.","title":"Event sources"},{"location":"eventing/#event-brokers-and-triggers","text":"Broker and Trigger objects make it easy to filter events based on event attributes. A Broker provides a bucket of events which can be selected by attribute. It receives events and forwards them to subscribers defined by one or more matching Triggers. Since a Broker implements Addressable, event senders can submit events to the Broker by POSTing the event to the Broker's status.address.url . A Trigger describes a filter on event attributes which should be delivered to an Addressable. You can create as many Triggers as necessary. For most use cases, a single bucket (Broker) per namespace is sufficient, but there are serveral use cases where multiple buckets (Brokers) can simplify architecture. For example, separate Brokers for events containing Personally Identifiable Information (PII) and non-PII events can simplify audit and access control rules.","title":"Event brokers and triggers"},{"location":"eventing/#event-registry","text":"Knative Eventing defines an EventType object to make it easier for consumers to discover the types of events they can consume from Brokers. The registry consists of a collection of event types. The event types stored in the registry contain (all) the required information for a consumer to create a Trigger without resorting to some other out-of-band mechanism. To learn how to use the registry, see the Event Registry documentation .","title":"Event registry"},{"location":"eventing/#event-channels-and-subscriptions","text":"Knative Eventing also defines an event forwarding and persistence layer, called a Channel . Each channel is a separate Kubernetes Custom Resource . Events are delivered to Services or forwarded to other channels (possibly of a different type) using Subscriptions . This allows message delivery in a cluster to vary based on requirements, so that some events might be handled by an in-memory implementation while others would be persisted using Apache Kafka or NATS Streaming. See the List of Channel implementations .","title":"Event channels and subscriptions"},{"location":"eventing/#higher-level-eventing-constructs","text":"There are cases where you may want to utilize a set of co-operating functions together and for those use cases, Knative Eventing provides two additional resources: Sequence provides a way to define an in-order list of functions. Parallel provides a way to define a list of branches for events.","title":"Higher Level eventing constructs"},{"location":"eventing/#future-design-goals","text":"The focus for the next Eventing release will be to enable easy implementation of event sources. Sources manage registration and delivery of events from external systems using Kubernetes Custom Resources . Learn more about Eventing development in the Eventing work group .","title":"Future design goals"},{"location":"eventing/#installation","text":"Follow the instructions to install on the platform of your choice .","title":"Installation"},{"location":"eventing/#getting-started","text":"Install Knative Run samples Default Channels provide a way to choose the persistence strategy for Channels across the cluster.","title":"Getting Started"},{"location":"eventing/accessing-traces/","text":"Accessing CloudEvent traces \u00b6 Depending on the request tracing tool that you have installed on your Knative Eventing cluster, see the corresponding section for details about how to visualize and trace your requests. Before you begin \u00b6 You must have a Knative cluster running with the Eventing component installed. Learn more Configuring tracing \u00b6 With the exception of importers, the Knative Eventing tracing is configured through the config-tracing ConfigMap in the knative-eventing namespace. Most importers do not use the ConfigMap and instead, use a static 1% sampling rate. You can use the config-tracing ConfigMap to configure the following Eventing components: - Brokers - Triggers - InMemoryChannel - ApiServerSource - PingSource - GitlabSource - KafkaSource - PrometheusSource Example: The following example config-tracing ConfigMap samples 10% of all CloudEvents: apiVersion : v1 kind : ConfigMap metadata : name : config-tracing namespace : knative-eventing data : enable : \"true\" zipkin-endpoint : \"http://zipkin.istio-system.svc.cluster.local:9411/api/v2/spans\" sample-rate : \"0.1\" Configuration options \u00b6 You can configure your config-tracing with following options: backend : Valid values are zipkin , stackdriver , or none . The default is none . zipkin-endpoint : Specifies the URL to the zipkin collector where you want to send the traces. Must be set if backend is set to zipkin . stackdriver-project-id : Specifies the GCP project ID into which the Stackdriver traces are written. You must specify the backend as stackdriver . If backend is unspecified, the GCP project ID is read from GCP metadata when running on GCP. sample-rate : Specifies the sampling rate. Valid values are decimals from 0 to 1 (interpreted as a float64), which indicate the probability that any given request is sampled. An example value is 0.5 , which gives each request a 50% sampling probablity. debug : Enables debugging. Valid values are true or false . Defaults to false when not specified. Set to true to enable debug mode, which forces the sample-rate to 1.0 and sends all spans to the server. Viewing your config-tracing ConfigMap \u00b6 To view your current configuration: kubectl -n knative-eventing get configmap config-tracing -oyaml Editing and deploying your config-tracing ConfigMap \u00b6 To edit and then immediately deploy changes to your ConfigMap, run the following command: kubectl -n knative-eventing edit configmap config-tracing Accessing traces in Eventing \u00b6 To access the traces, you use either the Zipkin or Jaeger tool. Details about using these tools to access traces are provided in the Knative Serving observability section: Zipkin Jaeger Example \u00b6 The following demonstrates how to trace requests in Knative Eventing with Zipkin, using the TestBrokerTracing End-to-End test. For this example, assume the following details: - Everything happens in the includes-incoming-trace-id-2qszn namespace. - The Broker is named br . - There are two Triggers that are associated with the Broker: - transformer - Filters to only allow events whose type is transformer . Sends the event to the Kubernetes Service transformer , which will reply with an identical event, except the replied event's type will be logger . - logger - Filters to only allow events whose type is logger . Sends the event to the Kubernetes Service logger . - An event is sent to the Broker with the type transformer , by the Pod named sender . Given the above, the expected path and behavior of an event is as follows: sender Pod sends the request to the Broker. Go to the Broker's ingress Pod. Go to the imc-dispatcher Channel (imc stands for InMemoryChannel). Go to both Triggers. Go to the Broker's filter Pod for the Trigger logger . The Trigger's filter ignores this event. Go to the Broker's filter Pod for the Trigger transformer . The filter does pass, so it goes to the Kubernetes Service pointed at, also named transformer . transformer Pod replies with the modified event. Go to an InMemory dispatcher. Go to the Broker's ingress Pod. Go to the InMemory dispatcher. Go to both Triggers. Go to the Broker's filter Pod for the Trigger transformer . The Trigger's filter ignores the event. Go to the Broker's filter Pod for the Trigger logger . The filter passes. Go to the logger Pod. There is no reply. This is a screenshot of the trace view in Zipkin. All the red letters have been added to the screenshot and correspond to the expectations earlier in this section: This is the same screenshot without the annotations. If you are interested, here is the raw JSON of the trace.","title":"Accessing CloudEvent Traces"},{"location":"eventing/accessing-traces/#accessing-cloudevent-traces","text":"Depending on the request tracing tool that you have installed on your Knative Eventing cluster, see the corresponding section for details about how to visualize and trace your requests.","title":"Accessing CloudEvent traces"},{"location":"eventing/accessing-traces/#before-you-begin","text":"You must have a Knative cluster running with the Eventing component installed. Learn more","title":"Before you begin"},{"location":"eventing/accessing-traces/#configuring-tracing","text":"With the exception of importers, the Knative Eventing tracing is configured through the config-tracing ConfigMap in the knative-eventing namespace. Most importers do not use the ConfigMap and instead, use a static 1% sampling rate. You can use the config-tracing ConfigMap to configure the following Eventing components: - Brokers - Triggers - InMemoryChannel - ApiServerSource - PingSource - GitlabSource - KafkaSource - PrometheusSource Example: The following example config-tracing ConfigMap samples 10% of all CloudEvents: apiVersion : v1 kind : ConfigMap metadata : name : config-tracing namespace : knative-eventing data : enable : \"true\" zipkin-endpoint : \"http://zipkin.istio-system.svc.cluster.local:9411/api/v2/spans\" sample-rate : \"0.1\"","title":"Configuring tracing"},{"location":"eventing/accessing-traces/#configuration-options","text":"You can configure your config-tracing with following options: backend : Valid values are zipkin , stackdriver , or none . The default is none . zipkin-endpoint : Specifies the URL to the zipkin collector where you want to send the traces. Must be set if backend is set to zipkin . stackdriver-project-id : Specifies the GCP project ID into which the Stackdriver traces are written. You must specify the backend as stackdriver . If backend is unspecified, the GCP project ID is read from GCP metadata when running on GCP. sample-rate : Specifies the sampling rate. Valid values are decimals from 0 to 1 (interpreted as a float64), which indicate the probability that any given request is sampled. An example value is 0.5 , which gives each request a 50% sampling probablity. debug : Enables debugging. Valid values are true or false . Defaults to false when not specified. Set to true to enable debug mode, which forces the sample-rate to 1.0 and sends all spans to the server.","title":"Configuration options"},{"location":"eventing/accessing-traces/#viewing-your-config-tracing-configmap","text":"To view your current configuration: kubectl -n knative-eventing get configmap config-tracing -oyaml","title":"Viewing your config-tracing ConfigMap"},{"location":"eventing/accessing-traces/#editing-and-deploying-your-config-tracing-configmap","text":"To edit and then immediately deploy changes to your ConfigMap, run the following command: kubectl -n knative-eventing edit configmap config-tracing","title":"Editing and deploying your config-tracing ConfigMap"},{"location":"eventing/accessing-traces/#accessing-traces-in-eventing","text":"To access the traces, you use either the Zipkin or Jaeger tool. Details about using these tools to access traces are provided in the Knative Serving observability section: Zipkin Jaeger","title":"Accessing traces in Eventing"},{"location":"eventing/accessing-traces/#example","text":"The following demonstrates how to trace requests in Knative Eventing with Zipkin, using the TestBrokerTracing End-to-End test. For this example, assume the following details: - Everything happens in the includes-incoming-trace-id-2qszn namespace. - The Broker is named br . - There are two Triggers that are associated with the Broker: - transformer - Filters to only allow events whose type is transformer . Sends the event to the Kubernetes Service transformer , which will reply with an identical event, except the replied event's type will be logger . - logger - Filters to only allow events whose type is logger . Sends the event to the Kubernetes Service logger . - An event is sent to the Broker with the type transformer , by the Pod named sender . Given the above, the expected path and behavior of an event is as follows: sender Pod sends the request to the Broker. Go to the Broker's ingress Pod. Go to the imc-dispatcher Channel (imc stands for InMemoryChannel). Go to both Triggers. Go to the Broker's filter Pod for the Trigger logger . The Trigger's filter ignores this event. Go to the Broker's filter Pod for the Trigger transformer . The filter does pass, so it goes to the Kubernetes Service pointed at, also named transformer . transformer Pod replies with the modified event. Go to an InMemory dispatcher. Go to the Broker's ingress Pod. Go to the InMemory dispatcher. Go to both Triggers. Go to the Broker's filter Pod for the Trigger transformer . The Trigger's filter ignores the event. Go to the Broker's filter Pod for the Trigger logger . The filter passes. Go to the logger Pod. There is no reply. This is a screenshot of the trace view in Zipkin. All the red letters have been added to the screenshot and correspond to the expectations earlier in this section: This is the same screenshot without the annotations. If you are interested, here is the raw JSON of the trace.","title":"Example"},{"location":"eventing/event-delivery/","text":"Event delivery \u00b6 Overview \u00b6 Knative Eventing provides various configuration parameters to control the delivery of events in case of failure. For instance, you can decide to retry sending events that failed to be consumed, and if this didn't work you can decide to forward those events to a dead letter sink. Configuring Subscription Delivery \u00b6 Knative Eventing offers fine-grained control on how events are delivered for each subscription by adding a delivery section. Consider this example: apiVersion : messaging.knative.dev/v1 kind : Subscription metadata : name : with-dead-letter-sink spec : channel : apiVersion : messaging.knative.dev/v1 kind : InMemoryChannel name : default delivery : deadLetterSink : ref : apiVersion : serving.knative.dev/v1 kind : Service name : error-handler subscriber : uri : http://doesnotexist.default.svc.cluster.local The deadLetterSink specifies where to send events that failed be consumed by subscriber . Configuring Broker Delivery \u00b6 Knative Eventing offers fine-grained control on how events are delivered for each broker by adding a delivery section. Consider this example: apiVersion : messaging.knative.dev/v1 kind : Subscription metadata : name : with-dead-letter-sink spec : channel : apiVersion : messaging.knative.dev/v1 kind : InMemoryChannel name : default delivery : retry : 5 backoffPolicy : exponential # or linear backoffDelay : \"PT0.5S\" # or ISO8601 duration deadLetterSink : ref : apiVersion : serving.knative.dev/v1 kind : Service name : error-handler subscriber : uri : http://doesnotexist.default.svc.cluster.local The Broker will retry sending events 5 times with a backoff delay of 500 milliseconds and exponential backoff policy. The deadLetterSink specifies where to send events that failed to be consumed by subscriber after the specified number of retries. Common Delivery Parameters \u00b6 The delivery value must be a Delivery Spec, which is a partial schema that is embedded in resources like Broker , Trigger and Subscription . # DeadLetterSink is the sink receiving event that could not be sent to # a destination. deadLetterSink : ref : apiVersion : v1 kind : Service name : my-service uri : /my-path # Retry is the minimum number of retries the sender should attempt when # sending an event before moving it to the dead letter sink. retry : 5 // BackoffPolicy is the retry backoff policy (linear, exponential). backoffPolicy : exponential # BackoffDelay is the delay before retrying. # More information on Duration format: # - https://www.iso.org/iso-8601-date-and-time-format.html # - https://en.wikipedia.org/wiki/ISO_8601 # # For linear policy, backoff delay is backoffDelay*<numberOfRetries>. # For exponential policy, backoff delay is backoffDelay*2^<numberOfRetries>. backoffDelay : PT2S deadLetterSink \u00b6 When present, events that failed to be consumed are sent to the deadLetterSink . In case of failure, the event is dropped and an error is logged into the system. The deadLetterSink value must be a Destination. # DeadLetterSink is the sink receiving event that could not be sent to # a destination. deadLetterSink : ref : apiVersion : v1 kind : Service name : my-service uri : /my-path Failed events may, depending on the specific Channel implementation in use, be enhanced with extension attributes prior to forwarding to the deadLetterSink . These extension attributes are as follows: knativeerrorcode Type: Int Description: The HTTP Response StatusCode from the final event dispatch attempt. Constraints: Should always be present as every HTTP Response contains a StatusCode . Examples: \"500\" ...any HTTP StatusCode... knativeerrordata Type: String Description: The HTTP Response Body from the final event dispatch attempt. Constraints: Will be empty if the HTTP Response Body was empty, and might be truncated if the length is excessive. Examples: 'Internal Server Error: Failed to process event.' '{\"key\": \"value\"}' ...any HTTP Response Body... Channel Support \u00b6 The table below summarizes what delivery parameters are supported for each channel implementation. Channel Type Supported Delivery Parameters GCP PubSub none In-Memory deadLetterSink , retry , backoffPolicy , backoffDelay Kafka deadLetterSink , retry , backoffPolicy , backoffDelay Natss none Broker Support \u00b6 The table below summarizes what delivery parameters are supported for each Broker implementation. Broker Class Supported Delivery Parameters googlecloud deadLetterSink [^1], retry , backoffPolicy , backoffDelay [^2] Kafka deadLetterSink , retry , backoffPolicy , backoffDelay MTChannelBasedBroker depends on the underlying channel RabbitMQBroker deadLetterSink , retry , backoffPolicy , backoffDelay [^1]: deadLetterSink must be a GCP Pub/Sub topic uri: deadLetterSink : uri : pubsub://dead-letter-topic Please see the [config-br-delivery](https://github.com/google/knative-gcp/blob/master/config/core/configmaps/br-delivery.yaml) ConfigMap for a complete example. [^2]: The googlecloud broker only supports the exponential backoffPolicy.","title":"Event Delivery"},{"location":"eventing/event-delivery/#event-delivery","text":"","title":"Event delivery"},{"location":"eventing/event-delivery/#overview","text":"Knative Eventing provides various configuration parameters to control the delivery of events in case of failure. For instance, you can decide to retry sending events that failed to be consumed, and if this didn't work you can decide to forward those events to a dead letter sink.","title":"Overview"},{"location":"eventing/event-delivery/#configuring-subscription-delivery","text":"Knative Eventing offers fine-grained control on how events are delivered for each subscription by adding a delivery section. Consider this example: apiVersion : messaging.knative.dev/v1 kind : Subscription metadata : name : with-dead-letter-sink spec : channel : apiVersion : messaging.knative.dev/v1 kind : InMemoryChannel name : default delivery : deadLetterSink : ref : apiVersion : serving.knative.dev/v1 kind : Service name : error-handler subscriber : uri : http://doesnotexist.default.svc.cluster.local The deadLetterSink specifies where to send events that failed be consumed by subscriber .","title":"Configuring Subscription Delivery"},{"location":"eventing/event-delivery/#configuring-broker-delivery","text":"Knative Eventing offers fine-grained control on how events are delivered for each broker by adding a delivery section. Consider this example: apiVersion : messaging.knative.dev/v1 kind : Subscription metadata : name : with-dead-letter-sink spec : channel : apiVersion : messaging.knative.dev/v1 kind : InMemoryChannel name : default delivery : retry : 5 backoffPolicy : exponential # or linear backoffDelay : \"PT0.5S\" # or ISO8601 duration deadLetterSink : ref : apiVersion : serving.knative.dev/v1 kind : Service name : error-handler subscriber : uri : http://doesnotexist.default.svc.cluster.local The Broker will retry sending events 5 times with a backoff delay of 500 milliseconds and exponential backoff policy. The deadLetterSink specifies where to send events that failed to be consumed by subscriber after the specified number of retries.","title":"Configuring Broker Delivery"},{"location":"eventing/event-delivery/#common-delivery-parameters","text":"The delivery value must be a Delivery Spec, which is a partial schema that is embedded in resources like Broker , Trigger and Subscription . # DeadLetterSink is the sink receiving event that could not be sent to # a destination. deadLetterSink : ref : apiVersion : v1 kind : Service name : my-service uri : /my-path # Retry is the minimum number of retries the sender should attempt when # sending an event before moving it to the dead letter sink. retry : 5 // BackoffPolicy is the retry backoff policy (linear, exponential). backoffPolicy : exponential # BackoffDelay is the delay before retrying. # More information on Duration format: # - https://www.iso.org/iso-8601-date-and-time-format.html # - https://en.wikipedia.org/wiki/ISO_8601 # # For linear policy, backoff delay is backoffDelay*<numberOfRetries>. # For exponential policy, backoff delay is backoffDelay*2^<numberOfRetries>. backoffDelay : PT2S","title":"Common Delivery Parameters"},{"location":"eventing/event-delivery/#deadlettersink","text":"When present, events that failed to be consumed are sent to the deadLetterSink . In case of failure, the event is dropped and an error is logged into the system. The deadLetterSink value must be a Destination. # DeadLetterSink is the sink receiving event that could not be sent to # a destination. deadLetterSink : ref : apiVersion : v1 kind : Service name : my-service uri : /my-path Failed events may, depending on the specific Channel implementation in use, be enhanced with extension attributes prior to forwarding to the deadLetterSink . These extension attributes are as follows: knativeerrorcode Type: Int Description: The HTTP Response StatusCode from the final event dispatch attempt. Constraints: Should always be present as every HTTP Response contains a StatusCode . Examples: \"500\" ...any HTTP StatusCode... knativeerrordata Type: String Description: The HTTP Response Body from the final event dispatch attempt. Constraints: Will be empty if the HTTP Response Body was empty, and might be truncated if the length is excessive. Examples: 'Internal Server Error: Failed to process event.' '{\"key\": \"value\"}' ...any HTTP Response Body...","title":"deadLetterSink"},{"location":"eventing/event-delivery/#channel-support","text":"The table below summarizes what delivery parameters are supported for each channel implementation. Channel Type Supported Delivery Parameters GCP PubSub none In-Memory deadLetterSink , retry , backoffPolicy , backoffDelay Kafka deadLetterSink , retry , backoffPolicy , backoffDelay Natss none","title":"Channel Support"},{"location":"eventing/event-delivery/#broker-support","text":"The table below summarizes what delivery parameters are supported for each Broker implementation. Broker Class Supported Delivery Parameters googlecloud deadLetterSink [^1], retry , backoffPolicy , backoffDelay [^2] Kafka deadLetterSink , retry , backoffPolicy , backoffDelay MTChannelBasedBroker depends on the underlying channel RabbitMQBroker deadLetterSink , retry , backoffPolicy , backoffDelay [^1]: deadLetterSink must be a GCP Pub/Sub topic uri: deadLetterSink : uri : pubsub://dead-letter-topic Please see the [config-br-delivery](https://github.com/google/knative-gcp/blob/master/config/core/configmaps/br-delivery.yaml) ConfigMap for a complete example. [^2]: The googlecloud broker only supports the exponential backoffPolicy.","title":"Broker Support"},{"location":"eventing/event-registry/","text":"Event registry \u00b6 Overview \u00b6 The event registry maintains a catalog of event types that can be consumed from different brokers. It introduces the EventType custom resource in order to persist the event type information in the cluster data store. Before you begin \u00b6 Read about the broker and trigger objects. Be familiar with the CloudEvents spec , particularly the Context Attributes section. Be familiar with event sources . Discovering events with the registry \u00b6 Using the registry, you can discover different types of events that can be consumed by broker event meshes. The registry is designed for use with the broker and trigger model, and aims to help you create triggers. To see event types in the registry that are available to subscribe to, enter the following command: kubectl get eventtypes -n <namespace> Below, we show an example output of executing the above command using the default namespace in a testing cluster. We will address the question of how this registry was populated in a later section. NAME TYPE SOURCE SCHEMA BROKER DESCRIPTION READY REASON dev.knative.source.github.push-34cnb dev.knative.source.github.push https://github.com/knative/eventing default True dev.knative.source.github.push-44svn dev.knative.source.github.push https://github.com/knative/serving default True dev.knative.source.github.pullrequest-86jhv dev.knative.source.github.pull_request https://github.com/knative/eventing default True dev.knative.source.github.pullrequest-97shf dev.knative.source.github.pull_request https://github.com/knative/serving default True dev.knative.kafka.event-cjvcr dev.knative.kafka.event /apis/v1/namespaces/default/kafkasources/kafka-sample#news default True dev.knative.kafka.event-tdt48 dev.knative.kafka.event /apis/v1/namespaces/default/kafkasources/kafka-sample#knative-demo default True google.pubsub.topic.publish-hrxhh google.pubsub.topic.publish //pubsub.googleapis.com/knative/topics/testing dev False BrokerIsNotReady Note This assumes that the event sources emitting the events reference a broker as their sink. There are seven different EventType objects in the registry of the default namespace. Use the following command to see an example of what the YAML for an EventType object looks like: kubectl get eventtype dev.knative.source.github.push-34cnb -o yaml Omitting irrelevant fields: apiVersion : eventing.knative.dev/v1 kind : EventType metadata : name : dev.knative.source.github.push-34cnb namespace : default labels : eventing.knative.dev/sourceName : github-sample spec : type : dev.knative.source.github.push source : https://github.com/knative/eventing schema : description : broker : default status : conditions : - status : \"True\" type : BrokerExists - status : \"True\" type : BrokerReady - status : \"True\" type : Ready From a consumer standpoint, the fields that matter the most are the spec fields as well as the status . The name is advisory (i.e., non-authoritative), and we typically generate it ( generateName ) to avoid naming collisions (e.g., two EventTypes listening to pull requests on two different Github repositories). As name nor generateName are needed for consumers to create Triggers, we defer their discussion for later on. Regarding status , its main purpose it to tell consumers (or cluster operators) whether the EventType is ready for consumption or not. That readiness is based on the Broker being ready. We can see from the example output that the PubSub EventType is not ready, as its dev Broker isn't. Let's talk in more details about the spec fields: type : is authoritative. This refers to the CloudEvent type as it enters into the event mesh. It is mandatory. Event consumers can (and in most cases would) create Triggers filtering on this attribute. source : refers to the CloudEvent source as it enters into the event mesh. It is mandatory. Event consumers can (and in most cases would) create Triggers filtering on this attribute. schema : is a valid URI with the EventType schema. It may be a JSON schema, a protobuf schema, etc. It is optional. description : is a string describing what the EventType is about. It is optional. broker refers to the Broker that can provide the EventType. It is mandatory. Subscribing to events \u00b6 Now that you know what events can be consumed from the Brokers' event meshes, you can create Triggers to subscribe to particular events. Here are a few example Triggers that subscribe to events using exact matching on type and/or source , based on the above registry output: Subscribes to GitHub pushes from any source. apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : push-trigger namespace : default spec : broker : default filter : attributes : type : dev.knative.source.github.push subscriber : ref : apiVersion : serving.knative.dev/v1 kind : Service name : push-service As per the registry output above, only two sources exist for that particular type of event ( knative's eventing and serving repositories). If later on new sources are registered for GitHub pushes, this trigger will be able to consume them. Subscribes to GitHub pull requests from knative's eventing repository. apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : gh-knative-eventing-pull-trigger namespace : default spec : broker : default filter : attributes : type : dev.knative.source.github.pull_request source : https://github.com/knative/eventing subscriber : ref : apiVersion : serving.knative.dev/v1 kind : Service name : gh-knative-eventing-pull-service Subscribes to Kafka messages sent to the knative-demo topic apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : kafka-knative-demo-trigger namespace : default spec : broker : default filter : attributes : type : dev.knative.kafka.event source : /apis/v1/namespaces/default/kafkasources/kafka-sample#knative-demo subscriber : ref : apiVersion : serving.knative.dev/v1 kind : Service name : kafka-knative-demo-service Subscribes to PubSub messages from GCP's knative project sent to the testing topic apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : gcp-pubsub-knative-testing-trigger namespace : default spec : broker : dev filter : attributes : source : //pubsub.googleapis.com/knative/topics/testing subscriber : ref : apiVersion : serving.knative.dev/v1 kind : Service name : gcp-pubsub-knative-testing-service Note that events won't be able to be consumed by this Trigger's subscriber until the Broker becomes ready. Populating the registry \u00b6 Now that we know how to discover events using the registry and how we can leverage that information to subscribe to events of interest, let's move on to the next topic: How do we actually populate the registry in the first place? Manual Registration In order to populate the registry, a cluster configurator can manually register the EventTypes. This means that the configurator can simply apply EventTypes yaml files, just as with any other Kubernetes resource: kubectl apply -f <event_type.yaml> Automatic Registration As Manual Registration might be tedious and error-prone, we also support automatic registration of EventTypes. The creation of the EventTypes is done upon instantiation of an Event Source. We currently support automatic registration of EventTypes for the following Event Sources: CronJobSource ApiServerSource GithubSource GcpPubSubSource KafkaSource AwsSqsSource Let's look at an example, in particular, the KafkaSource sample we used to populate the registry in our testing cluster. Below is what the yaml looks like. apiVersion : sources.knative.dev/v1beta1 kind : KafkaSource metadata : name : kafka-sample namespace : default spec : bootstrapServers : - my-cluster-kafka-bootstrap.kafka:9092 topics : - knative-demo - news sink : apiVersion : eventing.knative.dev/v1 kind : Broker name : default If you are interested in more information regarding configuration options of a KafkaSource, please refer to the KafKaSource sample . For this discussion, the relevant information from the yaml above are the sink and the topics . We observe that the sink is of kind Broker . We currently only support automatic creation of EventTypes for Sources instances that point to Brokers. Regarding topics , this is what we use to generate the EventTypes source field, which is equal to the CloudEvent source attribute. When you kubectl apply this yaml, the KafkaSource kafka-source-sample will be instantiated, and two EventTypes will be added to the registry (as there are two topics). You can see that in the registry example output from the previous sections. What's next \u00b6 To get started, install Knative Eventing if you haven't yet, and try experimenting with different Event Sources in your Knative cluster. Installing Knative in case you haven't already done so. Getting started with eventing in case you haven't read it. Knative code samples is a useful resource to better understand some of the Event Sources (remember to point them to a Broker if you want automatic registration of EventTypes in the registry).","title":"Event Registry"},{"location":"eventing/event-registry/#event-registry","text":"","title":"Event registry"},{"location":"eventing/event-registry/#overview","text":"The event registry maintains a catalog of event types that can be consumed from different brokers. It introduces the EventType custom resource in order to persist the event type information in the cluster data store.","title":"Overview"},{"location":"eventing/event-registry/#before-you-begin","text":"Read about the broker and trigger objects. Be familiar with the CloudEvents spec , particularly the Context Attributes section. Be familiar with event sources .","title":"Before you begin"},{"location":"eventing/event-registry/#discovering-events-with-the-registry","text":"Using the registry, you can discover different types of events that can be consumed by broker event meshes. The registry is designed for use with the broker and trigger model, and aims to help you create triggers. To see event types in the registry that are available to subscribe to, enter the following command: kubectl get eventtypes -n <namespace> Below, we show an example output of executing the above command using the default namespace in a testing cluster. We will address the question of how this registry was populated in a later section. NAME TYPE SOURCE SCHEMA BROKER DESCRIPTION READY REASON dev.knative.source.github.push-34cnb dev.knative.source.github.push https://github.com/knative/eventing default True dev.knative.source.github.push-44svn dev.knative.source.github.push https://github.com/knative/serving default True dev.knative.source.github.pullrequest-86jhv dev.knative.source.github.pull_request https://github.com/knative/eventing default True dev.knative.source.github.pullrequest-97shf dev.knative.source.github.pull_request https://github.com/knative/serving default True dev.knative.kafka.event-cjvcr dev.knative.kafka.event /apis/v1/namespaces/default/kafkasources/kafka-sample#news default True dev.knative.kafka.event-tdt48 dev.knative.kafka.event /apis/v1/namespaces/default/kafkasources/kafka-sample#knative-demo default True google.pubsub.topic.publish-hrxhh google.pubsub.topic.publish //pubsub.googleapis.com/knative/topics/testing dev False BrokerIsNotReady Note This assumes that the event sources emitting the events reference a broker as their sink. There are seven different EventType objects in the registry of the default namespace. Use the following command to see an example of what the YAML for an EventType object looks like: kubectl get eventtype dev.knative.source.github.push-34cnb -o yaml Omitting irrelevant fields: apiVersion : eventing.knative.dev/v1 kind : EventType metadata : name : dev.knative.source.github.push-34cnb namespace : default labels : eventing.knative.dev/sourceName : github-sample spec : type : dev.knative.source.github.push source : https://github.com/knative/eventing schema : description : broker : default status : conditions : - status : \"True\" type : BrokerExists - status : \"True\" type : BrokerReady - status : \"True\" type : Ready From a consumer standpoint, the fields that matter the most are the spec fields as well as the status . The name is advisory (i.e., non-authoritative), and we typically generate it ( generateName ) to avoid naming collisions (e.g., two EventTypes listening to pull requests on two different Github repositories). As name nor generateName are needed for consumers to create Triggers, we defer their discussion for later on. Regarding status , its main purpose it to tell consumers (or cluster operators) whether the EventType is ready for consumption or not. That readiness is based on the Broker being ready. We can see from the example output that the PubSub EventType is not ready, as its dev Broker isn't. Let's talk in more details about the spec fields: type : is authoritative. This refers to the CloudEvent type as it enters into the event mesh. It is mandatory. Event consumers can (and in most cases would) create Triggers filtering on this attribute. source : refers to the CloudEvent source as it enters into the event mesh. It is mandatory. Event consumers can (and in most cases would) create Triggers filtering on this attribute. schema : is a valid URI with the EventType schema. It may be a JSON schema, a protobuf schema, etc. It is optional. description : is a string describing what the EventType is about. It is optional. broker refers to the Broker that can provide the EventType. It is mandatory.","title":"Discovering events with the registry"},{"location":"eventing/event-registry/#subscribing-to-events","text":"Now that you know what events can be consumed from the Brokers' event meshes, you can create Triggers to subscribe to particular events. Here are a few example Triggers that subscribe to events using exact matching on type and/or source , based on the above registry output: Subscribes to GitHub pushes from any source. apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : push-trigger namespace : default spec : broker : default filter : attributes : type : dev.knative.source.github.push subscriber : ref : apiVersion : serving.knative.dev/v1 kind : Service name : push-service As per the registry output above, only two sources exist for that particular type of event ( knative's eventing and serving repositories). If later on new sources are registered for GitHub pushes, this trigger will be able to consume them. Subscribes to GitHub pull requests from knative's eventing repository. apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : gh-knative-eventing-pull-trigger namespace : default spec : broker : default filter : attributes : type : dev.knative.source.github.pull_request source : https://github.com/knative/eventing subscriber : ref : apiVersion : serving.knative.dev/v1 kind : Service name : gh-knative-eventing-pull-service Subscribes to Kafka messages sent to the knative-demo topic apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : kafka-knative-demo-trigger namespace : default spec : broker : default filter : attributes : type : dev.knative.kafka.event source : /apis/v1/namespaces/default/kafkasources/kafka-sample#knative-demo subscriber : ref : apiVersion : serving.knative.dev/v1 kind : Service name : kafka-knative-demo-service Subscribes to PubSub messages from GCP's knative project sent to the testing topic apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : gcp-pubsub-knative-testing-trigger namespace : default spec : broker : dev filter : attributes : source : //pubsub.googleapis.com/knative/topics/testing subscriber : ref : apiVersion : serving.knative.dev/v1 kind : Service name : gcp-pubsub-knative-testing-service Note that events won't be able to be consumed by this Trigger's subscriber until the Broker becomes ready.","title":"Subscribing to events"},{"location":"eventing/event-registry/#populating-the-registry","text":"Now that we know how to discover events using the registry and how we can leverage that information to subscribe to events of interest, let's move on to the next topic: How do we actually populate the registry in the first place? Manual Registration In order to populate the registry, a cluster configurator can manually register the EventTypes. This means that the configurator can simply apply EventTypes yaml files, just as with any other Kubernetes resource: kubectl apply -f <event_type.yaml> Automatic Registration As Manual Registration might be tedious and error-prone, we also support automatic registration of EventTypes. The creation of the EventTypes is done upon instantiation of an Event Source. We currently support automatic registration of EventTypes for the following Event Sources: CronJobSource ApiServerSource GithubSource GcpPubSubSource KafkaSource AwsSqsSource Let's look at an example, in particular, the KafkaSource sample we used to populate the registry in our testing cluster. Below is what the yaml looks like. apiVersion : sources.knative.dev/v1beta1 kind : KafkaSource metadata : name : kafka-sample namespace : default spec : bootstrapServers : - my-cluster-kafka-bootstrap.kafka:9092 topics : - knative-demo - news sink : apiVersion : eventing.knative.dev/v1 kind : Broker name : default If you are interested in more information regarding configuration options of a KafkaSource, please refer to the KafKaSource sample . For this discussion, the relevant information from the yaml above are the sink and the topics . We observe that the sink is of kind Broker . We currently only support automatic creation of EventTypes for Sources instances that point to Brokers. Regarding topics , this is what we use to generate the EventTypes source field, which is equal to the CloudEvent source attribute. When you kubectl apply this yaml, the KafkaSource kafka-source-sample will be instantiated, and two EventTypes will be added to the registry (as there are two topics). You can see that in the registry example output from the previous sections.","title":"Populating the registry"},{"location":"eventing/event-registry/#whats-next","text":"To get started, install Knative Eventing if you haven't yet, and try experimenting with different Event Sources in your Knative cluster. Installing Knative in case you haven't already done so. Getting started with eventing in case you haven't read it. Knative code samples is a useful resource to better understand some of the Event Sources (remember to point them to a Broker if you want automatic registration of EventTypes in the registry).","title":"What's next"},{"location":"eventing/broker/","text":"Knative provides a multi-tenant, channel-based broker implementation that uses channels for event routing. Before you can use the Knative Channel-based Broker, you must install a channel provider, such as InMemoryChannel, Kafka or Nats. Warning InMemoryChannel channels are for development use only and must not be used in a production deployment. For more information on which channels are available and how to install them, see the list of available channels . How it works \u00b6 When an Event is sent to the Broker, all request metadata other than the CloudEvent data and context attributes is stripped away. Unless the information existed as a CloudEvent attribute, no information is retained about how this Event entered the Broker. Once an Event has entered the Broker, it can be forwarded to event Channels by using Triggers. This event delivery mechanism hides details of event routing from the event producer and event consumer. Triggers register a subscriber's interest in a particular class of events, so that the subscriber's event sink will receive events that match the Trigger's filter. Default Broker configuration \u00b6 Knative Eventing provides a config-br-defaults ConfigMap, which lives in the knative-eventing namespace, and provides default configuration settings to enable the creation of Brokers and Channels by using defaults. For more information, see the config-br-defaults ConfigMap documentation. Create a Broker using the default settings: kubectl create -f - <<EOF apiVersion: eventing.knative.dev/v1 kind: Broker metadata: name: default namespace: default EOF Configuring broker classes \u00b6 You can configure Knative Eventing so that when you create a broker, it uses a different type of broker than the default Knative channel-based broker. To configure a different broker type, or class , you must modify the eventing.knative.dev/broker.class annotation and spec.config for the Broker object. MTChannelBasedBroker is the broker class default. Procedure \u00b6 Modify the eventing.knative.dev/broker.class annotation. Replace MTChannelBasedBroker with the class type you want to use: kind : Broker metadata : annotations : eventing.knative.dev/broker.class : MTChannelBasedBroker Configure the spec.config with the details of the ConfigMap that defines the backing channel for the broker class: kind : Broker spec : config : apiVersion : v1 kind : ConfigMap name : config-br-default-channel namespace : knative-eventing A full example combined into a fully specified resource could look like this: apiVersion : eventing.knative.dev/v1 kind : Broker metadata : annotations : eventing.knative.dev/broker.class : MTChannelBasedBroker name : default namespace : default spec : config : apiVersion : v1 kind : ConfigMap name : config-br-default-channel namespace : knative-eventing Next steps \u00b6 After you have created a Broker, you can complete the following tasks to finish setting up event delivery. Subscriber \u00b6 Create a function to receive events. This document uses a Knative Service, but it could be anything that is Callable . kubectl create -f - <<EOF apiVersion: serving.knative.dev/v1 kind: Service metadata: name: my-service namespace: default spec: template: spec: containers: - # This corresponds to # https://github.com/knative/eventing-contrib/tree/main/cmd/event_display image: gcr.io/knative-releases/knative.dev/eventing-contrib/cmd/event_display EOF Trigger \u00b6 Create a Trigger that sends only events of a particular type to the subscriber created above ( my-service ). For this example, we use Ping Source, and it emits events types dev.knative.sources.ping . kubectl create -f - <<EOF apiVersion: eventing.knative.dev/v1 kind: Trigger metadata: name: my-service-trigger namespace: default spec: broker: default filter: attributes: type: dev.knative.sources.ping subscriber: ref: apiVersion: serving.knative.dev/v1 kind: Service name: my-service EOF Emitting Events using Ping Source \u00b6 Knative Eventing comes with a Ping Source which emits an event on a configured schedule. For this we'll configure it to emit events once a minute, saying, yes, you guessed it Hello World! . kubectl create -f - <<EOF apiVersion: sources.knative.dev/v1beta2 kind: PingSource metadata: name: test-ping-source spec: schedule: \"*/1 * * * *\" contentType: \"application/json\" data: '{\"message\": \"Hello world!\"}' sink: ref: # Deliver events to Broker. apiVersion: eventing.knative.dev/v1 kind: Broker name: default EOF The following example is more complex, and demonstrates the use of deadLetterSink configuration to send failed events to Knative Service called dlq-service : apiVersion : eventing.knative.dev/v1 kind : Broker metadata : annotations : eventing.knative.dev/broker.class : MTChannelBasedBroker name : default spec : # Configuration specific to this broker. config : apiVersion : v1 kind : ConfigMap name : config-br-default-channel namespace : knative-eventing # Where to deliver Events that failed to be processed. delivery : deadLetterSink : ref : apiVersion : serving.knative.dev/v1 kind : Service name : dlq-service See also: Delivery Parameters","title":"Overview"},{"location":"eventing/broker/#how-it-works","text":"When an Event is sent to the Broker, all request metadata other than the CloudEvent data and context attributes is stripped away. Unless the information existed as a CloudEvent attribute, no information is retained about how this Event entered the Broker. Once an Event has entered the Broker, it can be forwarded to event Channels by using Triggers. This event delivery mechanism hides details of event routing from the event producer and event consumer. Triggers register a subscriber's interest in a particular class of events, so that the subscriber's event sink will receive events that match the Trigger's filter.","title":"How it works"},{"location":"eventing/broker/#default-broker-configuration","text":"Knative Eventing provides a config-br-defaults ConfigMap, which lives in the knative-eventing namespace, and provides default configuration settings to enable the creation of Brokers and Channels by using defaults. For more information, see the config-br-defaults ConfigMap documentation. Create a Broker using the default settings: kubectl create -f - <<EOF apiVersion: eventing.knative.dev/v1 kind: Broker metadata: name: default namespace: default EOF","title":"Default Broker configuration"},{"location":"eventing/broker/#configuring-broker-classes","text":"You can configure Knative Eventing so that when you create a broker, it uses a different type of broker than the default Knative channel-based broker. To configure a different broker type, or class , you must modify the eventing.knative.dev/broker.class annotation and spec.config for the Broker object. MTChannelBasedBroker is the broker class default.","title":"Configuring broker classes"},{"location":"eventing/broker/#procedure","text":"Modify the eventing.knative.dev/broker.class annotation. Replace MTChannelBasedBroker with the class type you want to use: kind : Broker metadata : annotations : eventing.knative.dev/broker.class : MTChannelBasedBroker Configure the spec.config with the details of the ConfigMap that defines the backing channel for the broker class: kind : Broker spec : config : apiVersion : v1 kind : ConfigMap name : config-br-default-channel namespace : knative-eventing A full example combined into a fully specified resource could look like this: apiVersion : eventing.knative.dev/v1 kind : Broker metadata : annotations : eventing.knative.dev/broker.class : MTChannelBasedBroker name : default namespace : default spec : config : apiVersion : v1 kind : ConfigMap name : config-br-default-channel namespace : knative-eventing","title":"Procedure"},{"location":"eventing/broker/#next-steps","text":"After you have created a Broker, you can complete the following tasks to finish setting up event delivery.","title":"Next steps"},{"location":"eventing/broker/#subscriber","text":"Create a function to receive events. This document uses a Knative Service, but it could be anything that is Callable . kubectl create -f - <<EOF apiVersion: serving.knative.dev/v1 kind: Service metadata: name: my-service namespace: default spec: template: spec: containers: - # This corresponds to # https://github.com/knative/eventing-contrib/tree/main/cmd/event_display image: gcr.io/knative-releases/knative.dev/eventing-contrib/cmd/event_display EOF","title":"Subscriber"},{"location":"eventing/broker/#trigger","text":"Create a Trigger that sends only events of a particular type to the subscriber created above ( my-service ). For this example, we use Ping Source, and it emits events types dev.knative.sources.ping . kubectl create -f - <<EOF apiVersion: eventing.knative.dev/v1 kind: Trigger metadata: name: my-service-trigger namespace: default spec: broker: default filter: attributes: type: dev.knative.sources.ping subscriber: ref: apiVersion: serving.knative.dev/v1 kind: Service name: my-service EOF","title":"Trigger"},{"location":"eventing/broker/#emitting-events-using-ping-source","text":"Knative Eventing comes with a Ping Source which emits an event on a configured schedule. For this we'll configure it to emit events once a minute, saying, yes, you guessed it Hello World! . kubectl create -f - <<EOF apiVersion: sources.knative.dev/v1beta2 kind: PingSource metadata: name: test-ping-source spec: schedule: \"*/1 * * * *\" contentType: \"application/json\" data: '{\"message\": \"Hello world!\"}' sink: ref: # Deliver events to Broker. apiVersion: eventing.knative.dev/v1 kind: Broker name: default EOF The following example is more complex, and demonstrates the use of deadLetterSink configuration to send failed events to Knative Service called dlq-service : apiVersion : eventing.knative.dev/v1 kind : Broker metadata : annotations : eventing.knative.dev/broker.class : MTChannelBasedBroker name : default spec : # Configuration specific to this broker. config : apiVersion : v1 kind : ConfigMap name : config-br-default-channel namespace : knative-eventing # Where to deliver Events that failed to be processed. delivery : deadLetterSink : ref : apiVersion : serving.knative.dev/v1 kind : Service name : dlq-service See also: Delivery Parameters","title":"Emitting Events using Ping Source"},{"location":"eventing/broker/_index/","text":"Macro Rendering Error \u00b6 TemplateSyntaxError : unexpected '%' Traceback (most recent call last): File \"/usr/local/lib/python3.9/site-packages/mkdocs_macros/plugin.py\", line 441, in render md_template = self.env.from_string(markdown) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 941, in from_string return cls.from_code(self, self.compile(source), globals, None) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 638, in compile self.handle_exception(source=source_hint) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 832, in handle_exception reraise(*rewrite_traceback_stack(source=source)) File \"/usr/local/lib/python3.9/site-packages/jinja2/_compat.py\", line 28, in reraise raise value.with_traceback(tb) File \"<unknown>\", line 3, in template jinja2.exceptions.TemplateSyntaxError: unexpected '%'","title":"Broker"},{"location":"eventing/broker/_index/#macro-rendering-error","text":"TemplateSyntaxError : unexpected '%' Traceback (most recent call last): File \"/usr/local/lib/python3.9/site-packages/mkdocs_macros/plugin.py\", line 441, in render md_template = self.env.from_string(markdown) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 941, in from_string return cls.from_code(self, self.compile(source), globals, None) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 638, in compile self.handle_exception(source=source_hint) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 832, in handle_exception reraise(*rewrite_traceback_stack(source=source)) File \"/usr/local/lib/python3.9/site-packages/jinja2/_compat.py\", line 28, in reraise raise value.with_traceback(tb) File \"<unknown>\", line 3, in template jinja2.exceptions.TemplateSyntaxError: unexpected '%'","title":"Macro Rendering Error"},{"location":"eventing/broker/config-br-defaults/","text":"Default Broker ConfigMap \u00b6 NOTE: This guide assumes Knative Eventing is installed in the knative-eventing namespace. If you have installed Knative Eventing in a different namespace, replace default with the name of that namespace. Knative Eventing provides a config-br-defaults ConfigMap, which provides default configuration settings to enable the creation of Brokers and Channels. If you are using the config-br-defaults ConfigMap default configuration, the example below will create a Broker called default in the default namespace, and uses MTChannelBasedBroker as the implementation. kubectl create -f - <<EOF apiVersion: eventing.knative.dev/v1 kind: Broker metadata: name: default namespace: default EOF The following example shows a Broker where the configuration is specified in a ConfigMap config-br-default-channel : apiVersion : eventing.knative.dev/v1 kind : Broker metadata : annotations : eventing.knative.dev/broker.class : MTChannelBasedBroker name : default spec : # Configuration specific to this broker. config : apiVersion : v1 kind : ConfigMap name : config-br-default-channel namespace : knative-eventing Format of the file \u00b6 Let's look at the ConfigMap that comes out of the box when you install a release (v0.16.0 in this example): apiVersion : v1 kind : ConfigMap metadata : name : config-br-defaults namespace : knative-eventing labels : eventing.knative.dev/release : devel data : # Configuration for defaulting channels that do not specify CRD implementations. default-br-config : | clusterDefault: brokerClass: MTChannelBasedBroker apiVersion: v1 kind: ConfigMap name: config-br-default-channel namespace: knative-eventing This means that any Broker created without a specific BrokerClass annotation will use MTChannelBasedBroker , and any Broker without a spec.config will have spec.config like so: spec: config: apiVersion: v1 kind: ConfigMap name: config-br-default-channel namespace: knative-eventing Changing the default BrokerClass \u00b6 Changing the default BrokerClass for the cluster \u00b6 If you have installed a different Broker, or multiple, you can change the default Broker used at the cluster level and by namespace. If you for example have installed MT Channel Based Broker as well as YourBroker and would prefer that by default any Broker created uses YourBroker you could modify the ConfigMap to look like this: apiVersion : v1 kind : ConfigMap metadata : name : config-br-defaults namespace : knative-eventing labels : eventing.knative.dev/release : devel data : # Configuration for defaulting channels that do not specify CRD implementations. default-br-config : | clusterDefault: brokerClass: YourBroker Now every Broker created in the cluster without the BrokerClass annotation will be using YourBroker as the Broker implementation. Note that you can always use a different implementation by explicitly specifying the BrokerClass annotation when you create a Broker. Changing the default BrokerClass for namespaces \u00b6 As mentioned, you can also control the defaulting behaviour for some set of namespaces. So, if for example, you wanted to use YourBroker for all the other Brokers created, but wanted to use MTChannelBasedBroker for the following namespaces: namespace1 and namespace2 . You would modify the config map like this: apiVersion : v1 kind : ConfigMap metadata : name : config-br-defaults namespace : knative-eventing labels : eventing.knative.dev/release : devel data : # Configuration for defaulting channels that do not specify CRD implementations. default-br-config : | clusterDefault: brokerClass: YourBroker namespaceDefaults: namespace1: brokerClass: MTChannelBasedBroker namespace2: brokerClass: MTChannelBasedBroker Changing the default configuration of the Broker \u00b6 Changing the default configuration for the cluster \u00b6 You can also control Broker configuration defaulting behaviour by specifying what gets defaulted into a broker.spec.config if left empty when being created. If you have installed a different Channel implementation (for example Kafka), and by default would like to use that for any Broker created you could change the ConfigMap to look like this: apiVersion : v1 kind : ConfigMap metadata : name : config-br-defaults namespace : knative-eventing labels : eventing.knative.dev/release : devel data : # Configuration for defaulting channels that do not specify CRD implementations. default-br-config : | clusterDefault: brokerClass: MTChannelBasedBroker apiVersion: v1 kind: ConfigMap name: config-kafka-channel namespace: knative-eventing Now every Broker created in the cluster without spec.config will be configured to use config-kafka-channel ConfigMap . Note that you can always still explicitly specify a different configuration for any given Broker by specifying it in the spec.config . Changing the default configuration for namespaces \u00b6 As mentioned, you can also control the defaulting behaviour for some set of namespaces. So, if for example, you wanted to use config-kafka-channel for all the other Brokers created, but wanted to use config-br-default-channel config the following namespaces: namespace3 and namespace4 . You would modify the ConfigMap like this: apiVersion : v1 kind : ConfigMap metadata : name : config-br-defaults namespace : knative-eventing labels : eventing.knative.dev/release : devel data : # Configuration for defaulting channels that do not specify CRD implementations. default-br-config : | clusterDefault: brokerClass: MTChannelBasedBroker apiVersion: v1 kind: ConfigMap name: config-kafka-channel namespace: knative-eventing namespaceDefaults: namespace3: apiVersion: v1 kind: ConfigMap name: config-br-default-channel namespace: knative-eventing namespace4: apiVersion: v1 kind: ConfigMap name: config-br-default-channel namespace: knative-eventing Note that we do not override the brokerClass for these namespaces. The brokerClass and config are independently configurable.","title":"Default Broker ConfigMap"},{"location":"eventing/broker/config-br-defaults/#default-broker-configmap","text":"NOTE: This guide assumes Knative Eventing is installed in the knative-eventing namespace. If you have installed Knative Eventing in a different namespace, replace default with the name of that namespace. Knative Eventing provides a config-br-defaults ConfigMap, which provides default configuration settings to enable the creation of Brokers and Channels. If you are using the config-br-defaults ConfigMap default configuration, the example below will create a Broker called default in the default namespace, and uses MTChannelBasedBroker as the implementation. kubectl create -f - <<EOF apiVersion: eventing.knative.dev/v1 kind: Broker metadata: name: default namespace: default EOF The following example shows a Broker where the configuration is specified in a ConfigMap config-br-default-channel : apiVersion : eventing.knative.dev/v1 kind : Broker metadata : annotations : eventing.knative.dev/broker.class : MTChannelBasedBroker name : default spec : # Configuration specific to this broker. config : apiVersion : v1 kind : ConfigMap name : config-br-default-channel namespace : knative-eventing","title":"Default Broker ConfigMap"},{"location":"eventing/broker/config-br-defaults/#format-of-the-file","text":"Let's look at the ConfigMap that comes out of the box when you install a release (v0.16.0 in this example): apiVersion : v1 kind : ConfigMap metadata : name : config-br-defaults namespace : knative-eventing labels : eventing.knative.dev/release : devel data : # Configuration for defaulting channels that do not specify CRD implementations. default-br-config : | clusterDefault: brokerClass: MTChannelBasedBroker apiVersion: v1 kind: ConfigMap name: config-br-default-channel namespace: knative-eventing This means that any Broker created without a specific BrokerClass annotation will use MTChannelBasedBroker , and any Broker without a spec.config will have spec.config like so: spec: config: apiVersion: v1 kind: ConfigMap name: config-br-default-channel namespace: knative-eventing","title":"Format of the file"},{"location":"eventing/broker/config-br-defaults/#changing-the-default-brokerclass","text":"","title":"Changing the default BrokerClass"},{"location":"eventing/broker/config-br-defaults/#changing-the-default-brokerclass-for-the-cluster","text":"If you have installed a different Broker, or multiple, you can change the default Broker used at the cluster level and by namespace. If you for example have installed MT Channel Based Broker as well as YourBroker and would prefer that by default any Broker created uses YourBroker you could modify the ConfigMap to look like this: apiVersion : v1 kind : ConfigMap metadata : name : config-br-defaults namespace : knative-eventing labels : eventing.knative.dev/release : devel data : # Configuration for defaulting channels that do not specify CRD implementations. default-br-config : | clusterDefault: brokerClass: YourBroker Now every Broker created in the cluster without the BrokerClass annotation will be using YourBroker as the Broker implementation. Note that you can always use a different implementation by explicitly specifying the BrokerClass annotation when you create a Broker.","title":"Changing the default BrokerClass for the cluster"},{"location":"eventing/broker/config-br-defaults/#changing-the-default-brokerclass-for-namespaces","text":"As mentioned, you can also control the defaulting behaviour for some set of namespaces. So, if for example, you wanted to use YourBroker for all the other Brokers created, but wanted to use MTChannelBasedBroker for the following namespaces: namespace1 and namespace2 . You would modify the config map like this: apiVersion : v1 kind : ConfigMap metadata : name : config-br-defaults namespace : knative-eventing labels : eventing.knative.dev/release : devel data : # Configuration for defaulting channels that do not specify CRD implementations. default-br-config : | clusterDefault: brokerClass: YourBroker namespaceDefaults: namespace1: brokerClass: MTChannelBasedBroker namespace2: brokerClass: MTChannelBasedBroker","title":"Changing the default BrokerClass for namespaces"},{"location":"eventing/broker/config-br-defaults/#changing-the-default-configuration-of-the-broker","text":"","title":"Changing the default configuration of the Broker"},{"location":"eventing/broker/config-br-defaults/#changing-the-default-configuration-for-the-cluster","text":"You can also control Broker configuration defaulting behaviour by specifying what gets defaulted into a broker.spec.config if left empty when being created. If you have installed a different Channel implementation (for example Kafka), and by default would like to use that for any Broker created you could change the ConfigMap to look like this: apiVersion : v1 kind : ConfigMap metadata : name : config-br-defaults namespace : knative-eventing labels : eventing.knative.dev/release : devel data : # Configuration for defaulting channels that do not specify CRD implementations. default-br-config : | clusterDefault: brokerClass: MTChannelBasedBroker apiVersion: v1 kind: ConfigMap name: config-kafka-channel namespace: knative-eventing Now every Broker created in the cluster without spec.config will be configured to use config-kafka-channel ConfigMap . Note that you can always still explicitly specify a different configuration for any given Broker by specifying it in the spec.config .","title":"Changing the default configuration for the cluster"},{"location":"eventing/broker/config-br-defaults/#changing-the-default-configuration-for-namespaces","text":"As mentioned, you can also control the defaulting behaviour for some set of namespaces. So, if for example, you wanted to use config-kafka-channel for all the other Brokers created, but wanted to use config-br-default-channel config the following namespaces: namespace3 and namespace4 . You would modify the ConfigMap like this: apiVersion : v1 kind : ConfigMap metadata : name : config-br-defaults namespace : knative-eventing labels : eventing.knative.dev/release : devel data : # Configuration for defaulting channels that do not specify CRD implementations. default-br-config : | clusterDefault: brokerClass: MTChannelBasedBroker apiVersion: v1 kind: ConfigMap name: config-kafka-channel namespace: knative-eventing namespaceDefaults: namespace3: apiVersion: v1 kind: ConfigMap name: config-br-default-channel namespace: knative-eventing namespace4: apiVersion: v1 kind: ConfigMap name: config-br-default-channel namespace: knative-eventing Note that we do not override the brokerClass for these namespaces. The brokerClass and config are independently configurable.","title":"Changing the default configuration for namespaces"},{"location":"eventing/broker/kafka-broker/","text":"Apache Kafka Broker \u00b6 The Apache Kafka Broker is a native Broker implementation, that reduces network hops, supports any Kafka version, and has a better integration with Apache Kafka for the Knative Broker and Trigger model. Notable features are: Control plane High Availability Horizontally scalable data plane Extensively configurable Ordered delivery of events based on CloudEvents partitioning extension Support any Kafka version, see compatibility matrix Prerequisites \u00b6 Installing Eventing using YAML files . An Apache Kafka cluster (if you're just getting started you can follow Strimzi Quickstart page ). Installation \u00b6 Install the Kafka controller by entering the following command: kubectl apply --filename http://github.com/knative-sandbox/eventing-kafka-broker/releases/download/v0.21.0/eventing-kafka-controller.yaml Install the Kafka Broker data plane by entering the following command: kubectl apply --filename http://github.com/knative-sandbox/eventing-kafka-broker/releases/download/v0.21.0/eventing-kafka-broker.yaml Verify that kafka-controller , kafka-broker-receiver and kafka-broker-dispatcher are running, by entering the following command: kubectl get deployments.apps -n knative-eventing Example output: NAME READY UP-TO-DATE AVAILABLE AGE eventing-controller 1 /1 1 1 10s eventing-webhook 1 /1 1 1 9s kafka-controller 1 /1 1 1 3s kafka-broker-dispatcher 1 /1 1 1 4s kafka-broker-receiver 1 /1 1 1 5s Create a Kafka Broker \u00b6 A Kafka Broker object looks like this: apiVersion : eventing.knative.dev/v1 kind : Broker metadata : annotations : # case-sensitive eventing.knative.dev/broker.class : Kafka name : default namespace : default spec : # Configuration specific to this broker. config : apiVersion : v1 kind : ConfigMap name : kafka-broker-config namespace : knative-eventing # Optional dead letter sink, you can specify either: # - deadLetterSink.ref, which is a reference to a Callable # - deadLetterSink.uri, which is an absolute URI to a Callable (It can potentially be out of the Kubernetes cluster) delivery : deadLetterSink : ref : apiVersion : serving.knative.dev/v1 kind : Service name : dlq-service spec.config should reference any ConfigMap that looks like the following: apiVersion : v1 kind : ConfigMap metadata : name : kafka-broker-config namespace : knative-eventing data : # Number of topic partitions default.topic.partitions : \"10\" # Replication factor of topic messages. default.topic.replication.factor : \"1\" # A comma separated list of bootstrap servers. (It can be in or out the k8s cluster) bootstrap.servers : \"my-cluster-kafka-bootstrap.kafka:9092\" The above ConfigMap is installed in the cluster. You can edit the configuration or create a new one with the same values depending on your needs. NOTE: The default.topic.replication.factor value must be less than or equal to the number of Kafka broker instances in your cluster. For example, if you only have one Kafka broker, the default.topic.replication.factor value should not be more than 1 . Set as default broker implementation \u00b6 To set the Kafka broker as the default implementation for all brokers in the Knative deployment, you can apply global settings by modifying the config-br-defaults ConfigMap in the knative-eventing namespace. This allows you to avoid configuring individual or per-namespace settings for each broker, such as metadata.annotations.eventing.knative.dev/broker.class or spec.config . The following YAML is an example of a config-br-defaults ConfigMap using Kafka broker as the default implementation. apiVersion : v1 kind : ConfigMap metadata : name : config-br-defaults namespace : knative-eventing data : default-br-config : | clusterDefault: brokerClass: Kafka apiVersion: v1 kind: ConfigMap name: kafka-broker-config namespace: knative-eventing namespaceDefaults: namespace1: brokerClass: Kafka apiVersion: v1 kind: ConfigMap name: kafka-broker-config namespace: knative-eventing namespace2: brokerClass: Kafka apiVersion: v1 kind: ConfigMap name: kafka-broker-config namespace: knative-eventing Security \u00b6 Apache Kafka supports different security features, Knative supports the followings: Authentication using SASL without encryption Authentication using SASL and encryption using SSL Authentication and encryption using SSL Encryption using SSL without client authentication To enable security features, in the ConfigMap referenced by broker.spec.config , we can reference a Secret : apiVersion : v1 kind : ConfigMap metadata : name : kafka-broker-config namespace : knative-eventing data : # Other configurations # ... # Reference a Secret called my_secret auth.secret.ref.name : my_secret The Secret my_secret must exist in the same namespace of the ConfigMap referenced by broker.spec.config , in this case: knative-eventing . Note: Certificates and keys must be in PEM format . Authentication using SASL \u00b6 Knative supports the following SASL mechanisms: PLAIN SCRAM-SHA-256 SCRAM-SHA-512 To use a specific SASL mechanism replace <sasl_mechanism> with the mechanism of your choice. Authentication using SASL without encryption \u00b6 kubectl create secret --namespace <namespace> generic <my_secret> \\ --from-literal = protocol = SASL_PLAINTEXT \\ --from-literal = sasl.mechanism = <sasl_mechanism> \\ --from-literal = user = <my_user> \\ --from-literal = password = <my_password> Authentication using SASL and encryption using SSL \u00b6 kubectl create secret --namespace <namespace> generic <my_secret> \\ --from-literal = protocol = SASL_SSL \\ --from-literal = sasl.mechanism = <sasl_mechanism> \\ --from-file = ca.crt = caroot.pem \\ --from-literal = user = <my_user> \\ --from-literal = password = <my_password> Encryption using SSL without client authentication \u00b6 kubectl create secret --namespace <namespace> generic <my_secret> \\ --from-literal = protocol = SSL \\ --from-file = ca.crt = <my_caroot.pem_file_path> \\ --from-literal = user.skip = true Authentication and encryption using SSL \u00b6 kubectl create secret --namespace <namespace> generic <my_secret> \\ --from-literal = protocol = SSL \\ --from-file = ca.crt = <my_caroot.pem_file_path> \\ --from-file = user.crt = <my_cert.pem_file_path> \\ --from-file = user.key = <my_key.pem_file_path> NOTE: ca.crt can be omitted to fallback to use system's root CA set. Kafka Producer and Consumer configurations \u00b6 Knative exposes all available Kafka producer and consumer configurations that can be modified to suit your workloads. You can change these configurations by modifying the config-kafka-broker-data-plane ConfigMap in the knative-eventing namespace. Documentation for the settings available in this ConfigMap is available on the Apache Kafka website , in particular, Producer configurations and Consumer configurations . Enable debug logging for data plane components \u00b6 The following YAML shows the default logging configuration for data plane components, that is created during the installation step: apiVersion : v1 kind : ConfigMap metadata : name : kafka-config-logging namespace : knative-eventing data : config.xml : | <configuration> <appender name=\"jsonConsoleAppender\" class=\"ch.qos.logback.core.ConsoleAppender\"> <encoder class=\"net.logstash.logback.encoder.LogstashEncoder\"/> </appender> <root level=\"INFO\"> <appender-ref ref=\"jsonConsoleAppender\"/> </root> </configuration> To change the logging level to DEBUG , you must: Apply the following kafka-config-logging ConfigMap or replace level=\"INFO\" with level=\"DEBUG\" to the ConfigMap kafka-config-logging : apiVersion : v1 kind : ConfigMap metadata : name : kafka-config-logging namespace : knative-eventing data : config.xml : | <configuration> <appender name=\"jsonConsoleAppender\" class=\"ch.qos.logback.core.ConsoleAppender\"> <encoder class=\"net.logstash.logback.encoder.LogstashEncoder\"/> </appender> <root level=\"DEBUG\"> <appender-ref ref=\"jsonConsoleAppender\"/> </root> </configuration> Restart the kafka-broker-receiver and the kafka-broker-dispatcher , by entering the following commands: kubectl rollout restart deployment -n knative-eventing kafka-broker-receiver kubectl rollout restart deployment -n knative-eventing kafka-broker-dispatcher Configuring the order of delivered events \u00b6 When dispatching events, the Kafka broker can be configured to support different delivery ordering guarantees. You can configure the delivery order of events using the kafka.eventing.knative.dev/delivery.order annotation on the Trigger object: apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : my-service-trigger annotations : kafka.eventing.knative.dev/delivery.order : ordered spec : broker : my-kafka-broker subscriber : ref : apiVersion : serving.knative.dev/v1 kind : Service name : my-service The supported consumer delivery guarantees are: unordered : Unordered consumer is a non-blocking consumer that potentially delivers messages unordered, while preserving proper offset management. ordered : Ordered consumer is a per-partition blocking consumer that delivers messages in order. unordered is the default ordering guarantee, while ordered is considered unstable, use with caution . Additional information \u00b6 To report a bug or request a feature, open an issue in the eventing-kafka-broker repository .","title":"Kafka Broker"},{"location":"eventing/broker/kafka-broker/#apache-kafka-broker","text":"The Apache Kafka Broker is a native Broker implementation, that reduces network hops, supports any Kafka version, and has a better integration with Apache Kafka for the Knative Broker and Trigger model. Notable features are: Control plane High Availability Horizontally scalable data plane Extensively configurable Ordered delivery of events based on CloudEvents partitioning extension Support any Kafka version, see compatibility matrix","title":"Apache Kafka Broker"},{"location":"eventing/broker/kafka-broker/#prerequisites","text":"Installing Eventing using YAML files . An Apache Kafka cluster (if you're just getting started you can follow Strimzi Quickstart page ).","title":"Prerequisites"},{"location":"eventing/broker/kafka-broker/#installation","text":"Install the Kafka controller by entering the following command: kubectl apply --filename http://github.com/knative-sandbox/eventing-kafka-broker/releases/download/v0.21.0/eventing-kafka-controller.yaml Install the Kafka Broker data plane by entering the following command: kubectl apply --filename http://github.com/knative-sandbox/eventing-kafka-broker/releases/download/v0.21.0/eventing-kafka-broker.yaml Verify that kafka-controller , kafka-broker-receiver and kafka-broker-dispatcher are running, by entering the following command: kubectl get deployments.apps -n knative-eventing Example output: NAME READY UP-TO-DATE AVAILABLE AGE eventing-controller 1 /1 1 1 10s eventing-webhook 1 /1 1 1 9s kafka-controller 1 /1 1 1 3s kafka-broker-dispatcher 1 /1 1 1 4s kafka-broker-receiver 1 /1 1 1 5s","title":"Installation"},{"location":"eventing/broker/kafka-broker/#create-a-kafka-broker","text":"A Kafka Broker object looks like this: apiVersion : eventing.knative.dev/v1 kind : Broker metadata : annotations : # case-sensitive eventing.knative.dev/broker.class : Kafka name : default namespace : default spec : # Configuration specific to this broker. config : apiVersion : v1 kind : ConfigMap name : kafka-broker-config namespace : knative-eventing # Optional dead letter sink, you can specify either: # - deadLetterSink.ref, which is a reference to a Callable # - deadLetterSink.uri, which is an absolute URI to a Callable (It can potentially be out of the Kubernetes cluster) delivery : deadLetterSink : ref : apiVersion : serving.knative.dev/v1 kind : Service name : dlq-service spec.config should reference any ConfigMap that looks like the following: apiVersion : v1 kind : ConfigMap metadata : name : kafka-broker-config namespace : knative-eventing data : # Number of topic partitions default.topic.partitions : \"10\" # Replication factor of topic messages. default.topic.replication.factor : \"1\" # A comma separated list of bootstrap servers. (It can be in or out the k8s cluster) bootstrap.servers : \"my-cluster-kafka-bootstrap.kafka:9092\" The above ConfigMap is installed in the cluster. You can edit the configuration or create a new one with the same values depending on your needs. NOTE: The default.topic.replication.factor value must be less than or equal to the number of Kafka broker instances in your cluster. For example, if you only have one Kafka broker, the default.topic.replication.factor value should not be more than 1 .","title":"Create a Kafka Broker"},{"location":"eventing/broker/kafka-broker/#set-as-default-broker-implementation","text":"To set the Kafka broker as the default implementation for all brokers in the Knative deployment, you can apply global settings by modifying the config-br-defaults ConfigMap in the knative-eventing namespace. This allows you to avoid configuring individual or per-namespace settings for each broker, such as metadata.annotations.eventing.knative.dev/broker.class or spec.config . The following YAML is an example of a config-br-defaults ConfigMap using Kafka broker as the default implementation. apiVersion : v1 kind : ConfigMap metadata : name : config-br-defaults namespace : knative-eventing data : default-br-config : | clusterDefault: brokerClass: Kafka apiVersion: v1 kind: ConfigMap name: kafka-broker-config namespace: knative-eventing namespaceDefaults: namespace1: brokerClass: Kafka apiVersion: v1 kind: ConfigMap name: kafka-broker-config namespace: knative-eventing namespace2: brokerClass: Kafka apiVersion: v1 kind: ConfigMap name: kafka-broker-config namespace: knative-eventing","title":"Set as default broker implementation"},{"location":"eventing/broker/kafka-broker/#security","text":"Apache Kafka supports different security features, Knative supports the followings: Authentication using SASL without encryption Authentication using SASL and encryption using SSL Authentication and encryption using SSL Encryption using SSL without client authentication To enable security features, in the ConfigMap referenced by broker.spec.config , we can reference a Secret : apiVersion : v1 kind : ConfigMap metadata : name : kafka-broker-config namespace : knative-eventing data : # Other configurations # ... # Reference a Secret called my_secret auth.secret.ref.name : my_secret The Secret my_secret must exist in the same namespace of the ConfigMap referenced by broker.spec.config , in this case: knative-eventing . Note: Certificates and keys must be in PEM format .","title":"Security"},{"location":"eventing/broker/kafka-broker/#authentication-using-sasl","text":"Knative supports the following SASL mechanisms: PLAIN SCRAM-SHA-256 SCRAM-SHA-512 To use a specific SASL mechanism replace <sasl_mechanism> with the mechanism of your choice.","title":"Authentication using SASL"},{"location":"eventing/broker/kafka-broker/#authentication-using-sasl-without-encryption","text":"kubectl create secret --namespace <namespace> generic <my_secret> \\ --from-literal = protocol = SASL_PLAINTEXT \\ --from-literal = sasl.mechanism = <sasl_mechanism> \\ --from-literal = user = <my_user> \\ --from-literal = password = <my_password>","title":"Authentication using SASL without encryption"},{"location":"eventing/broker/kafka-broker/#authentication-using-sasl-and-encryption-using-ssl","text":"kubectl create secret --namespace <namespace> generic <my_secret> \\ --from-literal = protocol = SASL_SSL \\ --from-literal = sasl.mechanism = <sasl_mechanism> \\ --from-file = ca.crt = caroot.pem \\ --from-literal = user = <my_user> \\ --from-literal = password = <my_password>","title":"Authentication using SASL and encryption using SSL"},{"location":"eventing/broker/kafka-broker/#encryption-using-ssl-without-client-authentication","text":"kubectl create secret --namespace <namespace> generic <my_secret> \\ --from-literal = protocol = SSL \\ --from-file = ca.crt = <my_caroot.pem_file_path> \\ --from-literal = user.skip = true","title":"Encryption using SSL without client authentication"},{"location":"eventing/broker/kafka-broker/#authentication-and-encryption-using-ssl","text":"kubectl create secret --namespace <namespace> generic <my_secret> \\ --from-literal = protocol = SSL \\ --from-file = ca.crt = <my_caroot.pem_file_path> \\ --from-file = user.crt = <my_cert.pem_file_path> \\ --from-file = user.key = <my_key.pem_file_path> NOTE: ca.crt can be omitted to fallback to use system's root CA set.","title":"Authentication and encryption using SSL"},{"location":"eventing/broker/kafka-broker/#kafka-producer-and-consumer-configurations","text":"Knative exposes all available Kafka producer and consumer configurations that can be modified to suit your workloads. You can change these configurations by modifying the config-kafka-broker-data-plane ConfigMap in the knative-eventing namespace. Documentation for the settings available in this ConfigMap is available on the Apache Kafka website , in particular, Producer configurations and Consumer configurations .","title":"Kafka Producer and Consumer configurations"},{"location":"eventing/broker/kafka-broker/#enable-debug-logging-for-data-plane-components","text":"The following YAML shows the default logging configuration for data plane components, that is created during the installation step: apiVersion : v1 kind : ConfigMap metadata : name : kafka-config-logging namespace : knative-eventing data : config.xml : | <configuration> <appender name=\"jsonConsoleAppender\" class=\"ch.qos.logback.core.ConsoleAppender\"> <encoder class=\"net.logstash.logback.encoder.LogstashEncoder\"/> </appender> <root level=\"INFO\"> <appender-ref ref=\"jsonConsoleAppender\"/> </root> </configuration> To change the logging level to DEBUG , you must: Apply the following kafka-config-logging ConfigMap or replace level=\"INFO\" with level=\"DEBUG\" to the ConfigMap kafka-config-logging : apiVersion : v1 kind : ConfigMap metadata : name : kafka-config-logging namespace : knative-eventing data : config.xml : | <configuration> <appender name=\"jsonConsoleAppender\" class=\"ch.qos.logback.core.ConsoleAppender\"> <encoder class=\"net.logstash.logback.encoder.LogstashEncoder\"/> </appender> <root level=\"DEBUG\"> <appender-ref ref=\"jsonConsoleAppender\"/> </root> </configuration> Restart the kafka-broker-receiver and the kafka-broker-dispatcher , by entering the following commands: kubectl rollout restart deployment -n knative-eventing kafka-broker-receiver kubectl rollout restart deployment -n knative-eventing kafka-broker-dispatcher","title":"Enable debug logging for data plane components"},{"location":"eventing/broker/kafka-broker/#configuring-the-order-of-delivered-events","text":"When dispatching events, the Kafka broker can be configured to support different delivery ordering guarantees. You can configure the delivery order of events using the kafka.eventing.knative.dev/delivery.order annotation on the Trigger object: apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : my-service-trigger annotations : kafka.eventing.knative.dev/delivery.order : ordered spec : broker : my-kafka-broker subscriber : ref : apiVersion : serving.knative.dev/v1 kind : Service name : my-service The supported consumer delivery guarantees are: unordered : Unordered consumer is a non-blocking consumer that potentially delivers messages unordered, while preserving proper offset management. ordered : Ordered consumer is a per-partition blocking consumer that delivers messages in order. unordered is the default ordering guarantee, while ordered is considered unstable, use with caution .","title":"Configuring the order of delivered events"},{"location":"eventing/broker/kafka-broker/#additional-information","text":"To report a bug or request a feature, open an issue in the eventing-kafka-broker repository .","title":"Additional information"},{"location":"eventing/broker/kafka-configmap/","text":"Kafka Channel ConfigMap \u00b6 NOTE: This guide assumes Knative Eventing is installed in the knative-eventing namespace. If you have installed Knative Eventing in a different namespace, replace knative-eventing with the name of that namespace. To use Kafka channels, you must create a YAML file that specifies how these channels will be created. NOTE: You must install the Kafka Channel first. You can copy the following sample code into your kafka-channel ConfigMap: apiVersion : v1 kind : ConfigMap metadata : name : kafka-channel namespace : knative-eventing data : channelTemplateSpec : | apiVersion: messaging.knative.dev/v1beta1 kind: KafkaChannel spec: numPartitions: 3 replicationFactor: 1 NOTE: This example specifies two extra parameters that are specific to Kafka Channels; numPartitions and replicationFactor . To create a Broker that uses the KafkaChannel, specify the kafka-channel ConfigMap: kubectl create -f - <<EOF apiVersion: eventing.knative.dev/v1 kind: Broker metadata: annotations: eventing.knative.dev/broker.class: MTChannelBasedBroker name: kafka-backed-broker namespace: default spec: config: apiVersion: v1 kind: ConfigMap name: kafka-channel namespace: knative-eventing EOF","title":"Kafka Channel ConfigMap"},{"location":"eventing/broker/kafka-configmap/#kafka-channel-configmap","text":"NOTE: This guide assumes Knative Eventing is installed in the knative-eventing namespace. If you have installed Knative Eventing in a different namespace, replace knative-eventing with the name of that namespace. To use Kafka channels, you must create a YAML file that specifies how these channels will be created. NOTE: You must install the Kafka Channel first. You can copy the following sample code into your kafka-channel ConfigMap: apiVersion : v1 kind : ConfigMap metadata : name : kafka-channel namespace : knative-eventing data : channelTemplateSpec : | apiVersion: messaging.knative.dev/v1beta1 kind: KafkaChannel spec: numPartitions: 3 replicationFactor: 1 NOTE: This example specifies two extra parameters that are specific to Kafka Channels; numPartitions and replicationFactor . To create a Broker that uses the KafkaChannel, specify the kafka-channel ConfigMap: kubectl create -f - <<EOF apiVersion: eventing.knative.dev/v1 kind: Broker metadata: annotations: eventing.knative.dev/broker.class: MTChannelBasedBroker name: kafka-backed-broker namespace: default spec: config: apiVersion: v1 kind: ConfigMap name: kafka-channel namespace: knative-eventing EOF","title":"Kafka Channel ConfigMap"},{"location":"eventing/broker/alternate/","text":"In the Knative Eventing ecosystem, alternate Broker implementations are welcome as long as they respect the Broker Conformance Spec . Below is a list of brokers provided by the community or vendors in addition to the default broker implementations provided by Knative Eventing. GCP Broker \u00b6 The GCP Broker is optimized for running in GCP. For more details, refer to the doc . Apache Kafka Broker \u00b6 For information about the Apache Kafka broker, see link .","title":"Index"},{"location":"eventing/broker/alternate/#gcp-broker","text":"The GCP Broker is optimized for running in GCP. For more details, refer to the doc .","title":"GCP Broker"},{"location":"eventing/broker/alternate/#apache-kafka-broker","text":"For information about the Apache Kafka broker, see link .","title":"Apache Kafka Broker"},{"location":"eventing/broker/alternate/_index/","text":"Macro Rendering Error \u00b6 TemplateSyntaxError : unexpected '%' Traceback (most recent call last): File \"/usr/local/lib/python3.9/site-packages/mkdocs_macros/plugin.py\", line 441, in render md_template = self.env.from_string(markdown) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 941, in from_string return cls.from_code(self, self.compile(source), globals, None) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 638, in compile self.handle_exception(source=source_hint) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 832, in handle_exception reraise(*rewrite_traceback_stack(source=source)) File \"/usr/local/lib/python3.9/site-packages/jinja2/_compat.py\", line 28, in reraise raise value.with_traceback(tb) File \"<unknown>\", line 3, in template jinja2.exceptions.TemplateSyntaxError: unexpected '%'","title":"Alternate Brokers"},{"location":"eventing/broker/alternate/_index/#macro-rendering-error","text":"TemplateSyntaxError : unexpected '%' Traceback (most recent call last): File \"/usr/local/lib/python3.9/site-packages/mkdocs_macros/plugin.py\", line 441, in render md_template = self.env.from_string(markdown) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 941, in from_string return cls.from_code(self, self.compile(source), globals, None) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 638, in compile self.handle_exception(source=source_hint) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 832, in handle_exception reraise(*rewrite_traceback_stack(source=source)) File \"/usr/local/lib/python3.9/site-packages/jinja2/_compat.py\", line 28, in reraise raise value.with_traceback(tb) File \"<unknown>\", line 3, in template jinja2.exceptions.TemplateSyntaxError: unexpected '%'","title":"Macro Rendering Error"},{"location":"eventing/channels/","text":"Channels \u00b6 Channels are Kubernetes custom resources that define a single event forwarding and persistence layer. A channel provides an event delivery mechanism that can fan-out received events, through subscriptions, to multiple destinations, or sinks. Examples of sinks include brokers and Knative services. Next steps \u00b6 Learn about default available channel types Create a channel Create a subscription","title":"Overview"},{"location":"eventing/channels/#channels","text":"Channels are Kubernetes custom resources that define a single event forwarding and persistence layer. A channel provides an event delivery mechanism that can fan-out received events, through subscriptions, to multiple destinations, or sinks. Examples of sinks include brokers and Knative services.","title":"Channels"},{"location":"eventing/channels/#next-steps","text":"Learn about default available channel types Create a channel Create a subscription","title":"Next steps"},{"location":"eventing/channels/channel-types-defaults/","text":"Channel types and defaults \u00b6 Knative provides the InMemoryChannel channel implementation by default. This default implementation is useful for developers who do not want to configure a specific implementation type, such as Apache Kafka or NATSS channels. NOTE: InMemoryChannel channels should not be used in production environments. The default channel implementation is specified in the default-ch-webhook ConfigMap in the knative-eventing namespace. For more information about modifying ConfigMaps, see Configuring the Eventing Operator custom resource . In the following example, the cluster default channel implementation is InMemoryChannel, while the namespace default channel implementation for the example-namespace is KafkaChannel. apiVersion : v1 kind : ConfigMap metadata : name : default-ch-webhook namespace : knative-eventing data : default-ch-config : | clusterDefault: apiVersion: messaging.knative.dev/v1 kind: InMemoryChannel namespaceDefaults: example-namespace: apiVersion: messaging.knative.dev/v1beta1 kind: KafkaChannel spec: numPartitions: 2 replicationFactor: 1 Default channels can be configured for the cluster, a namespace on the cluster, or both. NOTE: If a default channel implementation is configured for a namespace, this will overwrite the configuration for the cluster.","title":"Channel Types and Defaults"},{"location":"eventing/channels/channel-types-defaults/#channel-types-and-defaults","text":"Knative provides the InMemoryChannel channel implementation by default. This default implementation is useful for developers who do not want to configure a specific implementation type, such as Apache Kafka or NATSS channels. NOTE: InMemoryChannel channels should not be used in production environments. The default channel implementation is specified in the default-ch-webhook ConfigMap in the knative-eventing namespace. For more information about modifying ConfigMaps, see Configuring the Eventing Operator custom resource . In the following example, the cluster default channel implementation is InMemoryChannel, while the namespace default channel implementation for the example-namespace is KafkaChannel. apiVersion : v1 kind : ConfigMap metadata : name : default-ch-webhook namespace : knative-eventing data : default-ch-config : | clusterDefault: apiVersion: messaging.knative.dev/v1 kind: InMemoryChannel namespaceDefaults: example-namespace: apiVersion: messaging.knative.dev/v1beta1 kind: KafkaChannel spec: numPartitions: 2 replicationFactor: 1 Default channels can be configured for the cluster, a namespace on the cluster, or both. NOTE: If a default channel implementation is configured for a namespace, this will overwrite the configuration for the cluster.","title":"Channel types and defaults"},{"location":"eventing/channels/channels-crds/","text":"Available Channels \u00b6 This is a non-exhaustive list of the available Channels for Knative Eventing. Notes: Inclusion in this list is not an endorsement, nor does it imply any level of support. Name Status Support Description GCP PubSub Proof of Concept None Channels are backed by GCP PubSub . InMemoryChannel Proof of Concept None In-memory channels are a best effort Channel. They should NOT be used in Production. They are useful for development. KafkaChannel - Consolidated Proof of Concept None Channels are backed by Apache Kafka topics. The original Knative KafkaChannel implementation which utilizes a single combined Kafka Producer / Consumer deployment. KafkaChannel - Distributed Proof of Concept None Channels are backed by Apache Kafka topics. An alternate KafkaChannel implementation, contributed by SAP's Kyma project, which provides a more granular deployment of Producers / Consumers. NatssChannel Proof of Concept None Channels are backed by NATS Streaming .","title":"Available Channels"},{"location":"eventing/channels/channels-crds/#available-channels","text":"This is a non-exhaustive list of the available Channels for Knative Eventing. Notes: Inclusion in this list is not an endorsement, nor does it imply any level of support. Name Status Support Description GCP PubSub Proof of Concept None Channels are backed by GCP PubSub . InMemoryChannel Proof of Concept None In-memory channels are a best effort Channel. They should NOT be used in Production. They are useful for development. KafkaChannel - Consolidated Proof of Concept None Channels are backed by Apache Kafka topics. The original Knative KafkaChannel implementation which utilizes a single combined Kafka Producer / Consumer deployment. KafkaChannel - Distributed Proof of Concept None Channels are backed by Apache Kafka topics. An alternate KafkaChannel implementation, contributed by SAP's Kyma project, which provides a more granular deployment of Producers / Consumers. NatssChannel Proof of Concept None Channels are backed by NATS Streaming .","title":"Available Channels"},{"location":"eventing/channels/create-default-channel/","text":"Creating a channel using cluster or namespace defaults \u00b6 Developers can create channels of any supported implementation type by creating an instance of a Channel object. Create a Channel object: apiVersion : messaging.knative.dev/v1 kind : Channel metadata : name : my-channel namespace : default Since this object is created in the default namespace, according to the default ConfigMap example in the previous section, it will be an InMemoryChannel channel implementation. After the Channel object is created, a mutating admission webhook sets the spec.channelTemplate based on the default channel implementation: apiVersion : messaging.knative.dev/v1 kind : Channel metadata : name : my-channel namespace : default spec : channelTemplate : apiVersion : messaging.knative.dev/v1 kind : InMemoryChannel NOTE: The spec.channelTemplate property cannot be changed after creation, since it is set by the default channel mechanism, not the user. The channel controller creates a backing channel instance based on the spec.channelTemplate . When this mechanism is used, two objects are created, a generic Channel object, and an InMemoryChannel object. The generic object acts as a proxy for the InMemoryChannel object, by copying its subscriptions to and setting its status to that of the InMemoryChannel object. NOTE: Defaults only apply on object creation. Defaults are applied by the webhook only on Channel or Sequence creation. If the default settings change, the new defaults will only apply to newly-created channels, brokers, or sequences. Existing ones will not change.","title":"Creating Channels"},{"location":"eventing/channels/create-default-channel/#creating-a-channel-using-cluster-or-namespace-defaults","text":"Developers can create channels of any supported implementation type by creating an instance of a Channel object. Create a Channel object: apiVersion : messaging.knative.dev/v1 kind : Channel metadata : name : my-channel namespace : default Since this object is created in the default namespace, according to the default ConfigMap example in the previous section, it will be an InMemoryChannel channel implementation. After the Channel object is created, a mutating admission webhook sets the spec.channelTemplate based on the default channel implementation: apiVersion : messaging.knative.dev/v1 kind : Channel metadata : name : my-channel namespace : default spec : channelTemplate : apiVersion : messaging.knative.dev/v1 kind : InMemoryChannel NOTE: The spec.channelTemplate property cannot be changed after creation, since it is set by the default channel mechanism, not the user. The channel controller creates a backing channel instance based on the spec.channelTemplate . When this mechanism is used, two objects are created, a generic Channel object, and an InMemoryChannel object. The generic object acts as a proxy for the InMemoryChannel object, by copying its subscriptions to and setting its status to that of the InMemoryChannel object. NOTE: Defaults only apply on object creation. Defaults are applied by the webhook only on Channel or Sequence creation. If the default settings change, the new defaults will only apply to newly-created channels, brokers, or sequences. Existing ones will not change.","title":"Creating a channel using cluster or namespace defaults"},{"location":"eventing/channels/subscriptions/","text":"Macro Rendering Error \u00b6 TemplateSyntaxError : unexpected '<' Traceback (most recent call last): File \"/usr/local/lib/python3.9/site-packages/mkdocs_macros/plugin.py\", line 441, in render md_template = self.env.from_string(markdown) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 941, in from_string return cls.from_code(self, self.compile(source), globals, None) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 638, in compile self.handle_exception(source=source_hint) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 832, in handle_exception reraise(*rewrite_traceback_stack(source=source)) File \"/usr/local/lib/python3.9/site-packages/jinja2/_compat.py\", line 28, in reraise raise value.with_traceback(tb) File \"<unknown>\", line 7, in template jinja2.exceptions.TemplateSyntaxError: unexpected '<'","title":"Subscriptions"},{"location":"eventing/channels/subscriptions/#macro-rendering-error","text":"TemplateSyntaxError : unexpected '<' Traceback (most recent call last): File \"/usr/local/lib/python3.9/site-packages/mkdocs_macros/plugin.py\", line 441, in render md_template = self.env.from_string(markdown) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 941, in from_string return cls.from_code(self, self.compile(source), globals, None) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 638, in compile self.handle_exception(source=source_hint) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 832, in handle_exception reraise(*rewrite_traceback_stack(source=source)) File \"/usr/local/lib/python3.9/site-packages/jinja2/_compat.py\", line 28, in reraise raise value.with_traceback(tb) File \"<unknown>\", line 7, in template jinja2.exceptions.TemplateSyntaxError: unexpected '<'","title":"Macro Rendering Error"},{"location":"eventing/debugging/","text":"This is an evolving document on how to debug a non-working Knative Eventing setup. Audience \u00b6 This document is intended for people that are familiar with the object model of Knative Eventing . You don't need to be an expert, but do need to know roughly how things fit together. Prerequisites \u00b6 Setup Knative Eventing and an Eventing-contrib resource . Example \u00b6 This guide uses an example consisting of an event source that sends events to a function. See example.yaml for the entire YAML. For any commands in this guide to work, you must apply example.yaml : kubectl apply --filename example.yaml Triggering Events \u00b6 Knative events will occur whenever a Kubernetes Event occurs in the knative-debug namespace. We can cause this to occur with the following commands: kubectl --namespace knative-debug run to-be-deleted --image = image-that-doesnt-exist --restart = Never # 5 seconds is arbitrary. We want K8s to notice that the Pod needs to be scheduled and generate at least one event. sleep 5 kubectl --namespace knative-debug delete pod to-be-deleted Then we can see the Kubernetes Event s (note that these are not Knative events!): kubectl --namespace knative-debug get events This should produce output along the lines of: LAST SEEN FIRST SEEN COUNT NAME KIND SUBOBJECT TYPE REASON SOURCE MESSAGE 20s 20s 1 to-be-deleted.157aadb9f376fc4e Pod Normal Scheduled default-scheduler Successfully assigned knative-debug/to-be-deleted to gke-kn24-default-pool-c12ac83b-pjf2 Where are my events? \u00b6 You've applied example.yaml and you are inspecting fn 's logs: kubectl --namespace knative-debug logs -l app = fn -c user-container But you don't see any events arrive. Where is the problem? Control Plane \u00b6 We will first check the control plane, to ensure everything should be working properly. Resources \u00b6 The first thing to check are all the created resources, do their statuses contain ready true? We will attempt to determine why from the most basic pieces out: fn - The Deployment has no dependencies inside Knative. svc - The Service has no dependencies inside Knative. chan - The Channel depends on its backing channel implementation and somewhat depends on sub . src - The Source depends on chan . sub - The Subscription depends on both chan and svc . fn \u00b6 kubectl --namespace knative-debug get deployment fn -o jsonpath = '{.status.availableReplicas}' We want to see 1 . If you don't, then you need to debug the Deployment . Is there anything obviously wrong mentioned in the status ? kubectl --namespace knative-debug get deployment fn --output yaml If it is not obvious what is wrong, then you need to debug the Deployment , which is out of scope of this document. Verify that the Pod is Ready : kubectl --namespace knative-debug get pod -l app = fn -o jsonpath = '{.items[*].status.conditions[?(@.type == \"Ready\")].status}' This should return True . If it doesn't, then try to debug the Deployment using the Kubernetes Application Debugging guide. svc \u00b6 kubectl --namespace knative-debug get service svc We just want to ensure this exists and has the correct name. If it doesn't exist, then you probably need to re-apply example.yaml . Verify it points at the expected pod. svcLabels = $( kubectl --namespace knative-debug get service svc -o go-template = '{{range $k, $v := .spec.selector}}{{ $k }}={{ $v }},{{ end }}' | sed 's/.$//' ) kubectl --namespace knative-debug get pods -l $svcLabels This should return a single Pod, which if you inspect is the one generated by fn . chan \u00b6 chan uses the in-memory-channel . This is a very basic channel and has few failure modes that will be exhibited in chan 's status . kubectl --namespace knative-debug get channel.messaging.knative.dev chan -o jsonpath = '{.status.conditions[?(@.type == \"Ready\")].status}' This should return True . If it doesn't, get the full resource: kubectl --namespace knative-debug get channel.messaging.knative.dev chan --output yaml If status is completely missing, it implies that something is wrong with the in-memory-channel controller. See Channel Controller . Next verify that chan is addressable: kubectl --namespace knative-debug get channel.messaging.knative.dev chan -o jsonpath = '{.status.address.hostname}' This should return a URI, likely ending in '.cluster.local'. If it doesn't, then it implies that something went wrong during reconciliation. See Channel Controller . We will verify that the two resources that the chan creates exist and are Ready . Service \u00b6 chan creates a K8s Service . kubectl --namespace knative-debug get service -l messaging.knative.dev/role = in -memory-channel It's spec is completely unimportant, as Istio will ignore it. It just needs to exist so that src can send events to it. If it doesn't exist, it implies that something went wrong during chan reconciliation. See Channel Controller . src \u00b6 src is a ApiServerSource . First we will verify that src is writing to chan . kubectl --namespace knative-debug get apiserversource src -o jsonpath = '{.spec.sink}' Which should return map[apiVersion:messaging.knative.dev/v1 kind:Channel name:chan] . If it doesn't, then src was setup incorrectly and its spec needs to be fixed. Fixing should be as simple as updating its spec to have the correct sink (see example.yaml ). Now that we know src is sending to chan , let's verify that it is Ready . kubectl --namespace knative-debug get apiserversource src -o jsonpath = '{.status.conditions[?(.type == \"Ready\")].status}' sub \u00b6 sub is a Subscription from chan to fn . Verify that sub is Ready : kubectl --namespace knative-debug get subscription sub -o jsonpath = '{.status.conditions[?(.type == \"Ready\")].status}' This should return True . If it doesn't then, look at all the status entries. kubectl --namespace knative-debug get subscription sub --output yaml Controllers \u00b6 Each of the resources has a Controller that is watching it. As of today, they tend to do a poor job of writing failure status messages and events, so we need to look at the Controller's logs. Deployment Controller \u00b6 The Kubernetes Deployment Controller, controlling fn , is out of scope for this document. Service Controller \u00b6 The Kubernetes Service Controller, controlling svc , is out of scope for this document. Channel Controller \u00b6 There is not a single Channel Controller. Instead, there is one Controller for each Channel CRD. chan uses the InMemoryChannel Channel CRD , whose Controller is: kubectl --namespace knative-eventing get pod -l messaging.knative.dev/channel = in -memory-channel,messaging.knative.dev/role = controller --output yaml See its logs with: kubectl --namespace knative-eventing logs -l messaging.knative.dev/channel = in -memory-channel,messaging.knative.dev/role = controller Pay particular attention to any lines that have a logging level of warning or error . Source Controller \u00b6 Each Source will have its own Controller. src is a ApiServerSource , so its Controller is: kubectl --namespace knative-eventing get pod -l app = sources-controller This is actually a single binary that runs multiple Source Controllers, importantly including ApiServerSource Controller . ApiServerSource Controller \u00b6 The ApiServerSource Controller is run in the same binary as some other Source Controllers from Eventing. It is: kubectl --namespace knative-debug get pod -l eventing.knative.dev/sourceName = src,eventing.knative.dev/source = apiserver-source-controller View its logs with: kubectl --namespace knative-debug logs -l eventing.knative.dev/sourceName = src,eventing.knative.dev/source = apiserver-source-controller Pay particular attention to any lines that have a logging level of warning or error . Subscription Controller \u00b6 The Subscription Controller controls sub . It attempts to resolve the addresses that a Channel should send events to, and once resolved, inject those into the Channel 's spec.subscribable . kubectl --namespace knative-eventing get pod -l app = eventing-controller View its logs with: kubectl --namespace knative-eventing logs -l app = eventing-controller Pay particular attention to any lines that have a logging level of warning or error . Data Plane \u00b6 The entire Control Plane looks healthy, but we're still not getting any events. Now we need to investigate the data plane. The Knative event takes the following path: Event is generated by src . In this case, it is caused by having a Kubernetes Event trigger it, but as far as Knative is concerned, the Source is generating the event denovo (from nothing). src is POSTing the event to chan 's address, http://chan-kn-channel.knative-debug.svc.cluster.local . The Channel Dispatcher receives the request and introspects the Host header to determine which Channel it corresponds to. It sees that it corresponds to knative-debug/chan so forwards the request to the subscribers defined in sub , in particular svc , which is backed by fn . fn receives the request and logs it. We will investigate components in the order in which events should travel. Channel Dispatcher \u00b6 The Channel Dispatcher is the component that receives POSTs pushing events into Channel s and then POSTs to subscribers of those Channel s when an event is received. For the in-memory-channel used in this example, there is a single binary that handles both the receiving and dispatching sides for all in-memory-channel Channel s. First we will inspect the Dispatcher's logs to see if it is anything obvious: kubectl --namespace knative-eventing logs -l messaging.knative.dev/channel = in -memory-channel,messaging.knative.dev/role = dispatcher -c dispatcher Ideally we will see lines like: { \"level\" : \"info\" , \"ts\" : \"2019-08-16T13:50:55.424Z\" , \"logger\" : \"inmemorychannel-dispatcher.in-memory-channel-dispatcher\" , \"caller\" : \"provisioners/message_receiver.go:147\" , \"msg\" : \"Request mapped to channel: knative-debug/chan-kn-channel\" , \"knative.dev/controller\" : \"in-memory-channel-dispatcher\" } { \"level\" : \"info\" , \"ts\" : \"2019-08-16T13:50:55.425Z\" , \"logger\" : \"inmemorychannel-dispatcher.in-memory-channel-dispatcher\" , \"caller\" : \"provisioners/message_dispatcher.go:112\" , \"msg\" : \"Dispatching message to http://svc.knative-debug.svc.cluster.local/\" , \"knative.dev/controller\" : \"in-memory-channel-dispatcher\" } { \"level\" : \"info\" , \"ts\" : \"2019-08-16T13:50:55.981Z\" , \"logger\" : \"inmemorychannel-dispatcher.in-memory-channel-dispatcher\" , \"caller\" : \"provisioners/message_receiver.go:140\" , \"msg\" : \"Received request for chan-kn-channel.knative-debug.svc.cluster.local\" , \"knative.dev/controller\" : \"in-memory-channel-dispatcher\" } Which shows that the request is being received and then sent to svc , which is returning a 2XX response code (likely 200, 202, or 204). However if we see something like: { \"level\" : \"info\" , \"ts\" : \"2019-08-16T16:10:16.859Z\" , \"logger\" : \"inmemorychannel-dispatcher.in-memory-channel-dispatcher\" , \"caller\" : \"provisioners/message_receiver.go:140\" , \"msg\" : \"Received request for chan-kn-channel.knative-debug.svc.cluster.local\" , \"knative.dev/controller\" : \"in-memory-channel-dispatcher\" } { \"level\" : \"info\" , \"ts\" : \"2019-08-16T16:10:16.859Z\" , \"logger\" : \"inmemorychannel-dispatcher.in-memory-channel-dispatcher\" , \"caller\" : \"provisioners/message_receiver.go:147\" , \"msg\" : \"Request mapped to channel: knative-debug/chan-kn-channel\" , \"knative.dev/controller\" : \"in-memory-channel-dispatcher\" } { \"level\" : \"info\" , \"ts\" : \"2019-08-16T16:10:16.859Z\" , \"logger\" : \"inmemorychannel-dispatcher.in-memory-channel-dispatcher\" , \"caller\" : \"provisioners/message_dispatcher.go:112\" , \"msg\" : \"Dispatching message to http://svc.knative-debug.svc.cluster.local/\" , \"knative.dev/controller\" : \"in-memory-channel-dispatcher\" } { \"level\" : \"error\" , \"ts\" : \"2019-08-16T16:10:38.169Z\" , \"logger\" : \"inmemorychannel-dispatcher.in-memory-channel-dispatcher\" , \"caller\" : \"fanout/fanout_handler.go:121\" , \"msg\" : \"Fanout had an error\" , \"knative.dev/controller\" : \"in-memory-channel-dispatcher\" , \"error\" : \"Unable to complete request Post http://svc.knative-debug.svc.cluster.local/: dial tcp 10.4.44.156:80: i/o timeout\" , \"stacktrace\" : \"knative.dev/eventing/pkg/provisioners/fanout.(*Handler).dispatch\\n\\t/Users/xxxxxx/go/src/knative.dev/eventing/pkg/provisioners/fanout/fanout_handler.go:121\\nknative.dev/eventing/pkg/provisioners/fanout.createReceiverFunction.func1.1\\n\\t/Users/i512777/go/src/knative.dev/eventing/pkg/provisioners/fanout/fanout_handler.go:95\" } Then we know there was a problem posting to http://svc.knative-debug.svc.cluster.local/ . TODO Finish this section. Especially after the Channel Dispatcher emits K8s events about failures. fn \u00b6 TODO Fill in this section. TODO Finish the guide. \u00b6","title":"Debugging"},{"location":"eventing/debugging/#audience","text":"This document is intended for people that are familiar with the object model of Knative Eventing . You don't need to be an expert, but do need to know roughly how things fit together.","title":"Audience"},{"location":"eventing/debugging/#prerequisites","text":"Setup Knative Eventing and an Eventing-contrib resource .","title":"Prerequisites"},{"location":"eventing/debugging/#example","text":"This guide uses an example consisting of an event source that sends events to a function. See example.yaml for the entire YAML. For any commands in this guide to work, you must apply example.yaml : kubectl apply --filename example.yaml","title":"Example"},{"location":"eventing/debugging/#triggering-events","text":"Knative events will occur whenever a Kubernetes Event occurs in the knative-debug namespace. We can cause this to occur with the following commands: kubectl --namespace knative-debug run to-be-deleted --image = image-that-doesnt-exist --restart = Never # 5 seconds is arbitrary. We want K8s to notice that the Pod needs to be scheduled and generate at least one event. sleep 5 kubectl --namespace knative-debug delete pod to-be-deleted Then we can see the Kubernetes Event s (note that these are not Knative events!): kubectl --namespace knative-debug get events This should produce output along the lines of: LAST SEEN FIRST SEEN COUNT NAME KIND SUBOBJECT TYPE REASON SOURCE MESSAGE 20s 20s 1 to-be-deleted.157aadb9f376fc4e Pod Normal Scheduled default-scheduler Successfully assigned knative-debug/to-be-deleted to gke-kn24-default-pool-c12ac83b-pjf2","title":"Triggering Events"},{"location":"eventing/debugging/#where-are-my-events","text":"You've applied example.yaml and you are inspecting fn 's logs: kubectl --namespace knative-debug logs -l app = fn -c user-container But you don't see any events arrive. Where is the problem?","title":"Where are my events?"},{"location":"eventing/debugging/#control-plane","text":"We will first check the control plane, to ensure everything should be working properly.","title":"Control Plane"},{"location":"eventing/debugging/#resources","text":"The first thing to check are all the created resources, do their statuses contain ready true? We will attempt to determine why from the most basic pieces out: fn - The Deployment has no dependencies inside Knative. svc - The Service has no dependencies inside Knative. chan - The Channel depends on its backing channel implementation and somewhat depends on sub . src - The Source depends on chan . sub - The Subscription depends on both chan and svc .","title":"Resources"},{"location":"eventing/debugging/#fn","text":"kubectl --namespace knative-debug get deployment fn -o jsonpath = '{.status.availableReplicas}' We want to see 1 . If you don't, then you need to debug the Deployment . Is there anything obviously wrong mentioned in the status ? kubectl --namespace knative-debug get deployment fn --output yaml If it is not obvious what is wrong, then you need to debug the Deployment , which is out of scope of this document. Verify that the Pod is Ready : kubectl --namespace knative-debug get pod -l app = fn -o jsonpath = '{.items[*].status.conditions[?(@.type == \"Ready\")].status}' This should return True . If it doesn't, then try to debug the Deployment using the Kubernetes Application Debugging guide.","title":"fn"},{"location":"eventing/debugging/#svc","text":"kubectl --namespace knative-debug get service svc We just want to ensure this exists and has the correct name. If it doesn't exist, then you probably need to re-apply example.yaml . Verify it points at the expected pod. svcLabels = $( kubectl --namespace knative-debug get service svc -o go-template = '{{range $k, $v := .spec.selector}}{{ $k }}={{ $v }},{{ end }}' | sed 's/.$//' ) kubectl --namespace knative-debug get pods -l $svcLabels This should return a single Pod, which if you inspect is the one generated by fn .","title":"svc"},{"location":"eventing/debugging/#chan","text":"chan uses the in-memory-channel . This is a very basic channel and has few failure modes that will be exhibited in chan 's status . kubectl --namespace knative-debug get channel.messaging.knative.dev chan -o jsonpath = '{.status.conditions[?(@.type == \"Ready\")].status}' This should return True . If it doesn't, get the full resource: kubectl --namespace knative-debug get channel.messaging.knative.dev chan --output yaml If status is completely missing, it implies that something is wrong with the in-memory-channel controller. See Channel Controller . Next verify that chan is addressable: kubectl --namespace knative-debug get channel.messaging.knative.dev chan -o jsonpath = '{.status.address.hostname}' This should return a URI, likely ending in '.cluster.local'. If it doesn't, then it implies that something went wrong during reconciliation. See Channel Controller . We will verify that the two resources that the chan creates exist and are Ready .","title":"chan"},{"location":"eventing/debugging/#service","text":"chan creates a K8s Service . kubectl --namespace knative-debug get service -l messaging.knative.dev/role = in -memory-channel It's spec is completely unimportant, as Istio will ignore it. It just needs to exist so that src can send events to it. If it doesn't exist, it implies that something went wrong during chan reconciliation. See Channel Controller .","title":"Service"},{"location":"eventing/debugging/#src","text":"src is a ApiServerSource . First we will verify that src is writing to chan . kubectl --namespace knative-debug get apiserversource src -o jsonpath = '{.spec.sink}' Which should return map[apiVersion:messaging.knative.dev/v1 kind:Channel name:chan] . If it doesn't, then src was setup incorrectly and its spec needs to be fixed. Fixing should be as simple as updating its spec to have the correct sink (see example.yaml ). Now that we know src is sending to chan , let's verify that it is Ready . kubectl --namespace knative-debug get apiserversource src -o jsonpath = '{.status.conditions[?(.type == \"Ready\")].status}'","title":"src"},{"location":"eventing/debugging/#sub","text":"sub is a Subscription from chan to fn . Verify that sub is Ready : kubectl --namespace knative-debug get subscription sub -o jsonpath = '{.status.conditions[?(.type == \"Ready\")].status}' This should return True . If it doesn't then, look at all the status entries. kubectl --namespace knative-debug get subscription sub --output yaml","title":"sub"},{"location":"eventing/debugging/#controllers","text":"Each of the resources has a Controller that is watching it. As of today, they tend to do a poor job of writing failure status messages and events, so we need to look at the Controller's logs.","title":"Controllers"},{"location":"eventing/debugging/#deployment-controller","text":"The Kubernetes Deployment Controller, controlling fn , is out of scope for this document.","title":"Deployment Controller"},{"location":"eventing/debugging/#service-controller","text":"The Kubernetes Service Controller, controlling svc , is out of scope for this document.","title":"Service Controller"},{"location":"eventing/debugging/#channel-controller","text":"There is not a single Channel Controller. Instead, there is one Controller for each Channel CRD. chan uses the InMemoryChannel Channel CRD , whose Controller is: kubectl --namespace knative-eventing get pod -l messaging.knative.dev/channel = in -memory-channel,messaging.knative.dev/role = controller --output yaml See its logs with: kubectl --namespace knative-eventing logs -l messaging.knative.dev/channel = in -memory-channel,messaging.knative.dev/role = controller Pay particular attention to any lines that have a logging level of warning or error .","title":"Channel Controller"},{"location":"eventing/debugging/#source-controller","text":"Each Source will have its own Controller. src is a ApiServerSource , so its Controller is: kubectl --namespace knative-eventing get pod -l app = sources-controller This is actually a single binary that runs multiple Source Controllers, importantly including ApiServerSource Controller .","title":"Source Controller"},{"location":"eventing/debugging/#apiserversource-controller","text":"The ApiServerSource Controller is run in the same binary as some other Source Controllers from Eventing. It is: kubectl --namespace knative-debug get pod -l eventing.knative.dev/sourceName = src,eventing.knative.dev/source = apiserver-source-controller View its logs with: kubectl --namespace knative-debug logs -l eventing.knative.dev/sourceName = src,eventing.knative.dev/source = apiserver-source-controller Pay particular attention to any lines that have a logging level of warning or error .","title":"ApiServerSource Controller"},{"location":"eventing/debugging/#subscription-controller","text":"The Subscription Controller controls sub . It attempts to resolve the addresses that a Channel should send events to, and once resolved, inject those into the Channel 's spec.subscribable . kubectl --namespace knative-eventing get pod -l app = eventing-controller View its logs with: kubectl --namespace knative-eventing logs -l app = eventing-controller Pay particular attention to any lines that have a logging level of warning or error .","title":"Subscription Controller"},{"location":"eventing/debugging/#data-plane","text":"The entire Control Plane looks healthy, but we're still not getting any events. Now we need to investigate the data plane. The Knative event takes the following path: Event is generated by src . In this case, it is caused by having a Kubernetes Event trigger it, but as far as Knative is concerned, the Source is generating the event denovo (from nothing). src is POSTing the event to chan 's address, http://chan-kn-channel.knative-debug.svc.cluster.local . The Channel Dispatcher receives the request and introspects the Host header to determine which Channel it corresponds to. It sees that it corresponds to knative-debug/chan so forwards the request to the subscribers defined in sub , in particular svc , which is backed by fn . fn receives the request and logs it. We will investigate components in the order in which events should travel.","title":"Data Plane"},{"location":"eventing/debugging/#channel-dispatcher","text":"The Channel Dispatcher is the component that receives POSTs pushing events into Channel s and then POSTs to subscribers of those Channel s when an event is received. For the in-memory-channel used in this example, there is a single binary that handles both the receiving and dispatching sides for all in-memory-channel Channel s. First we will inspect the Dispatcher's logs to see if it is anything obvious: kubectl --namespace knative-eventing logs -l messaging.knative.dev/channel = in -memory-channel,messaging.knative.dev/role = dispatcher -c dispatcher Ideally we will see lines like: { \"level\" : \"info\" , \"ts\" : \"2019-08-16T13:50:55.424Z\" , \"logger\" : \"inmemorychannel-dispatcher.in-memory-channel-dispatcher\" , \"caller\" : \"provisioners/message_receiver.go:147\" , \"msg\" : \"Request mapped to channel: knative-debug/chan-kn-channel\" , \"knative.dev/controller\" : \"in-memory-channel-dispatcher\" } { \"level\" : \"info\" , \"ts\" : \"2019-08-16T13:50:55.425Z\" , \"logger\" : \"inmemorychannel-dispatcher.in-memory-channel-dispatcher\" , \"caller\" : \"provisioners/message_dispatcher.go:112\" , \"msg\" : \"Dispatching message to http://svc.knative-debug.svc.cluster.local/\" , \"knative.dev/controller\" : \"in-memory-channel-dispatcher\" } { \"level\" : \"info\" , \"ts\" : \"2019-08-16T13:50:55.981Z\" , \"logger\" : \"inmemorychannel-dispatcher.in-memory-channel-dispatcher\" , \"caller\" : \"provisioners/message_receiver.go:140\" , \"msg\" : \"Received request for chan-kn-channel.knative-debug.svc.cluster.local\" , \"knative.dev/controller\" : \"in-memory-channel-dispatcher\" } Which shows that the request is being received and then sent to svc , which is returning a 2XX response code (likely 200, 202, or 204). However if we see something like: { \"level\" : \"info\" , \"ts\" : \"2019-08-16T16:10:16.859Z\" , \"logger\" : \"inmemorychannel-dispatcher.in-memory-channel-dispatcher\" , \"caller\" : \"provisioners/message_receiver.go:140\" , \"msg\" : \"Received request for chan-kn-channel.knative-debug.svc.cluster.local\" , \"knative.dev/controller\" : \"in-memory-channel-dispatcher\" } { \"level\" : \"info\" , \"ts\" : \"2019-08-16T16:10:16.859Z\" , \"logger\" : \"inmemorychannel-dispatcher.in-memory-channel-dispatcher\" , \"caller\" : \"provisioners/message_receiver.go:147\" , \"msg\" : \"Request mapped to channel: knative-debug/chan-kn-channel\" , \"knative.dev/controller\" : \"in-memory-channel-dispatcher\" } { \"level\" : \"info\" , \"ts\" : \"2019-08-16T16:10:16.859Z\" , \"logger\" : \"inmemorychannel-dispatcher.in-memory-channel-dispatcher\" , \"caller\" : \"provisioners/message_dispatcher.go:112\" , \"msg\" : \"Dispatching message to http://svc.knative-debug.svc.cluster.local/\" , \"knative.dev/controller\" : \"in-memory-channel-dispatcher\" } { \"level\" : \"error\" , \"ts\" : \"2019-08-16T16:10:38.169Z\" , \"logger\" : \"inmemorychannel-dispatcher.in-memory-channel-dispatcher\" , \"caller\" : \"fanout/fanout_handler.go:121\" , \"msg\" : \"Fanout had an error\" , \"knative.dev/controller\" : \"in-memory-channel-dispatcher\" , \"error\" : \"Unable to complete request Post http://svc.knative-debug.svc.cluster.local/: dial tcp 10.4.44.156:80: i/o timeout\" , \"stacktrace\" : \"knative.dev/eventing/pkg/provisioners/fanout.(*Handler).dispatch\\n\\t/Users/xxxxxx/go/src/knative.dev/eventing/pkg/provisioners/fanout/fanout_handler.go:121\\nknative.dev/eventing/pkg/provisioners/fanout.createReceiverFunction.func1.1\\n\\t/Users/i512777/go/src/knative.dev/eventing/pkg/provisioners/fanout/fanout_handler.go:95\" } Then we know there was a problem posting to http://svc.knative-debug.svc.cluster.local/ . TODO Finish this section. Especially after the Channel Dispatcher emits K8s events about failures.","title":"Channel Dispatcher"},{"location":"eventing/debugging/#fn_1","text":"TODO Fill in this section.","title":"fn"},{"location":"eventing/debugging/#todo-finish-the-guide","text":"","title":"TODO Finish the guide."},{"location":"eventing/flows/","text":"Knative Eventing provides a collection of CRDs for describing event flows: Sequence is for defining an in-order list of functions. Parallel is for defining a list of branches, each receiving the same CloudEvent.","title":"Overview"},{"location":"eventing/flows/_index/","text":"Macro Rendering Error \u00b6 TemplateSyntaxError : unexpected '%' Traceback (most recent call last): File \"/usr/local/lib/python3.9/site-packages/mkdocs_macros/plugin.py\", line 441, in render md_template = self.env.from_string(markdown) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 941, in from_string return cls.from_code(self, self.compile(source), globals, None) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 638, in compile self.handle_exception(source=source_hint) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 832, in handle_exception reraise(*rewrite_traceback_stack(source=source)) File \"/usr/local/lib/python3.9/site-packages/jinja2/_compat.py\", line 28, in reraise raise value.with_traceback(tb) File \"<unknown>\", line 3, in template jinja2.exceptions.TemplateSyntaxError: unexpected '%'","title":"Eventing Flows"},{"location":"eventing/flows/_index/#macro-rendering-error","text":"TemplateSyntaxError : unexpected '%' Traceback (most recent call last): File \"/usr/local/lib/python3.9/site-packages/mkdocs_macros/plugin.py\", line 441, in render md_template = self.env.from_string(markdown) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 941, in from_string return cls.from_code(self, self.compile(source), globals, None) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 638, in compile self.handle_exception(source=source_hint) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 832, in handle_exception reraise(*rewrite_traceback_stack(source=source)) File \"/usr/local/lib/python3.9/site-packages/jinja2/_compat.py\", line 28, in reraise raise value.with_traceback(tb) File \"<unknown>\", line 3, in template jinja2.exceptions.TemplateSyntaxError: unexpected '%'","title":"Macro Rendering Error"},{"location":"eventing/flows/parallel/","text":"Parallel \u00b6 Parallel CRD provides a way to easily define a list of branches, each receiving the same CloudEvent sent to the Parallel ingress channel. Typically, each branch consists of a filter function guarding the execution of the branch. Parallel creates Channel s and Subscription s under the hood. Usage \u00b6 Parallel Spec \u00b6 Parallel has three parts for the Spec: branches defines the list of filter and subscriber pairs, one per branch, and optionally a reply object. For each branch: (optional) the filter is evaluated and when it returns an event the subscriber is executed. Both filter and subscriber must be Addressable . the event returned by the subscriber is sent to the branch reply object. When the reply is empty, the event is sent to the spec.reply object (see below). (optional) channelTemplate defines the Template which will be used to create Channel s. (optional) reply defines where the result of each branch is sent to when the branch does not have its own reply object. Parallel Status \u00b6 Parallel has three parts for the Status: conditions which details the overall status of the Parallel object ingressChannelStatus and branchesStatuses which convey the status of underlying Channel and Subscription resource that are created as part of this Parallel. address which is exposed so that Parallel can be used where Addressable can be used. Sending to this address will target the Channel which is fronting this Parallel (same as ingressChannelStatus ). Examples \u00b6 Learn how to use Parallel by following the examples","title":"Parallel"},{"location":"eventing/flows/parallel/#parallel","text":"Parallel CRD provides a way to easily define a list of branches, each receiving the same CloudEvent sent to the Parallel ingress channel. Typically, each branch consists of a filter function guarding the execution of the branch. Parallel creates Channel s and Subscription s under the hood.","title":"Parallel"},{"location":"eventing/flows/parallel/#usage","text":"","title":"Usage"},{"location":"eventing/flows/parallel/#parallel-spec","text":"Parallel has three parts for the Spec: branches defines the list of filter and subscriber pairs, one per branch, and optionally a reply object. For each branch: (optional) the filter is evaluated and when it returns an event the subscriber is executed. Both filter and subscriber must be Addressable . the event returned by the subscriber is sent to the branch reply object. When the reply is empty, the event is sent to the spec.reply object (see below). (optional) channelTemplate defines the Template which will be used to create Channel s. (optional) reply defines where the result of each branch is sent to when the branch does not have its own reply object.","title":"Parallel Spec"},{"location":"eventing/flows/parallel/#parallel-status","text":"Parallel has three parts for the Status: conditions which details the overall status of the Parallel object ingressChannelStatus and branchesStatuses which convey the status of underlying Channel and Subscription resource that are created as part of this Parallel. address which is exposed so that Parallel can be used where Addressable can be used. Sending to this address will target the Channel which is fronting this Parallel (same as ingressChannelStatus ).","title":"Parallel Status"},{"location":"eventing/flows/parallel/#examples","text":"Learn how to use Parallel by following the examples","title":"Examples"},{"location":"eventing/flows/sequence/","text":"Sequence \u00b6 Sequence CRD provides a way to define an in-order list of functions that will be invoked. Each step can modify, filter or create a new kind of an event. Sequence creates Channel s and Subscription s under the hood. Usage \u00b6 Sequence Spec \u00b6 Sequence has three parts for the Spec: Steps which defines the in-order list of Subscriber s, aka, which functions are executed in the listed order. These are specified using the messaging.v1.SubscriberSpec just like you would when creating Subscription . Each step should be Addressable . ChannelTemplate defines the Template which will be used to create Channel s between the steps. Reply (Optional) Reference to where the results of the final step in the sequence are sent to. Sequence Status \u00b6 Sequence has four parts for the Status: Conditions which detail the overall Status of the Sequence object ChannelStatuses which convey the Status of underlying Channel resources that are created as part of this Sequence. It is an array and each Status corresponds to the Step number, so the first entry in the array is the Status of the Channel before the first Step. SubscriptionStatuses which convey the Status of underlying Subscription resources that are created as part of this Sequence. It is an array and each Status corresponds to the Step number, so the first entry in the array is the Subscription which is created to wire the first channel to the first step in the Steps array. AddressStatus which is exposed so that Sequence can be used where Addressable can be used. Sending to this address will target the Channel which is fronting the first Step in the Sequence. Examples \u00b6 For each of these examples below, we'll use PingSource as the source of events. We also use a very simple transformer which performs very trivial transformation of the incoming events to demonstrate they have passed through each stage. Sequence with no reply (terminal last Step) \u00b6 For the first example, we'll use a 3 Step Sequence that is wired directly into the PingSource . Each of the steps simply tacks on \"- Handled by \", for example the first Step in the Sequence will take the incoming message and append \"- Handled by 0\" to the incoming message. Sequence with reply (last Step produces output) \u00b6 For the next example, we'll use the same 3 Step Sequence that is wired directly into the PingSource . Each of the steps simply tacks on \"- Handled by \", for example the first Step in the Sequence will take the incoming message and append \"- Handled by 0\" to the incoming message. The only difference is that we'll use the Subscriber.Spec.Reply field to wire the output of the last Step to an event display pod. Chaining Sequences together \u00b6 For the next example, we'll use the same 3 Step Sequence that is wired directly into the PingSource . Each of the steps simply tacks on \"- Handled by \", for example the first Step in the Sequence will take the incoming message and append \"- Handled by 0\" to the incoming message. The only difference is that we'll use the Subscriber.Spec.Reply field to wire the output of the last Step to another Sequence that does the same message modifications as the first pipeline (with different steps however). Using Sequence with Broker/Trigger model \u00b6 You can also create a Trigger which targets Sequence . This time we'll wire PingSource to send events to a Broker and then we'll have the Sequence emit the resulting Events back into the Broker so that the results of the Sequence can be observed by other Trigger s.","title":"Sequence"},{"location":"eventing/flows/sequence/#sequence","text":"Sequence CRD provides a way to define an in-order list of functions that will be invoked. Each step can modify, filter or create a new kind of an event. Sequence creates Channel s and Subscription s under the hood.","title":"Sequence"},{"location":"eventing/flows/sequence/#usage","text":"","title":"Usage"},{"location":"eventing/flows/sequence/#sequence-spec","text":"Sequence has three parts for the Spec: Steps which defines the in-order list of Subscriber s, aka, which functions are executed in the listed order. These are specified using the messaging.v1.SubscriberSpec just like you would when creating Subscription . Each step should be Addressable . ChannelTemplate defines the Template which will be used to create Channel s between the steps. Reply (Optional) Reference to where the results of the final step in the sequence are sent to.","title":"Sequence Spec"},{"location":"eventing/flows/sequence/#sequence-status","text":"Sequence has four parts for the Status: Conditions which detail the overall Status of the Sequence object ChannelStatuses which convey the Status of underlying Channel resources that are created as part of this Sequence. It is an array and each Status corresponds to the Step number, so the first entry in the array is the Status of the Channel before the first Step. SubscriptionStatuses which convey the Status of underlying Subscription resources that are created as part of this Sequence. It is an array and each Status corresponds to the Step number, so the first entry in the array is the Subscription which is created to wire the first channel to the first step in the Steps array. AddressStatus which is exposed so that Sequence can be used where Addressable can be used. Sending to this address will target the Channel which is fronting the first Step in the Sequence.","title":"Sequence Status"},{"location":"eventing/flows/sequence/#examples","text":"For each of these examples below, we'll use PingSource as the source of events. We also use a very simple transformer which performs very trivial transformation of the incoming events to demonstrate they have passed through each stage.","title":"Examples"},{"location":"eventing/flows/sequence/#sequence-with-no-reply-terminal-last-step","text":"For the first example, we'll use a 3 Step Sequence that is wired directly into the PingSource . Each of the steps simply tacks on \"- Handled by \", for example the first Step in the Sequence will take the incoming message and append \"- Handled by 0\" to the incoming message.","title":"Sequence with no reply (terminal last Step)"},{"location":"eventing/flows/sequence/#sequence-with-reply-last-step-produces-output","text":"For the next example, we'll use the same 3 Step Sequence that is wired directly into the PingSource . Each of the steps simply tacks on \"- Handled by \", for example the first Step in the Sequence will take the incoming message and append \"- Handled by 0\" to the incoming message. The only difference is that we'll use the Subscriber.Spec.Reply field to wire the output of the last Step to an event display pod.","title":"Sequence with reply (last Step produces output)"},{"location":"eventing/flows/sequence/#chaining-sequences-together","text":"For the next example, we'll use the same 3 Step Sequence that is wired directly into the PingSource . Each of the steps simply tacks on \"- Handled by \", for example the first Step in the Sequence will take the incoming message and append \"- Handled by 0\" to the incoming message. The only difference is that we'll use the Subscriber.Spec.Reply field to wire the output of the last Step to another Sequence that does the same message modifications as the first pipeline (with different steps however).","title":"Chaining Sequences together"},{"location":"eventing/flows/sequence/#using-sequence-with-brokertrigger-model","text":"You can also create a Trigger which targets Sequence . This time we'll wire PingSource to send events to a Broker and then we'll have the Sequence emit the resulting Events back into the Broker so that the results of the Sequence can be observed by other Trigger s.","title":"Using Sequence with Broker/Trigger model"},{"location":"eventing/samples/","text":"Use the following code samples to help you understand the various use cases for Knative Eventing and Event Sources. Learn more about Knative Eventing and Eventing Sources . See all Knative code samples","title":"Index"},{"location":"eventing/samples/_index/","text":"Macro Rendering Error \u00b6 TemplateSyntaxError : unexpected '%' Traceback (most recent call last): File \"/usr/local/lib/python3.9/site-packages/mkdocs_macros/plugin.py\", line 441, in render md_template = self.env.from_string(markdown) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 941, in from_string return cls.from_code(self, self.compile(source), globals, None) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 638, in compile self.handle_exception(source=source_hint) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 832, in handle_exception reraise(*rewrite_traceback_stack(source=source)) File \"/usr/local/lib/python3.9/site-packages/jinja2/_compat.py\", line 28, in reraise raise value.with_traceback(tb) File \"<unknown>\", line 3, in template jinja2.exceptions.TemplateSyntaxError: unexpected '%'","title":"Knative Eventing code samples"},{"location":"eventing/samples/_index/#macro-rendering-error","text":"TemplateSyntaxError : unexpected '%' Traceback (most recent call last): File \"/usr/local/lib/python3.9/site-packages/mkdocs_macros/plugin.py\", line 441, in render md_template = self.env.from_string(markdown) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 941, in from_string return cls.from_code(self, self.compile(source), globals, None) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 638, in compile self.handle_exception(source=source_hint) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 832, in handle_exception reraise(*rewrite_traceback_stack(source=source)) File \"/usr/local/lib/python3.9/site-packages/jinja2/_compat.py\", line 28, in reraise raise value.with_traceback(tb) File \"<unknown>\", line 3, in template jinja2.exceptions.TemplateSyntaxError: unexpected '%'","title":"Macro Rendering Error"},{"location":"eventing/samples/apache-camel-source/","text":"These samples show how to configure Camel Sources. These event sources are highly dynamic and allow you to generate events from a variety of systems (cloud platforms, social networks, datastores, message brokers, legacy systems, etc.), leveraging all the 300+ components provided by Apache Camel . All Camel Sources use Apache Camel K as the runtime engine. Prerequisites \u00b6 Install Knative Serving and Eventing . Install the Apache Camel K Operator in any namespace where you want to run Camel sources. The preferred version that is compatible with Camel sources is Camel K v1.0.0-M4 . Installation instructions are provided in the Apache Camel K Manual . Documentation includes specific instructions for common Kubernetes environments, including development clusters. Install the Camel Source from the camel.yaml in the Eventing Sources release page : kubectl apply --filename camel.yaml Create Test Resources \u00b6 All the CamelSource examples use some test resources for the purpose of displaying the generated events. The following resources need to be created: a simple Knative event display service that prints incoming events to its log an in-memory channel named camel-test that buffers events created by the event source a subscription to direct events from the test channel to the event display service Deploy the display_resources.yaml : kubectl apply --filename display_resources.yaml Run a Timer CamelSource \u00b6 The samples directory contains some sample sources that can be used to generate events. The simplest example of CamelSource , that does not require additional configuration, is the timer source. The timer source periodically generates \"Hello world!\" events and forwards them to the provided destination. If you want, you can customize the source behavior using options available in the Apache Camel documentation for the timer component . All Camel components are documented in the Apache Camel Website . Install the timer CamelSource from source: kubectl apply -f source_timer.yaml Verify that the published events were sent into the Knative eventing system by looking at what is downstream of the CamelSource . kubectl logs --selector serving.knative.dev/service = camel-event-display -c user-container If you have deployed the timer source, you should see new log lines appearing every 3 seconds. Run a MQTT CamelSource \u00b6 One of the 300+ Camel components that you can leverage is Camel-Paho , based on the Eclipse Paho open source project. A source based on Paho (like the provided MQTT CamelSource ) allows to bridge any MQTT broker to a Knative resource, automatically converting IoT messages to Cloudevents. To use the MQTT source, you need a MQTT broker running and reachable from your cluster. For example, it's possible to run a Mosquito MQTT Broker for testing purposes. First, edit the MQTT CamelSource and put the correct address of the MQTT broker in the brokerUrl field. You also need to provide the name of the topic that you want to subscribe to: just change paho:mytopic to match the topic that you want to use. You can also scale this source out, in order to obtain more throughput, by changing the value of the replicas field. By default it creates 2 replicas for demonstration purposes. To reduce noise in the event display, you can remove all previously created CamelSources from the namespace: kubectl delete camelsource --all Install the mqtt CamelSource : kubectl apply -f source_mqtt.yaml You can now send MQTT messages to your broker using your favourite client (you can even use Camel K for sending test events). Each message you send to the MQTT broker will be printed by the event display as a Cloudevent. You can verify that your messages reach the event display by checking its logs: kubectl logs --selector serving.knative.dev/service = camel-event-display -c user-container Run a Telegram CamelSource \u00b6 Another useful component available with Camel is the Telegram component. It can be used to forward messages of a Telegram chat into Knative channels as events. Before using the provided Telegram CamelSource example, you need to follow the instructions on the Telegram website for creating a Telegram Bot . The quickest way to create a bot is to contact the Bot Father , another Telegram Bot, using your preferred Telegram client (mobile or web). After you create the bot, you will receive an authorization token that is needed for the source to work. First, edit the telegram CamelSource and put the authorization token, replacing the <put-your-token-here> placeholder. To reduce noise in the event display, you can remove all previously created CamelSources from the namespace: kubectl delete camelsource --all Install the telegram CamelSource : kubectl apply -f source_telegram.yaml Now, you can contact your bot with any Telegram client. Each message you send to the bot will be printed by the event display as a Cloudevent. You can verify that your messages reach the event display by checking its logs: kubectl logs --selector serving.knative.dev/service = camel-event-display -c user-container Run an HTTP Poller CamelSource \u00b6 CamelSources are not limited to using a single Camel component. For example, you can combine the Camel Timer component with the Camel HTTP component to periodically fetch an external API, transform the result into a Cloudevent and forward it to a given destination. The example will retrieve a static JSON file from a remote URL, but you can edit the HTTP poller CamelSource to add your own API. If you have previously deployed other CamelSources, to reduce noise in the event display, you can remove them all from the namespace: kubectl delete camelsource --all Install the HTTP poller CamelSource : kubectl apply -f source_http_poller.yaml The event display will show some JSON data periodically pulled from the external REST API. To check the logs: kubectl logs --selector serving.knative.dev/service = camel-event-display -c user-container","title":"Apache Camel Source"},{"location":"eventing/samples/apache-camel-source/#prerequisites","text":"Install Knative Serving and Eventing . Install the Apache Camel K Operator in any namespace where you want to run Camel sources. The preferred version that is compatible with Camel sources is Camel K v1.0.0-M4 . Installation instructions are provided in the Apache Camel K Manual . Documentation includes specific instructions for common Kubernetes environments, including development clusters. Install the Camel Source from the camel.yaml in the Eventing Sources release page : kubectl apply --filename camel.yaml","title":"Prerequisites"},{"location":"eventing/samples/apache-camel-source/#create-test-resources","text":"All the CamelSource examples use some test resources for the purpose of displaying the generated events. The following resources need to be created: a simple Knative event display service that prints incoming events to its log an in-memory channel named camel-test that buffers events created by the event source a subscription to direct events from the test channel to the event display service Deploy the display_resources.yaml : kubectl apply --filename display_resources.yaml","title":"Create Test Resources"},{"location":"eventing/samples/apache-camel-source/#run-a-timer-camelsource","text":"The samples directory contains some sample sources that can be used to generate events. The simplest example of CamelSource , that does not require additional configuration, is the timer source. The timer source periodically generates \"Hello world!\" events and forwards them to the provided destination. If you want, you can customize the source behavior using options available in the Apache Camel documentation for the timer component . All Camel components are documented in the Apache Camel Website . Install the timer CamelSource from source: kubectl apply -f source_timer.yaml Verify that the published events were sent into the Knative eventing system by looking at what is downstream of the CamelSource . kubectl logs --selector serving.knative.dev/service = camel-event-display -c user-container If you have deployed the timer source, you should see new log lines appearing every 3 seconds.","title":"Run a Timer CamelSource"},{"location":"eventing/samples/apache-camel-source/#run-a-mqtt-camelsource","text":"One of the 300+ Camel components that you can leverage is Camel-Paho , based on the Eclipse Paho open source project. A source based on Paho (like the provided MQTT CamelSource ) allows to bridge any MQTT broker to a Knative resource, automatically converting IoT messages to Cloudevents. To use the MQTT source, you need a MQTT broker running and reachable from your cluster. For example, it's possible to run a Mosquito MQTT Broker for testing purposes. First, edit the MQTT CamelSource and put the correct address of the MQTT broker in the brokerUrl field. You also need to provide the name of the topic that you want to subscribe to: just change paho:mytopic to match the topic that you want to use. You can also scale this source out, in order to obtain more throughput, by changing the value of the replicas field. By default it creates 2 replicas for demonstration purposes. To reduce noise in the event display, you can remove all previously created CamelSources from the namespace: kubectl delete camelsource --all Install the mqtt CamelSource : kubectl apply -f source_mqtt.yaml You can now send MQTT messages to your broker using your favourite client (you can even use Camel K for sending test events). Each message you send to the MQTT broker will be printed by the event display as a Cloudevent. You can verify that your messages reach the event display by checking its logs: kubectl logs --selector serving.knative.dev/service = camel-event-display -c user-container","title":"Run a MQTT CamelSource"},{"location":"eventing/samples/apache-camel-source/#run-a-telegram-camelsource","text":"Another useful component available with Camel is the Telegram component. It can be used to forward messages of a Telegram chat into Knative channels as events. Before using the provided Telegram CamelSource example, you need to follow the instructions on the Telegram website for creating a Telegram Bot . The quickest way to create a bot is to contact the Bot Father , another Telegram Bot, using your preferred Telegram client (mobile or web). After you create the bot, you will receive an authorization token that is needed for the source to work. First, edit the telegram CamelSource and put the authorization token, replacing the <put-your-token-here> placeholder. To reduce noise in the event display, you can remove all previously created CamelSources from the namespace: kubectl delete camelsource --all Install the telegram CamelSource : kubectl apply -f source_telegram.yaml Now, you can contact your bot with any Telegram client. Each message you send to the bot will be printed by the event display as a Cloudevent. You can verify that your messages reach the event display by checking its logs: kubectl logs --selector serving.knative.dev/service = camel-event-display -c user-container","title":"Run a Telegram CamelSource"},{"location":"eventing/samples/apache-camel-source/#run-an-http-poller-camelsource","text":"CamelSources are not limited to using a single Camel component. For example, you can combine the Camel Timer component with the Camel HTTP component to periodically fetch an external API, transform the result into a Cloudevent and forward it to a given destination. The example will retrieve a static JSON file from a remote URL, but you can edit the HTTP poller CamelSource to add your own API. If you have previously deployed other CamelSources, to reduce noise in the event display, you can remove them all from the namespace: kubectl delete camelsource --all Install the HTTP poller CamelSource : kubectl apply -f source_http_poller.yaml The event display will show some JSON data periodically pulled from the external REST API. To check the logs: kubectl logs --selector serving.knative.dev/service = camel-event-display -c user-container","title":"Run an HTTP Poller CamelSource"},{"location":"eventing/samples/cloud-audit-logs-source/","text":"Please refer to the example in knative-gcp.","title":"CloudAuditLogsSource"},{"location":"eventing/samples/cloud-pubsub-source/","text":"Please refer to the example in knative-gcp.","title":"CloudPubSubSource"},{"location":"eventing/samples/cloud-scheduler-source/","text":"Please refer to the example in knative-gcp.","title":"CloudSchedulerSource"},{"location":"eventing/samples/cloud-storage-source/","text":"Please refer to the example in knative-gcp.","title":"CloudStorageSource"},{"location":"eventing/samples/container-source/","text":"ContainerSource will start a container image which will generate events under certain situations and send messages to a sink URI. It also can be an easy way to support your own event sources in Knative. This guide shows how to configure ContainerSource as an event source for functions and summarizes guidelines for creating your own event source as a ContainerSource. Create a heartbeats ContainerSource \u00b6 Prerequisites \u00b6 Setup Knative Serving . Setup Knative Eventing and Sources . Prepare the heartbeats image \u00b6 Knative event-sources has a sample of heartbeats event source. You could clone the source code by git clone -b \"v0.21.x\" https://github.com/knative/eventing-contrib.git And then build a heartbeats image and publish to your image repo with ko publish knative.dev/eventing-contrib/cmd/heartbeats Note : ko publish requires: KO_DOCKER_REPO to be set. (e.g. gcr.io/[gcloud-project] or docker.io/<username> ) you to be authenticated with your KO_DOCKER_REPO docker to be installed Create a Knative Service \u00b6 In order to verify ContainerSource is working, we will create a Event Display Service that dumps incoming messages to its log. apiVersion : serving.knative.dev/v1 kind : Service metadata : name : event-display spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing-contrib/cmd/event_display Use following command to create the service from service.yaml : kubectl apply --filename service.yaml The status of the created service can be seen using: kubectl get ksvc NAME URL LATESTCREATED LATESTREADY READY REASON event-display http://event-display.default.1.2.3.4.xip.io event-display-gqjbw event-display-gqjbw True Create a ContainerSource using the heartbeats image \u00b6 In order to run the heartbeats container as an event source, you have to create a concrete ContainerSource with specific arguments and environment settings. Be sure to replace heartbeats_image_uri with a valid uri for your heartbeats image in your image repo in heartbeats-source.yaml file. Note that arguments and environment variables are set and will be passed to the container. apiVersion : sources.knative.dev/v1 kind : ContainerSource metadata : name : test-heartbeats spec : template : spec : containers : - image : <heartbeats_image_uri> name : heartbeats args : - --period=1 env : - name : POD_NAME value : \"mypod\" - name : POD_NAMESPACE value : \"event-test\" sink : ref : apiVersion : serving.knative.dev/v1 kind : Service name : event-display Use the following command to create the event source from heartbeats-source.yaml : kubectl apply --filename heartbeats-source.yaml Verify \u00b6 We will verify that the message was sent to the Knative eventing system by looking at event-display service logs. kubectl logs -l serving.knative.dev/service = event-display -c user-container --since = 10m You should see log lines showing the request headers and body of the event message sent by the heartbeats source to the display function: \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1.0 type: dev.knative.eventing.samples.heartbeat source: https://knative.dev/eventing-contrib/cmd/heartbeats/#event-test/mypod id: 2b72d7bf-c38f-4a98-a433-608fbcdd2596 time: 2019-10-18T15:23:20.809775386Z contenttype: application/json Extensions, beats: true heart: yes the: 42 Data, { \"id\": 2, \"label\": \"\" } Create a new event source using ContainerSource \u00b6 In order to create a new event source using ContainerSource, you will create a container image at first, and then create a ContainerSource with the image uri and specify the values of parameters. Develop, build and publish a container image \u00b6 The container image can be developed with any language, build and publish with any tools you like. Here are some basic guidelines: The container image must have a main method to start with. The main method will accept parameters from arguments and environment variables. Two environments variables will be injected by the ContainerSource controller, K_SINK and K_CE_OVERRIDES , resolved from spec.sink and spec.ceOverrides respectively. The event messages shall be sent to the sink URI specified in K_SINK . The message can be any format. CloudEvents format is recommended. heartbeats event source is a sample for your reference. Create the ContainerSource using this container image \u00b6 When the container image is ready, a YAML file will be used to create a concrete ContainerSource . Use heartbeats-source.yaml as a sample for reference. Learn more about the ContainerSource specification .","title":"Container Source"},{"location":"eventing/samples/container-source/#create-a-heartbeats-containersource","text":"","title":"Create a heartbeats ContainerSource"},{"location":"eventing/samples/container-source/#prerequisites","text":"Setup Knative Serving . Setup Knative Eventing and Sources .","title":"Prerequisites"},{"location":"eventing/samples/container-source/#prepare-the-heartbeats-image","text":"Knative event-sources has a sample of heartbeats event source. You could clone the source code by git clone -b \"v0.21.x\" https://github.com/knative/eventing-contrib.git And then build a heartbeats image and publish to your image repo with ko publish knative.dev/eventing-contrib/cmd/heartbeats Note : ko publish requires: KO_DOCKER_REPO to be set. (e.g. gcr.io/[gcloud-project] or docker.io/<username> ) you to be authenticated with your KO_DOCKER_REPO docker to be installed","title":"Prepare the heartbeats image"},{"location":"eventing/samples/container-source/#create-a-knative-service","text":"In order to verify ContainerSource is working, we will create a Event Display Service that dumps incoming messages to its log. apiVersion : serving.knative.dev/v1 kind : Service metadata : name : event-display spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing-contrib/cmd/event_display Use following command to create the service from service.yaml : kubectl apply --filename service.yaml The status of the created service can be seen using: kubectl get ksvc NAME URL LATESTCREATED LATESTREADY READY REASON event-display http://event-display.default.1.2.3.4.xip.io event-display-gqjbw event-display-gqjbw True","title":"Create a Knative Service"},{"location":"eventing/samples/container-source/#create-a-containersource-using-the-heartbeats-image","text":"In order to run the heartbeats container as an event source, you have to create a concrete ContainerSource with specific arguments and environment settings. Be sure to replace heartbeats_image_uri with a valid uri for your heartbeats image in your image repo in heartbeats-source.yaml file. Note that arguments and environment variables are set and will be passed to the container. apiVersion : sources.knative.dev/v1 kind : ContainerSource metadata : name : test-heartbeats spec : template : spec : containers : - image : <heartbeats_image_uri> name : heartbeats args : - --period=1 env : - name : POD_NAME value : \"mypod\" - name : POD_NAMESPACE value : \"event-test\" sink : ref : apiVersion : serving.knative.dev/v1 kind : Service name : event-display Use the following command to create the event source from heartbeats-source.yaml : kubectl apply --filename heartbeats-source.yaml","title":"Create a ContainerSource using the heartbeats image"},{"location":"eventing/samples/container-source/#verify","text":"We will verify that the message was sent to the Knative eventing system by looking at event-display service logs. kubectl logs -l serving.knative.dev/service = event-display -c user-container --since = 10m You should see log lines showing the request headers and body of the event message sent by the heartbeats source to the display function: \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1.0 type: dev.knative.eventing.samples.heartbeat source: https://knative.dev/eventing-contrib/cmd/heartbeats/#event-test/mypod id: 2b72d7bf-c38f-4a98-a433-608fbcdd2596 time: 2019-10-18T15:23:20.809775386Z contenttype: application/json Extensions, beats: true heart: yes the: 42 Data, { \"id\": 2, \"label\": \"\" }","title":"Verify"},{"location":"eventing/samples/container-source/#create-a-new-event-source-using-containersource","text":"In order to create a new event source using ContainerSource, you will create a container image at first, and then create a ContainerSource with the image uri and specify the values of parameters.","title":"Create a new event source using ContainerSource"},{"location":"eventing/samples/container-source/#develop-build-and-publish-a-container-image","text":"The container image can be developed with any language, build and publish with any tools you like. Here are some basic guidelines: The container image must have a main method to start with. The main method will accept parameters from arguments and environment variables. Two environments variables will be injected by the ContainerSource controller, K_SINK and K_CE_OVERRIDES , resolved from spec.sink and spec.ceOverrides respectively. The event messages shall be sent to the sink URI specified in K_SINK . The message can be any format. CloudEvents format is recommended. heartbeats event source is a sample for your reference.","title":"Develop, build and publish a container image"},{"location":"eventing/samples/container-source/#create-the-containersource-using-this-container-image","text":"When the container image is ready, a YAML file will be used to create a concrete ContainerSource . Use heartbeats-source.yaml as a sample for reference. Learn more about the ContainerSource specification .","title":"Create the ContainerSource using this container image"},{"location":"eventing/samples/github-source/","text":"GitHub Source example shows how to wire GitHub events for consumption by a Knative Service. Before you begin \u00b6 Set up Knative Serving . Ensure Knative Serving is configured with a domain name that allows GitHub to call into the cluster. If you're using GKE, you'll also want to assign a static IP address . Set up Knative Eventing with the GitHub source. Create a Knative Service \u00b6 To verify the GitHub source is working, create a simple Knative Service that dumps incoming messages to its log. The service.yaml file defines this basic Service. apiVersion : serving.knative.dev/v1 kind : Service metadata : name : github-message-dumper spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing-contrib/cmd/event_display Enter the following command to create the service from service.yaml : kubectl --namespace default apply --filename service.yaml Create GitHub Tokens \u00b6 Create a personal access token for GitHub that the GitHub source can use to register webhooks with the GitHub API. Also decide on a secret token that your code will use to authenticate the incoming webhooks from GitHub ( secretToken ). The token can be named anything you find convenient. The Source requires repo:public_repo and admin:repo_hook , to let it fire events from your public repositories and to create webhooks for those repositories. Copy and save this token; GitHub will force you to generate it again if misplaced. Here's an example for a token named \"GitHubSource Sample\" with the recommended scopes: Update githubsecret.yaml with those values. If your generated access token is 'personal_access_token_value' and you choose your secretToken as 'asdfasfdsaf' , you'd modify githubsecret.yaml like so: apiVersion : v1 kind : Secret metadata : name : githubsecret type : Opaque stringData : accessToken : personal_access_token_value secretToken : asdfasfdsaf Hint: you can makeup a random secretToken with: head -c 8 /dev/urandom | base64 Then, apply the githubsecret using kubectl : kubectl --namespace default apply --filename githubsecret.yaml Create Event Source for GitHub Events \u00b6 In order to receive GitHub events, you have to create a concrete Event Source for a specific namespace. Be sure to replace the ownerAndRepository value with a valid GitHub public repository owned by your GitHub user. If using GitHub enterprise you will need to add an additional githubAPIURL field to the spec specifying your GitHub enterprise API endpoint, see here apiVersion : sources.knative.dev/v1alpha1 kind : GitHubSource metadata : name : githubsourcesample spec : eventTypes : - pull_request ownerAndRepository : <YOUR USER>/<YOUR REPO> accessToken : secretKeyRef : name : githubsecret key : accessToken secretToken : secretKeyRef : name : githubsecret key : secretToken sink : ref : apiVersion : serving.knative.dev/v1 kind : Service name : github-message-dumper Then, apply that yaml using kubectl : kubectl --namespace default apply --filename github-source.yaml Verify \u00b6 Verify the GitHub webhook was created by looking at the list of webhooks under the Settings tab in your GitHub repository. A hook should be listed that points to your Knative cluster with a green check mark to the left of the hook URL, as shown below. Create Events \u00b6 Create a pull request in your GitHub repository. We will verify that the GitHub events were sent into the Knative eventing system by looking at our message dumper function logs. kubectl --namespace default get pods kubectl --namespace default logs github-event-display-XXXX user-container You should log lines similar to: 2018/11/08 18:25:34 Message Dumper received a message: POST / HTTP/1.1 Host: github-event-display.knative-demo.svc.cluster.local Accept-Encoding: gzip Ce-Cloudeventsversion: 0.1 Ce-Eventid: a8d4cf20-e383-11e8-8069-46e3c8ad2b4d Ce-Eventtime: 2018-11-08T18:25:32.819548012Z Ce-Eventtype: dev.knative.source.github.pull_request Ce-Source: https://github.com/someuser/somerepo/pull/1 Content-Length: 21060 Content-Type: application/json User-Agent: Go-http-client/1.1 X-B3-Parentspanid: b2e514c3dbe94c03 X-B3-Sampled: 1 X-B3-Spanid: c85e346d89c8be4e X-B3-Traceid: abf6292d458fb8e7 X-Envoy-Expected-Rq-Timeout-Ms: 60000 X-Envoy-Internal: true X-Forwarded-For: 127.0.0.1, 127.0.0.1 X-Forwarded-Proto: http X-Request-Id: 8a2201af-5075-9447-b593-ec3a243aff52 {\"action\":\"opened\",\"number\":1,\"pull_request\": ...} Cleanup \u00b6 You can remove the Github webhook by deleting the Github source: kubectl --namespace default delete --filename github-source.yaml Similarly, you can remove the Service and Secret via: kubectl --namespace default delete --filename service.yaml kubectl --namespace default delete --filename githubsecret.yaml","title":"Github Source"},{"location":"eventing/samples/github-source/#before-you-begin","text":"Set up Knative Serving . Ensure Knative Serving is configured with a domain name that allows GitHub to call into the cluster. If you're using GKE, you'll also want to assign a static IP address . Set up Knative Eventing with the GitHub source.","title":"Before you begin"},{"location":"eventing/samples/github-source/#create-a-knative-service","text":"To verify the GitHub source is working, create a simple Knative Service that dumps incoming messages to its log. The service.yaml file defines this basic Service. apiVersion : serving.knative.dev/v1 kind : Service metadata : name : github-message-dumper spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing-contrib/cmd/event_display Enter the following command to create the service from service.yaml : kubectl --namespace default apply --filename service.yaml","title":"Create a Knative Service"},{"location":"eventing/samples/github-source/#create-github-tokens","text":"Create a personal access token for GitHub that the GitHub source can use to register webhooks with the GitHub API. Also decide on a secret token that your code will use to authenticate the incoming webhooks from GitHub ( secretToken ). The token can be named anything you find convenient. The Source requires repo:public_repo and admin:repo_hook , to let it fire events from your public repositories and to create webhooks for those repositories. Copy and save this token; GitHub will force you to generate it again if misplaced. Here's an example for a token named \"GitHubSource Sample\" with the recommended scopes: Update githubsecret.yaml with those values. If your generated access token is 'personal_access_token_value' and you choose your secretToken as 'asdfasfdsaf' , you'd modify githubsecret.yaml like so: apiVersion : v1 kind : Secret metadata : name : githubsecret type : Opaque stringData : accessToken : personal_access_token_value secretToken : asdfasfdsaf Hint: you can makeup a random secretToken with: head -c 8 /dev/urandom | base64 Then, apply the githubsecret using kubectl : kubectl --namespace default apply --filename githubsecret.yaml","title":"Create GitHub Tokens"},{"location":"eventing/samples/github-source/#create-event-source-for-github-events","text":"In order to receive GitHub events, you have to create a concrete Event Source for a specific namespace. Be sure to replace the ownerAndRepository value with a valid GitHub public repository owned by your GitHub user. If using GitHub enterprise you will need to add an additional githubAPIURL field to the spec specifying your GitHub enterprise API endpoint, see here apiVersion : sources.knative.dev/v1alpha1 kind : GitHubSource metadata : name : githubsourcesample spec : eventTypes : - pull_request ownerAndRepository : <YOUR USER>/<YOUR REPO> accessToken : secretKeyRef : name : githubsecret key : accessToken secretToken : secretKeyRef : name : githubsecret key : secretToken sink : ref : apiVersion : serving.knative.dev/v1 kind : Service name : github-message-dumper Then, apply that yaml using kubectl : kubectl --namespace default apply --filename github-source.yaml","title":"Create Event Source for GitHub Events"},{"location":"eventing/samples/github-source/#verify","text":"Verify the GitHub webhook was created by looking at the list of webhooks under the Settings tab in your GitHub repository. A hook should be listed that points to your Knative cluster with a green check mark to the left of the hook URL, as shown below.","title":"Verify"},{"location":"eventing/samples/github-source/#create-events","text":"Create a pull request in your GitHub repository. We will verify that the GitHub events were sent into the Knative eventing system by looking at our message dumper function logs. kubectl --namespace default get pods kubectl --namespace default logs github-event-display-XXXX user-container You should log lines similar to: 2018/11/08 18:25:34 Message Dumper received a message: POST / HTTP/1.1 Host: github-event-display.knative-demo.svc.cluster.local Accept-Encoding: gzip Ce-Cloudeventsversion: 0.1 Ce-Eventid: a8d4cf20-e383-11e8-8069-46e3c8ad2b4d Ce-Eventtime: 2018-11-08T18:25:32.819548012Z Ce-Eventtype: dev.knative.source.github.pull_request Ce-Source: https://github.com/someuser/somerepo/pull/1 Content-Length: 21060 Content-Type: application/json User-Agent: Go-http-client/1.1 X-B3-Parentspanid: b2e514c3dbe94c03 X-B3-Sampled: 1 X-B3-Spanid: c85e346d89c8be4e X-B3-Traceid: abf6292d458fb8e7 X-Envoy-Expected-Rq-Timeout-Ms: 60000 X-Envoy-Internal: true X-Forwarded-For: 127.0.0.1, 127.0.0.1 X-Forwarded-Proto: http X-Request-Id: 8a2201af-5075-9447-b593-ec3a243aff52 {\"action\":\"opened\",\"number\":1,\"pull_request\": ...}","title":"Create Events"},{"location":"eventing/samples/github-source/#cleanup","text":"You can remove the Github webhook by deleting the Github source: kubectl --namespace default delete --filename github-source.yaml Similarly, you can remove the Service and Secret via: kubectl --namespace default delete --filename service.yaml kubectl --namespace default delete --filename githubsecret.yaml","title":"Cleanup"},{"location":"eventing/samples/gitlab-source/","text":"GitLab Source example shows how to wire GitLab events for consumption by a Knative Service. Gitlab source deployment \u00b6 Prerequisites \u00b6 You will need: An internet-accessible Kubernetes cluster with Knative Serving installed. Follow the installation instructions if you need to create one. Ensure Knative Serving is configured with a domain name that allows GitLab to call into the cluster. If you're using GKE, you'll also want to assign a static IP address . Install Knative Eventing . Install GitLab Event Source \u00b6 GitLab Event source lives in the knative/eventing-contrib . Head to the releases page, find the latest release with gitlab.yaml artifact and replace the <RELEASE> with version tag: kubectl apply -f https://github.com/knative/eventing-contrib/releases/download/<RELEASE>/gitlab.yaml Check that the manager is running: kubectl -n knative-sources get pods --selector control-plane = gitlab-controller-manager With the controller running you can now move on to a user persona and setup a GitLab webhook as well as a function that will consume GitLab events. Using the GitLab Event Source \u00b6 You are now ready to use the Event Source and trigger functions based on GitLab projects events. We will: Create a Knative service which will receive the events. To keep things simple this service will simply dump the events to stdout , this is the so-called: event_display Create a GitLab access token and a random secret token used to secure the webhooks Create the event source by posting a GitLab source object manifest to Kubernetes Create a Knative Service \u00b6 The event-display.yaml file shown below defines the basic service which will receive events from the GitLab source. apiVersion : serving.knative.dev/v1 kind : Service metadata : name : gitlab-event-display spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing-contrib/cmd/event_display Create the service: kubectl -n default apply -f event-display.yaml Create GitLab Tokens \u00b6 Create a personal access token which the GitLab source will use to register webhooks with the GitLab API. The token must have an \"api\" access scope in order to create repository webhooks. Also decide on a secret token that your source will use to authenticate the incoming webhooks from GitLab. Update a secret values in secret.yaml defined below: accessToken is the personal access token created in step 1 and secretToken is any token of your choosing. Hint: you can generate a random secretToken with: head -c 8 /dev/urandom | base64 secret.yaml : apiVersion : v1 kind : Secret metadata : name : gitlabsecret type : Opaque stringData : accessToken : <personal_access_token_value> secretToken : <random_string> Create the secret using kubectl . kubectl -n default apply -f secret.yaml Create Event Source for GitLab Events \u00b6 In order to receive GitLab events, you have to create a concrete Event Source for a specific namespace. Replace the projectUrl value in the gitlabsource.yaml file with your GitLab project URL, for example https://gitlab.com/knative-examples/functions . gitlabsource.yaml : apiVersion : sources.knative.dev/v1alpha1 kind : GitLabSource metadata : name : gitlabsource-sample spec : eventTypes : - push_events - issues_events projectUrl : <project url> accessToken : secretKeyRef : name : gitlabsecret key : accessToken secretToken : secretKeyRef : name : gitlabsecret key : secretToken sink : ref : apiVersion : serving.knative.dev/v1 kind : Service name : gitlab-event-display Apply the yaml file using kubectl : kubectl -n default apply -f gitlabsource.yaml Verify \u00b6 Verify that GitLab webhook was created by looking at the list of webhooks under Settings >> Integrations in your GitLab project. A hook should be listed that points to your Knative cluster. Create a push event and check the logs of the Pod backing the gitlab-event-display knative service. You will see the event: \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 0.3 type: dev.knative.sources.gitlabsource.Push Hook source: https://gitlab.com/<user>/<project> id: f83c080f-c2af-48ff-8d8b-fd5b21c5938e time: 2020-03-12T11:08:41.414572482Z datacontenttype: application/json Data, { <Event payload> } Cleanup \u00b6 You can remove the GitLab webhook by deleting the GitLab source: kubectl --namespace default delete --filename gitlabsource.yaml Similarly, you can remove the Service and Secret via: kubectl --namespace default delete --filename event-display.yaml kubectl --namespace default delete --filename secret.yaml","title":"Gitlab Source"},{"location":"eventing/samples/gitlab-source/#gitlab-source-deployment","text":"","title":"Gitlab source deployment"},{"location":"eventing/samples/gitlab-source/#prerequisites","text":"You will need: An internet-accessible Kubernetes cluster with Knative Serving installed. Follow the installation instructions if you need to create one. Ensure Knative Serving is configured with a domain name that allows GitLab to call into the cluster. If you're using GKE, you'll also want to assign a static IP address . Install Knative Eventing .","title":"Prerequisites"},{"location":"eventing/samples/gitlab-source/#install-gitlab-event-source","text":"GitLab Event source lives in the knative/eventing-contrib . Head to the releases page, find the latest release with gitlab.yaml artifact and replace the <RELEASE> with version tag: kubectl apply -f https://github.com/knative/eventing-contrib/releases/download/<RELEASE>/gitlab.yaml Check that the manager is running: kubectl -n knative-sources get pods --selector control-plane = gitlab-controller-manager With the controller running you can now move on to a user persona and setup a GitLab webhook as well as a function that will consume GitLab events.","title":"Install GitLab Event Source"},{"location":"eventing/samples/gitlab-source/#using-the-gitlab-event-source","text":"You are now ready to use the Event Source and trigger functions based on GitLab projects events. We will: Create a Knative service which will receive the events. To keep things simple this service will simply dump the events to stdout , this is the so-called: event_display Create a GitLab access token and a random secret token used to secure the webhooks Create the event source by posting a GitLab source object manifest to Kubernetes","title":"Using the GitLab Event Source"},{"location":"eventing/samples/gitlab-source/#create-a-knative-service","text":"The event-display.yaml file shown below defines the basic service which will receive events from the GitLab source. apiVersion : serving.knative.dev/v1 kind : Service metadata : name : gitlab-event-display spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing-contrib/cmd/event_display Create the service: kubectl -n default apply -f event-display.yaml","title":"Create a Knative Service"},{"location":"eventing/samples/gitlab-source/#create-gitlab-tokens","text":"Create a personal access token which the GitLab source will use to register webhooks with the GitLab API. The token must have an \"api\" access scope in order to create repository webhooks. Also decide on a secret token that your source will use to authenticate the incoming webhooks from GitLab. Update a secret values in secret.yaml defined below: accessToken is the personal access token created in step 1 and secretToken is any token of your choosing. Hint: you can generate a random secretToken with: head -c 8 /dev/urandom | base64 secret.yaml : apiVersion : v1 kind : Secret metadata : name : gitlabsecret type : Opaque stringData : accessToken : <personal_access_token_value> secretToken : <random_string> Create the secret using kubectl . kubectl -n default apply -f secret.yaml","title":"Create GitLab Tokens"},{"location":"eventing/samples/gitlab-source/#create-event-source-for-gitlab-events","text":"In order to receive GitLab events, you have to create a concrete Event Source for a specific namespace. Replace the projectUrl value in the gitlabsource.yaml file with your GitLab project URL, for example https://gitlab.com/knative-examples/functions . gitlabsource.yaml : apiVersion : sources.knative.dev/v1alpha1 kind : GitLabSource metadata : name : gitlabsource-sample spec : eventTypes : - push_events - issues_events projectUrl : <project url> accessToken : secretKeyRef : name : gitlabsecret key : accessToken secretToken : secretKeyRef : name : gitlabsecret key : secretToken sink : ref : apiVersion : serving.knative.dev/v1 kind : Service name : gitlab-event-display Apply the yaml file using kubectl : kubectl -n default apply -f gitlabsource.yaml","title":"Create Event Source for GitLab Events"},{"location":"eventing/samples/gitlab-source/#verify","text":"Verify that GitLab webhook was created by looking at the list of webhooks under Settings >> Integrations in your GitLab project. A hook should be listed that points to your Knative cluster. Create a push event and check the logs of the Pod backing the gitlab-event-display knative service. You will see the event: \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 0.3 type: dev.knative.sources.gitlabsource.Push Hook source: https://gitlab.com/<user>/<project> id: f83c080f-c2af-48ff-8d8b-fd5b21c5938e time: 2020-03-12T11:08:41.414572482Z datacontenttype: application/json Data, { <Event payload> }","title":"Verify"},{"location":"eventing/samples/gitlab-source/#cleanup","text":"You can remove the GitLab webhook by deleting the GitLab source: kubectl --namespace default delete --filename gitlabsource.yaml Similarly, you can remove the Service and Secret via: kubectl --namespace default delete --filename event-display.yaml kubectl --namespace default delete --filename secret.yaml","title":"Cleanup"},{"location":"eventing/samples/helloworld/","text":"Following examples include a simple web app written in the language of your choice that you can use to test knative eventing. It shows how to consume a CloudEvent in Knative eventing, and optionally how to respond back with another CloudEvent in the HTTP response. We will deploy the app as a Kubernetes Deployment along with a Kubernetes Service . However, you can also deploy the app as a Knative Serving Service Prerequisites \u00b6 A Kubernetes cluster with Knative Eventing installed. If you decide to deploy the app as a Knative Serving Service then you will have to install Knative Serving . Docker installed and running on your local machine, and a Docker Hub account configured (we'll use it for a container registry).","title":"Overview"},{"location":"eventing/samples/helloworld/#prerequisites","text":"A Kubernetes cluster with Knative Eventing installed. If you decide to deploy the app as a Knative Serving Service then you will have to install Knative Serving . Docker installed and running on your local machine, and a Docker Hub account configured (we'll use it for a container registry).","title":"Prerequisites"},{"location":"eventing/samples/helloworld/_index/","text":"Macro Rendering Error \u00b6 TemplateSyntaxError : unexpected '%' Traceback (most recent call last): File \"/usr/local/lib/python3.9/site-packages/mkdocs_macros/plugin.py\", line 441, in render md_template = self.env.from_string(markdown) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 941, in from_string return cls.from_code(self, self.compile(source), globals, None) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 638, in compile self.handle_exception(source=source_hint) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 832, in handle_exception reraise(*rewrite_traceback_stack(source=source)) File \"/usr/local/lib/python3.9/site-packages/jinja2/_compat.py\", line 28, in reraise raise value.with_traceback(tb) File \"<unknown>\", line 3, in template jinja2.exceptions.TemplateSyntaxError: unexpected '%'","title":"Knative Eventing - Hello World app"},{"location":"eventing/samples/helloworld/_index/#macro-rendering-error","text":"TemplateSyntaxError : unexpected '%' Traceback (most recent call last): File \"/usr/local/lib/python3.9/site-packages/mkdocs_macros/plugin.py\", line 441, in render md_template = self.env.from_string(markdown) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 941, in from_string return cls.from_code(self, self.compile(source), globals, None) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 638, in compile self.handle_exception(source=source_hint) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 832, in handle_exception reraise(*rewrite_traceback_stack(source=source)) File \"/usr/local/lib/python3.9/site-packages/jinja2/_compat.py\", line 28, in reraise raise value.with_traceback(tb) File \"<unknown>\", line 3, in template jinja2.exceptions.TemplateSyntaxError: unexpected '%'","title":"Macro Rendering Error"},{"location":"eventing/samples/helloworld/helloworld-go/","text":"A simple web app written in Go that you can use to test knative eventing. It shows how to consume a CloudEvent in Knative eventing, and optionally how to respond back with another CloudEvent in the http response, using the Go SDK for CloudEvents We will deploy the app as a Kubernetes Deployment along with a Kubernetes Service . However, you can also deploy the app as a Knative Serving Service . Follow the steps below to create the sample code and then deploy the app to your cluster. You can also download a working copy of the sample, by running the following commands: git clone -b \"v0.21.x\" https://github.com/knative/docs knative-docs cd knative-docs/docs/eventing/samples/helloworld/helloworld-go Before you begin \u00b6 A Kubernetes cluster with Knative Eventing installed. Docker installed and running on your local machine, and a Docker Hub account configured (we'll use it for a container registry). Recreating the sample code \u00b6 Create a new file named helloworld.go and paste the following code. This code creates a basic web server which listens on port 8080: import ( \"context\" \"log\" cloudevents \"github.com/cloudevents/sdk-go/v2\" \"github.com/google/uuid\" ) func receive ( ctx context . Context , event cloudevents . Event ) ( * cloudevents . Event , cloudevents . Result ) { // Here is where your code to process the event will go. // In this example we will log the event msg log . Printf ( \"Event received. \\n%s\\n\" , event ) data := & HelloWorld {} if err := event . DataAs ( data ); err != nil { log . Printf ( \"Error while extracting cloudevent Data: %s\\n\" , err . Error ()) return nil , cloudevents . NewHTTPResult ( 400 , \"failed to convert data: %s\" , err ) } log . Printf ( \"Hello World Message from received event %q\" , data . Msg ) // Respond with another event (optional) // This is optional and is intended to show how to respond back with another event after processing. // The response will go back into the knative eventing system just like any other event newEvent := cloudevents . NewEvent () newEvent . SetID ( uuid . New (). String ()) newEvent . SetSource ( \"knative/eventing/samples/hello-world\" ) newEvent . SetType ( \"dev.knative.samples.hifromknative\" ) if err := newEvent . SetData ( cloudevents . ApplicationJSON , HiFromKnative { Msg : \"Hi from helloworld-go app!\" }); err != nil { return nil , cloudevents . NewHTTPResult ( 500 , \"failed to set response data: %s\" , err ) } log . Printf ( \"Responding with event\\n%s\\n\" , newEvent ) return & newEvent , nil } func main () { log . Print ( \"Hello world sample started.\" ) c , err := cloudevents . NewDefaultClient () if err != nil { log . Fatalf ( \"failed to create client, %v\" , err ) } log . Fatal ( c . StartReceiver ( context . Background (), receive )) } Create a new file named eventschemas.go and paste the following code. This defines the data schema of the CloudEvents. package main // HelloWorld defines the Data of CloudEvent with type=dev.knative.samples.helloworld type HelloWorld struct { // Msg holds the message from the event Msg string `json:\"msg,omitempty,string\"` } // HiFromKnative defines the Data of CloudEvent with type=dev.knative.samples.hifromknative type HiFromKnative struct { // Msg holds the message from the event Msg string `json:\"msg,omitempty,string\"` } In your project directory, create a file named Dockerfile and copy the code block below into it. For detailed instructions on dockerizing a Go app, see Deploying Go servers with Docker . # Use the official Golang image to create a build artifact. # This is based on Debian and sets the GOPATH to /go. # https://hub.docker.com/_/golang FROM golang:1.14 as builder # Copy local code to the container image. WORKDIR /app # Retrieve application dependencies using go modules. # Allows container builds to reuse downloaded dependencies. COPY go.* ./ RUN go mod download # Copy local code to the container image. COPY . ./ # Build the binary. # -mod = readonly ensures immutable go.mod and go.sum in container builds. RUN CGO_ENABLED = 0 GOOS = linux go build -mod = readonly -v -o helloworld # Use a Docker multi-stage build to create a lean production image. # https://docs.docker.com/develop/develop-images/multistage-build/#use-multi-stage-builds FROM alpine:3 RUN apk add --no-cache ca-certificates # Copy the binary to the production image from the builder stage. COPY --from = builder /app/helloworld /helloworld # Run the web service on container startup. CMD [ \"/helloworld\" ] Create a new file, sample-app.yaml and copy the following service definition into the file. Make sure to replace {username} with your Docker Hub username. # Namespace for sample application apiVersion : v1 kind : Namespace metadata : name : knative-samples --- # A default broker apiVersion : eventing.knative.dev/v1 kind : Broker metadata : name : default namespace : knative-samples spec : {} --- # Helloworld-go app deploment apiVersion : apps/v1 kind : Deployment metadata : name : helloworld-go namespace : knative-samples spec : replicas : 1 selector : matchLabels : &labels app : helloworld-go template : metadata : labels : *labels spec : containers : - name : helloworld-go image : docker.io/{username}/helloworld-go --- # Service that exposes helloworld-go app. # This will be the subscriber for the Trigger kind : Service apiVersion : v1 metadata : name : helloworld-go namespace : knative-samples spec : selector : app : helloworld-go ports : - protocol : TCP port : 80 targetPort : 8080 --- # Knative Eventing Trigger to trigger the helloworld-go service apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : helloworld-go namespace : knative-samples spec : broker : default filter : attributes : type : dev.knative.samples.helloworld source : dev.knative.samples/helloworldsource subscriber : ref : apiVersion : v1 kind : Service name : helloworld-go Use the go tool to create a go.mod manifest. go mod init github.com/knative/docs/docs/serving/samples/hello-world/helloworld-go Building and deploying the sample \u00b6 Once you have recreated the sample code files (or used the files in the sample folder) you're ready to build and deploy the sample app. Use Docker to build the sample code into a container. To build and push with Docker Hub, run these commands replacing {username} with your Docker Hub username: # Build the container on your local machine docker build -t { username } /helloworld-go . # Push the container to docker registry docker push { username } /helloworld-go After the build has completed and the container is pushed to docker hub, you can deploy the sample application into your cluster. Ensure that the container image value in sample-app.yaml matches the container you built in the previous step. Apply the configuration using kubectl : kubectl apply --filename sample-app.yaml Above command created a namespace knative-samples and create a default Broker it. Verify using the following command: kubectl get broker --namespace knative-samples Note: you can also use injection based on labels with the Eventing sugar controller. For how to install the Eventing sugar controller, see Install optional Eventing extensions . It deployed the helloworld-go app as a K8s Deployment and created a K8s service names helloworld-go. Verify using the following command. kubectl --namespace knative-samples get deployments helloworld-go kubectl --namespace knative-samples get svc helloworld-go It created a Knative Eventing Trigger to route certain events to the helloworld-go application. Make sure that Ready=true kubectl --namespace knative-samples get trigger helloworld-go Send and verify CloudEvents \u00b6 Once you have deployed the application and verified that the namespace, sample application and trigger are ready, let's send a CloudEvent. Send CloudEvent to the Broker \u00b6 We can send an http request directly to the Broker with correct CloudEvent headers set. Deploy a curl pod and SSH into it kubectl --namespace knative-samples run curl --image = radial/busyboxplus:curl -it Get the Broker URL kubectl --namespace knative-samples get broker default Run the following in the SSH terminal. Please replace the URL with the URL of the default broker. curl -v \"http://broker-ingress.knative-eventing.svc.cluster.local/knative-samples/default\" \\ -X POST \\ -H \"Ce-Id: 536808d3-88be-4077-9d7a-a3f162705f79\" \\ -H \"Ce-Specversion: 1.0\" \\ -H \"Ce-Type: dev.knative.samples.helloworld\" \\ -H \"Ce-Source: dev.knative.samples/helloworldsource\" \\ -H \"Content-Type: application/json\" \\ -d '{\"msg\":\"Hello World from the curl pod.\"}' exit Verify that event is received by helloworld-go app \u00b6 Helloworld-go app logs the context and the msg of the above event, and replies back with another event. Display helloworld-go app logs kubectl --namespace knative-samples logs -l app = helloworld-go --tail = 50 You should see something similar to: Event received. Validation: valid Context Attributes, specversion: 1 .0 type: dev.knative.samples.helloworld source: dev.knative.samples/helloworldsource id: 536808d3-88be-4077-9d7a-a3f162705f79 time: 2019 -10-04T22:35:26.05871736Z datacontenttype: application/json Extensions, knativearrivaltime: 2019 -10-04T22:35:26Z knativehistory: default-kn2-trigger-kn-channel.knative-samples.svc.cluster.local traceparent: 00 -971d4644229653483d38c46e92a959c7-92c66312e4bb39be-00 Data, { \"msg\" : \"Hello World from the curl pod.\" } Hello World Message \"Hello World from the curl pod.\" Responded with event Validation: valid Context Attributes, specversion: 1 .0 type: dev.knative.samples.hifromknative source: knative/eventing/samples/hello-world id: 37458d77-01f5-411e-a243-a459bbf79682 datacontenttype: application/json Data, { \"msg\" : \"Hi from Knative!\" } Play around with the CloudEvent attributes in the curl command and the trigger specification to understand how Triggers work . Verify reply from helloworld-go app \u00b6 helloworld-go app replies back with an event of type= dev.knative.samples.hifromknative , and source=knative/eventing/samples/hello-world . This event enters the eventing mesh via the Broker and can be delivered to other services using a Trigger Deploy a pod that receives any CloudEvent and logs the event to its output. kubectl --namespace knative-samples apply --filename - << END # event-display app deploment apiVersion: apps/v1 kind: Deployment metadata: name: event-display namespace: knative-samples spec: replicas: 1 selector: matchLabels: &labels app: event-display template: metadata: labels: *labels spec: containers: - name: helloworld-go # Source code: https://github.com/knative/eventing-contrib/tree/main/cmd/event_display image: gcr.io/knative-releases/knative.dev/eventing-contrib/cmd/event_display --- # Service that exposes event-display app. # This will be the subscriber for the Trigger kind: Service apiVersion: v1 metadata: name: event-display namespace: knative-samples spec: selector: app: event-display ports: - protocol: TCP port: 80 targetPort: 8080 END Create a trigger to deliver the event to the above service kubectl --namespace knative-samples apply --filename - << END apiVersion: eventing.knative.dev/v1 kind: Trigger metadata: name: event-display namespace: knative-samples spec: broker: default filter: attributes: type: dev.knative.samples.hifromknative source: knative/eventing/samples/hello-world subscriber: ref: apiVersion: v1 kind: Service name: event-display END Send a CloudEvent to the Broker Check the logs of event-display service kubectl --namespace knative-samples logs -l app = event-display --tail = 50 You should see something similar to: cloudevents.Event Validation: valid Context Attributes, specversion: 1 .0 type: dev.knative.samples.hifromknative source: knative/eventing/samples/hello-world id: 8a7384b9-8bbe-4634-bf0f-ead07e450b2a time: 2019 -10-04T22:53:39.844943931Z datacontenttype: application/json Extensions, knativearrivaltime: 2019 -10-04T22:53:39Z knativehistory: default-kn2-ingress-kn-channel.knative-samples.svc.cluster.local traceparent: 00 -4b01db030b9ea04bb150b77c8fa86509-2740816590a7604f-00 Data, { \"msg\" : \"Hi from helloworld-go app!\" } Note: You could use the above approach to test your applications too. Removing the sample app deployment \u00b6 To remove the sample app from your cluster, delete the service record: kubectl delete --filename sample-app.yaml","title":"Go"},{"location":"eventing/samples/helloworld/helloworld-go/#before-you-begin","text":"A Kubernetes cluster with Knative Eventing installed. Docker installed and running on your local machine, and a Docker Hub account configured (we'll use it for a container registry).","title":"Before you begin"},{"location":"eventing/samples/helloworld/helloworld-go/#recreating-the-sample-code","text":"Create a new file named helloworld.go and paste the following code. This code creates a basic web server which listens on port 8080: import ( \"context\" \"log\" cloudevents \"github.com/cloudevents/sdk-go/v2\" \"github.com/google/uuid\" ) func receive ( ctx context . Context , event cloudevents . Event ) ( * cloudevents . Event , cloudevents . Result ) { // Here is where your code to process the event will go. // In this example we will log the event msg log . Printf ( \"Event received. \\n%s\\n\" , event ) data := & HelloWorld {} if err := event . DataAs ( data ); err != nil { log . Printf ( \"Error while extracting cloudevent Data: %s\\n\" , err . Error ()) return nil , cloudevents . NewHTTPResult ( 400 , \"failed to convert data: %s\" , err ) } log . Printf ( \"Hello World Message from received event %q\" , data . Msg ) // Respond with another event (optional) // This is optional and is intended to show how to respond back with another event after processing. // The response will go back into the knative eventing system just like any other event newEvent := cloudevents . NewEvent () newEvent . SetID ( uuid . New (). String ()) newEvent . SetSource ( \"knative/eventing/samples/hello-world\" ) newEvent . SetType ( \"dev.knative.samples.hifromknative\" ) if err := newEvent . SetData ( cloudevents . ApplicationJSON , HiFromKnative { Msg : \"Hi from helloworld-go app!\" }); err != nil { return nil , cloudevents . NewHTTPResult ( 500 , \"failed to set response data: %s\" , err ) } log . Printf ( \"Responding with event\\n%s\\n\" , newEvent ) return & newEvent , nil } func main () { log . Print ( \"Hello world sample started.\" ) c , err := cloudevents . NewDefaultClient () if err != nil { log . Fatalf ( \"failed to create client, %v\" , err ) } log . Fatal ( c . StartReceiver ( context . Background (), receive )) } Create a new file named eventschemas.go and paste the following code. This defines the data schema of the CloudEvents. package main // HelloWorld defines the Data of CloudEvent with type=dev.knative.samples.helloworld type HelloWorld struct { // Msg holds the message from the event Msg string `json:\"msg,omitempty,string\"` } // HiFromKnative defines the Data of CloudEvent with type=dev.knative.samples.hifromknative type HiFromKnative struct { // Msg holds the message from the event Msg string `json:\"msg,omitempty,string\"` } In your project directory, create a file named Dockerfile and copy the code block below into it. For detailed instructions on dockerizing a Go app, see Deploying Go servers with Docker . # Use the official Golang image to create a build artifact. # This is based on Debian and sets the GOPATH to /go. # https://hub.docker.com/_/golang FROM golang:1.14 as builder # Copy local code to the container image. WORKDIR /app # Retrieve application dependencies using go modules. # Allows container builds to reuse downloaded dependencies. COPY go.* ./ RUN go mod download # Copy local code to the container image. COPY . ./ # Build the binary. # -mod = readonly ensures immutable go.mod and go.sum in container builds. RUN CGO_ENABLED = 0 GOOS = linux go build -mod = readonly -v -o helloworld # Use a Docker multi-stage build to create a lean production image. # https://docs.docker.com/develop/develop-images/multistage-build/#use-multi-stage-builds FROM alpine:3 RUN apk add --no-cache ca-certificates # Copy the binary to the production image from the builder stage. COPY --from = builder /app/helloworld /helloworld # Run the web service on container startup. CMD [ \"/helloworld\" ] Create a new file, sample-app.yaml and copy the following service definition into the file. Make sure to replace {username} with your Docker Hub username. # Namespace for sample application apiVersion : v1 kind : Namespace metadata : name : knative-samples --- # A default broker apiVersion : eventing.knative.dev/v1 kind : Broker metadata : name : default namespace : knative-samples spec : {} --- # Helloworld-go app deploment apiVersion : apps/v1 kind : Deployment metadata : name : helloworld-go namespace : knative-samples spec : replicas : 1 selector : matchLabels : &labels app : helloworld-go template : metadata : labels : *labels spec : containers : - name : helloworld-go image : docker.io/{username}/helloworld-go --- # Service that exposes helloworld-go app. # This will be the subscriber for the Trigger kind : Service apiVersion : v1 metadata : name : helloworld-go namespace : knative-samples spec : selector : app : helloworld-go ports : - protocol : TCP port : 80 targetPort : 8080 --- # Knative Eventing Trigger to trigger the helloworld-go service apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : helloworld-go namespace : knative-samples spec : broker : default filter : attributes : type : dev.knative.samples.helloworld source : dev.knative.samples/helloworldsource subscriber : ref : apiVersion : v1 kind : Service name : helloworld-go Use the go tool to create a go.mod manifest. go mod init github.com/knative/docs/docs/serving/samples/hello-world/helloworld-go","title":"Recreating the sample code"},{"location":"eventing/samples/helloworld/helloworld-go/#building-and-deploying-the-sample","text":"Once you have recreated the sample code files (or used the files in the sample folder) you're ready to build and deploy the sample app. Use Docker to build the sample code into a container. To build and push with Docker Hub, run these commands replacing {username} with your Docker Hub username: # Build the container on your local machine docker build -t { username } /helloworld-go . # Push the container to docker registry docker push { username } /helloworld-go After the build has completed and the container is pushed to docker hub, you can deploy the sample application into your cluster. Ensure that the container image value in sample-app.yaml matches the container you built in the previous step. Apply the configuration using kubectl : kubectl apply --filename sample-app.yaml Above command created a namespace knative-samples and create a default Broker it. Verify using the following command: kubectl get broker --namespace knative-samples Note: you can also use injection based on labels with the Eventing sugar controller. For how to install the Eventing sugar controller, see Install optional Eventing extensions . It deployed the helloworld-go app as a K8s Deployment and created a K8s service names helloworld-go. Verify using the following command. kubectl --namespace knative-samples get deployments helloworld-go kubectl --namespace knative-samples get svc helloworld-go It created a Knative Eventing Trigger to route certain events to the helloworld-go application. Make sure that Ready=true kubectl --namespace knative-samples get trigger helloworld-go","title":"Building and deploying the sample"},{"location":"eventing/samples/helloworld/helloworld-go/#send-and-verify-cloudevents","text":"Once you have deployed the application and verified that the namespace, sample application and trigger are ready, let's send a CloudEvent.","title":"Send and verify CloudEvents"},{"location":"eventing/samples/helloworld/helloworld-go/#send-cloudevent-to-the-broker","text":"We can send an http request directly to the Broker with correct CloudEvent headers set. Deploy a curl pod and SSH into it kubectl --namespace knative-samples run curl --image = radial/busyboxplus:curl -it Get the Broker URL kubectl --namespace knative-samples get broker default Run the following in the SSH terminal. Please replace the URL with the URL of the default broker. curl -v \"http://broker-ingress.knative-eventing.svc.cluster.local/knative-samples/default\" \\ -X POST \\ -H \"Ce-Id: 536808d3-88be-4077-9d7a-a3f162705f79\" \\ -H \"Ce-Specversion: 1.0\" \\ -H \"Ce-Type: dev.knative.samples.helloworld\" \\ -H \"Ce-Source: dev.knative.samples/helloworldsource\" \\ -H \"Content-Type: application/json\" \\ -d '{\"msg\":\"Hello World from the curl pod.\"}' exit","title":"Send CloudEvent to the Broker"},{"location":"eventing/samples/helloworld/helloworld-go/#verify-that-event-is-received-by-helloworld-go-app","text":"Helloworld-go app logs the context and the msg of the above event, and replies back with another event. Display helloworld-go app logs kubectl --namespace knative-samples logs -l app = helloworld-go --tail = 50 You should see something similar to: Event received. Validation: valid Context Attributes, specversion: 1 .0 type: dev.knative.samples.helloworld source: dev.knative.samples/helloworldsource id: 536808d3-88be-4077-9d7a-a3f162705f79 time: 2019 -10-04T22:35:26.05871736Z datacontenttype: application/json Extensions, knativearrivaltime: 2019 -10-04T22:35:26Z knativehistory: default-kn2-trigger-kn-channel.knative-samples.svc.cluster.local traceparent: 00 -971d4644229653483d38c46e92a959c7-92c66312e4bb39be-00 Data, { \"msg\" : \"Hello World from the curl pod.\" } Hello World Message \"Hello World from the curl pod.\" Responded with event Validation: valid Context Attributes, specversion: 1 .0 type: dev.knative.samples.hifromknative source: knative/eventing/samples/hello-world id: 37458d77-01f5-411e-a243-a459bbf79682 datacontenttype: application/json Data, { \"msg\" : \"Hi from Knative!\" } Play around with the CloudEvent attributes in the curl command and the trigger specification to understand how Triggers work .","title":"Verify that event is received by helloworld-go app"},{"location":"eventing/samples/helloworld/helloworld-go/#verify-reply-from-helloworld-go-app","text":"helloworld-go app replies back with an event of type= dev.knative.samples.hifromknative , and source=knative/eventing/samples/hello-world . This event enters the eventing mesh via the Broker and can be delivered to other services using a Trigger Deploy a pod that receives any CloudEvent and logs the event to its output. kubectl --namespace knative-samples apply --filename - << END # event-display app deploment apiVersion: apps/v1 kind: Deployment metadata: name: event-display namespace: knative-samples spec: replicas: 1 selector: matchLabels: &labels app: event-display template: metadata: labels: *labels spec: containers: - name: helloworld-go # Source code: https://github.com/knative/eventing-contrib/tree/main/cmd/event_display image: gcr.io/knative-releases/knative.dev/eventing-contrib/cmd/event_display --- # Service that exposes event-display app. # This will be the subscriber for the Trigger kind: Service apiVersion: v1 metadata: name: event-display namespace: knative-samples spec: selector: app: event-display ports: - protocol: TCP port: 80 targetPort: 8080 END Create a trigger to deliver the event to the above service kubectl --namespace knative-samples apply --filename - << END apiVersion: eventing.knative.dev/v1 kind: Trigger metadata: name: event-display namespace: knative-samples spec: broker: default filter: attributes: type: dev.knative.samples.hifromknative source: knative/eventing/samples/hello-world subscriber: ref: apiVersion: v1 kind: Service name: event-display END Send a CloudEvent to the Broker Check the logs of event-display service kubectl --namespace knative-samples logs -l app = event-display --tail = 50 You should see something similar to: cloudevents.Event Validation: valid Context Attributes, specversion: 1 .0 type: dev.knative.samples.hifromknative source: knative/eventing/samples/hello-world id: 8a7384b9-8bbe-4634-bf0f-ead07e450b2a time: 2019 -10-04T22:53:39.844943931Z datacontenttype: application/json Extensions, knativearrivaltime: 2019 -10-04T22:53:39Z knativehistory: default-kn2-ingress-kn-channel.knative-samples.svc.cluster.local traceparent: 00 -4b01db030b9ea04bb150b77c8fa86509-2740816590a7604f-00 Data, { \"msg\" : \"Hi from helloworld-go app!\" } Note: You could use the above approach to test your applications too.","title":"Verify reply from helloworld-go app"},{"location":"eventing/samples/helloworld/helloworld-go/#removing-the-sample-app-deployment","text":"To remove the sample app from your cluster, delete the service record: kubectl delete --filename sample-app.yaml","title":"Removing the sample app deployment"},{"location":"eventing/samples/helloworld/helloworld-go/_index/","text":"Macro Rendering Error \u00b6 TemplateSyntaxError : unexpected '%' Traceback (most recent call last): File \"/usr/local/lib/python3.9/site-packages/mkdocs_macros/plugin.py\", line 441, in render md_template = self.env.from_string(markdown) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 941, in from_string return cls.from_code(self, self.compile(source), globals, None) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 638, in compile self.handle_exception(source=source_hint) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 832, in handle_exception reraise(*rewrite_traceback_stack(source=source)) File \"/usr/local/lib/python3.9/site-packages/jinja2/_compat.py\", line 28, in reraise raise value.with_traceback(tb) File \"<unknown>\", line 3, in template jinja2.exceptions.TemplateSyntaxError: unexpected '%'","title":"Hello World - Golang"},{"location":"eventing/samples/helloworld/helloworld-go/_index/#macro-rendering-error","text":"TemplateSyntaxError : unexpected '%' Traceback (most recent call last): File \"/usr/local/lib/python3.9/site-packages/mkdocs_macros/plugin.py\", line 441, in render md_template = self.env.from_string(markdown) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 941, in from_string return cls.from_code(self, self.compile(source), globals, None) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 638, in compile self.handle_exception(source=source_hint) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 832, in handle_exception reraise(*rewrite_traceback_stack(source=source)) File \"/usr/local/lib/python3.9/site-packages/jinja2/_compat.py\", line 28, in reraise raise value.with_traceback(tb) File \"<unknown>\", line 3, in template jinja2.exceptions.TemplateSyntaxError: unexpected '%'","title":"Macro Rendering Error"},{"location":"eventing/samples/helloworld/helloworld-python/","text":"A simple web app written in Python that you can use to test knative eventing. It shows how to consume a CloudEvent in Knative eventing, and optionally how to respond back with another CloudEvent in the http response, by adding the Cloud Eventing headers outlined in the Cloud Events standard definition. We will deploy the app as a Kubernetes Deployment along with a Kubernetes Service . However, you can also deploy the app as a Knative Serving Service . Follow the steps below to create the sample code and then deploy the app to your cluster. You can also download a working copy of the sample, by running the following commands: # Clone the relevant branch version such as \"release-0.13\" git clone -b \"v0.21.x\" https://github.com/knative/docs knative-docs cd knative-docs/docs/eventing/samples/helloworld/helloworld-python Before you begin \u00b6 A Kubernetes cluster with Knative Eventing installed. Docker installed and running on your local machine, and a Docker Hub account configured (we'll use it for a container registry). Recreating the sample code \u00b6 Create a new file named helloworld.py and paste the following code. This code creates a basic web server which listens on port 8080: from flask import Flask , request , make_response import uuid app = Flask ( __name__ ) @app . route ( '/' , methods = [ 'POST' ]) def hello_world (): app . logger . warning ( request . data ) # Respond with another event (optional) response = make_response ({ \"msg\" : \"Hi from helloworld-python app!\" }) response . headers [ \"Ce-Id\" ] = str ( uuid . uuid4 ()) response . headers [ \"Ce-specversion\" ] = \"0.3\" response . headers [ \"Ce-Source\" ] = \"knative/eventing/samples/hello-world\" response . headers [ \"Ce-Type\" ] = \"dev.knative.samples.hifromknative\" return response if __name__ == '__main__' : app . run ( debug = True , host = '0.0.0.0' , port = 8080 ) Add a requirements.txt file containing the following contents: Flask == 1 .1.1 1. In your project directory, create a file named Dockerfile and copy the code block below into it. For detailed instructions on dockerizing a Go app, see Deploying Go servers with Docker . FROM python:alpine3.7 COPY . /app WORKDIR /app RUN pip install -r requirements.txt EXPOSE 8080 ENTRYPOINT [ \"python\" ] CMD [ \"helloworld.py\" ] Create a new file, sample-app.yaml and copy the following service definition into the file. Make sure to replace {username} with your Docker Hub username. # Namespace for sample application with eventing enabled apiVersion : v1 kind : Namespace metadata : name : knative-samples labels : eventing.knative.dev/injection : enabled --- # Helloworld-python app deploment apiVersion : apps/v1 kind : Deployment metadata : name : helloworld-python namespace : knative-samples spec : replicas : 1 selector : matchLabels : &labels app : helloworld-python template : metadata : labels : *labels spec : containers : - name : helloworld-python image : docker.io/{username}/helloworld-python imagePullPolicy : IfNotPresent --- # Service that exposes helloworld-python app. # This will be the subscriber for the Trigger kind : Service apiVersion : v1 metadata : name : helloworld-python namespace : knative-samples spec : selector : app : helloworld-python ports : - protocol : TCP port : 80 targetPort : 8080 --- # Knative Eventing Trigger to trigger the helloworld-python service apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : helloworld-python namespace : knative-samples spec : broker : default filter : attributes : type : dev.knative.samples.helloworld source : dev.knative.samples/helloworldsource subscriber : ref : apiVersion : v1 kind : Service name : helloworld-python Building and deploying the sample \u00b6 Once you have recreated the sample code files (or used the files in the sample folder) you're ready to build and deploy the sample app. Use Docker to build the sample code into a container. To build and push with Docker Hub, run these commands replacing {username} with your Docker Hub username: # Build the container on your local machine docker build -t { username } /helloworld-python . # Push the container to docker registry docker push { username } /helloworld-python After the build has completed and the container is pushed to Docker Hub, you can deploy the sample application into your cluster. Ensure that the container image value in sample-app.yaml matches the container you built in the previous step. Apply the configuration using kubectl : kubectl apply --filename sample-app.yaml 1. Above command created a namespace knative-samples and labelled it with knative-eventing-injection=enabled , to enable eventing in the namespace. Verify using the following command: kubectl get ns knative-samples --show-labels 1. It deployed the helloworld-python app as a K8s Deployment and created a K8s service names helloworld-python. Verify using the following command. kubectl --namespace knative-samples get deployments helloworld-python kubectl --namespace knative-samples get svc helloworld-python 1. It created a Knative Eventing Trigger to route certain events to the helloworld-python application. Make sure that Ready=true kubectl --namespace knative-samples get trigger helloworld-python Send and verify CloudEvents \u00b6 After you have deployed the application, and have verified that the namespace, sample application and trigger are ready, you can send a CloudEvent. Send CloudEvent to the Broker \u00b6 You can send an HTTP request directly to the Knative broker if the correct CloudEvent headers are set. Deploy a curl pod and SSH into it kubectl --namespace knative-samples run curl --image = radial/busyboxplus:curl -it Run the following in the SSH terminal curl -v \"default-broker.knative-samples.svc.cluster.local\" \\ -X POST \\ -H \"Ce-Id: 536808d3-88be-4077-9d7a-a3f162705f79\" \\ -H \"Ce-specversion: 0.3\" \\ -H \"Ce-Type: dev.knative.samples.helloworld\" \\ -H \"Ce-Source: dev.knative.samples/helloworldsource\" \\ -H \"Content-Type: application/json\" \\ -d '{\"msg\":\"Hello World from the curl pod.\"}' exit Verify that event is received by helloworld-python app \u00b6 Helloworld-python app logs the context and the msg of the above event, and replies back with another event. 1. Display helloworld-python app logs kubectl --namespace knative-samples logs -l app = helloworld-python --tail = 50 You should see something similar to: Event received. Context: Context Attributes, specversion: 0 .3 type: dev.knative.samples.helloworld source: dev.knative.samples/helloworldsource id: 536808d3-88be-4077-9d7a-a3f162705f79 time: 2019 -10-04T22:35:26.05871736Z datacontenttype: application/json Extensions, knativearrivaltime: 2019 -10-04T22:35:26Z knativehistory: default-kn2-trigger-kn-channel.knative-samples.svc.cluster.local traceparent: 00 -971d4644229653483d38c46e92a959c7-92c66312e4bb39be-00 Hello World Message \"Hello World from the curl pod.\" Responded with event Validation: valid Context Attributes, specversion: 0 .2 type: dev.knative.samples.hifromknative source: knative/eventing/samples/hello-world id: 37458d77-01f5-411e-a243-a459bbf79682 Data, { \"msg\" : \"Hi from Knative!\" } Try the CloudEvent attributes in the curl command and the trigger specification to understand how triggers work. Verify reply from helloworld-python app \u00b6 The helloworld-python app replies with an event type type= dev.knative.samples.hifromknative , and source source=knative/eventing/samples/hello-world . The event enters the eventing mesh through the broker, and can be delivered to event sinks using a trigger Deploy a pod that receives any CloudEvent and logs the event to its output. kubectl --namespace knative-samples apply --filename - << END # event-display app deploment apiVersion: apps/v1 kind: Deployment metadata: name: event-display namespace: knative-samples spec: replicas: 1 selector: matchLabels: &labels app: event-display template: metadata: labels: *labels spec: containers: - name: helloworld-python image: gcr.io/knative-releases/knative.dev/eventing-contrib/cmd/event_display --- # Service that exposes event-display app. # This will be the subscriber for the Trigger kind: Service apiVersion: v1 metadata: name: event-display namespace: knative-samples spec: selector: app: event-display ports: - protocol: TCP port: 80 targetPort: 8080 END Create a trigger to deliver the event to the above service kubectl --namespace knative-samples apply --filename - << END apiVersion: eventing.knative.dev/v1 kind: Trigger metadata: name: event-display namespace: knative-samples spec: broker: default filter: attributes: type: dev.knative.samples.hifromknative source: knative/eventing/samples/hello-world subscriber: ref: apiVersion: v1 kind: Service name: event-display END Send a CloudEvent to the Broker Check the logs of event-display service kubectl --namespace knative-samples logs -l app = event-display --tail = 50 You should see something similar to: cloudevents.Event Validation: valid Context Attributes, specversion: 0 .3 type: dev.knative.samples.hifromknative source: knative/eventing/samples/hello-world id: 8a7384b9-8bbe-4634-bf0f-ead07e450b2a time: 2019 -10-04T22:53:39.844943931Z datacontenttype: application/json Extensions, knativearrivaltime: 2019 -10-04T22:53:39Z knativehistory: default-kn2-ingress-kn-channel.knative-samples.svc.cluster.local traceparent: 00 -4b01db030b9ea04bb150b77c8fa86509-2740816590a7604f-00 Data, { \"msg\" : \"Hi from helloworld- app!\" } Note: You could use the above approach to test your applications too. Removing the sample app deployment \u00b6 To remove the sample app from your cluster, delete the service record: kubectl delete --filename sample-app.yaml","title":"Python"},{"location":"eventing/samples/helloworld/helloworld-python/#before-you-begin","text":"A Kubernetes cluster with Knative Eventing installed. Docker installed and running on your local machine, and a Docker Hub account configured (we'll use it for a container registry).","title":"Before you begin"},{"location":"eventing/samples/helloworld/helloworld-python/#recreating-the-sample-code","text":"Create a new file named helloworld.py and paste the following code. This code creates a basic web server which listens on port 8080: from flask import Flask , request , make_response import uuid app = Flask ( __name__ ) @app . route ( '/' , methods = [ 'POST' ]) def hello_world (): app . logger . warning ( request . data ) # Respond with another event (optional) response = make_response ({ \"msg\" : \"Hi from helloworld-python app!\" }) response . headers [ \"Ce-Id\" ] = str ( uuid . uuid4 ()) response . headers [ \"Ce-specversion\" ] = \"0.3\" response . headers [ \"Ce-Source\" ] = \"knative/eventing/samples/hello-world\" response . headers [ \"Ce-Type\" ] = \"dev.knative.samples.hifromknative\" return response if __name__ == '__main__' : app . run ( debug = True , host = '0.0.0.0' , port = 8080 ) Add a requirements.txt file containing the following contents: Flask == 1 .1.1 1. In your project directory, create a file named Dockerfile and copy the code block below into it. For detailed instructions on dockerizing a Go app, see Deploying Go servers with Docker . FROM python:alpine3.7 COPY . /app WORKDIR /app RUN pip install -r requirements.txt EXPOSE 8080 ENTRYPOINT [ \"python\" ] CMD [ \"helloworld.py\" ] Create a new file, sample-app.yaml and copy the following service definition into the file. Make sure to replace {username} with your Docker Hub username. # Namespace for sample application with eventing enabled apiVersion : v1 kind : Namespace metadata : name : knative-samples labels : eventing.knative.dev/injection : enabled --- # Helloworld-python app deploment apiVersion : apps/v1 kind : Deployment metadata : name : helloworld-python namespace : knative-samples spec : replicas : 1 selector : matchLabels : &labels app : helloworld-python template : metadata : labels : *labels spec : containers : - name : helloworld-python image : docker.io/{username}/helloworld-python imagePullPolicy : IfNotPresent --- # Service that exposes helloworld-python app. # This will be the subscriber for the Trigger kind : Service apiVersion : v1 metadata : name : helloworld-python namespace : knative-samples spec : selector : app : helloworld-python ports : - protocol : TCP port : 80 targetPort : 8080 --- # Knative Eventing Trigger to trigger the helloworld-python service apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : helloworld-python namespace : knative-samples spec : broker : default filter : attributes : type : dev.knative.samples.helloworld source : dev.knative.samples/helloworldsource subscriber : ref : apiVersion : v1 kind : Service name : helloworld-python","title":"Recreating the sample code"},{"location":"eventing/samples/helloworld/helloworld-python/#building-and-deploying-the-sample","text":"Once you have recreated the sample code files (or used the files in the sample folder) you're ready to build and deploy the sample app. Use Docker to build the sample code into a container. To build and push with Docker Hub, run these commands replacing {username} with your Docker Hub username: # Build the container on your local machine docker build -t { username } /helloworld-python . # Push the container to docker registry docker push { username } /helloworld-python After the build has completed and the container is pushed to Docker Hub, you can deploy the sample application into your cluster. Ensure that the container image value in sample-app.yaml matches the container you built in the previous step. Apply the configuration using kubectl : kubectl apply --filename sample-app.yaml 1. Above command created a namespace knative-samples and labelled it with knative-eventing-injection=enabled , to enable eventing in the namespace. Verify using the following command: kubectl get ns knative-samples --show-labels 1. It deployed the helloworld-python app as a K8s Deployment and created a K8s service names helloworld-python. Verify using the following command. kubectl --namespace knative-samples get deployments helloworld-python kubectl --namespace knative-samples get svc helloworld-python 1. It created a Knative Eventing Trigger to route certain events to the helloworld-python application. Make sure that Ready=true kubectl --namespace knative-samples get trigger helloworld-python","title":"Building and deploying the sample"},{"location":"eventing/samples/helloworld/helloworld-python/#send-and-verify-cloudevents","text":"After you have deployed the application, and have verified that the namespace, sample application and trigger are ready, you can send a CloudEvent.","title":"Send and verify CloudEvents"},{"location":"eventing/samples/helloworld/helloworld-python/#send-cloudevent-to-the-broker","text":"You can send an HTTP request directly to the Knative broker if the correct CloudEvent headers are set. Deploy a curl pod and SSH into it kubectl --namespace knative-samples run curl --image = radial/busyboxplus:curl -it Run the following in the SSH terminal curl -v \"default-broker.knative-samples.svc.cluster.local\" \\ -X POST \\ -H \"Ce-Id: 536808d3-88be-4077-9d7a-a3f162705f79\" \\ -H \"Ce-specversion: 0.3\" \\ -H \"Ce-Type: dev.knative.samples.helloworld\" \\ -H \"Ce-Source: dev.knative.samples/helloworldsource\" \\ -H \"Content-Type: application/json\" \\ -d '{\"msg\":\"Hello World from the curl pod.\"}' exit","title":"Send CloudEvent to the Broker"},{"location":"eventing/samples/helloworld/helloworld-python/#verify-that-event-is-received-by-helloworld-python-app","text":"Helloworld-python app logs the context and the msg of the above event, and replies back with another event. 1. Display helloworld-python app logs kubectl --namespace knative-samples logs -l app = helloworld-python --tail = 50 You should see something similar to: Event received. Context: Context Attributes, specversion: 0 .3 type: dev.knative.samples.helloworld source: dev.knative.samples/helloworldsource id: 536808d3-88be-4077-9d7a-a3f162705f79 time: 2019 -10-04T22:35:26.05871736Z datacontenttype: application/json Extensions, knativearrivaltime: 2019 -10-04T22:35:26Z knativehistory: default-kn2-trigger-kn-channel.knative-samples.svc.cluster.local traceparent: 00 -971d4644229653483d38c46e92a959c7-92c66312e4bb39be-00 Hello World Message \"Hello World from the curl pod.\" Responded with event Validation: valid Context Attributes, specversion: 0 .2 type: dev.knative.samples.hifromknative source: knative/eventing/samples/hello-world id: 37458d77-01f5-411e-a243-a459bbf79682 Data, { \"msg\" : \"Hi from Knative!\" } Try the CloudEvent attributes in the curl command and the trigger specification to understand how triggers work.","title":"Verify that event is received by helloworld-python app"},{"location":"eventing/samples/helloworld/helloworld-python/#verify-reply-from-helloworld-python-app","text":"The helloworld-python app replies with an event type type= dev.knative.samples.hifromknative , and source source=knative/eventing/samples/hello-world . The event enters the eventing mesh through the broker, and can be delivered to event sinks using a trigger Deploy a pod that receives any CloudEvent and logs the event to its output. kubectl --namespace knative-samples apply --filename - << END # event-display app deploment apiVersion: apps/v1 kind: Deployment metadata: name: event-display namespace: knative-samples spec: replicas: 1 selector: matchLabels: &labels app: event-display template: metadata: labels: *labels spec: containers: - name: helloworld-python image: gcr.io/knative-releases/knative.dev/eventing-contrib/cmd/event_display --- # Service that exposes event-display app. # This will be the subscriber for the Trigger kind: Service apiVersion: v1 metadata: name: event-display namespace: knative-samples spec: selector: app: event-display ports: - protocol: TCP port: 80 targetPort: 8080 END Create a trigger to deliver the event to the above service kubectl --namespace knative-samples apply --filename - << END apiVersion: eventing.knative.dev/v1 kind: Trigger metadata: name: event-display namespace: knative-samples spec: broker: default filter: attributes: type: dev.knative.samples.hifromknative source: knative/eventing/samples/hello-world subscriber: ref: apiVersion: v1 kind: Service name: event-display END Send a CloudEvent to the Broker Check the logs of event-display service kubectl --namespace knative-samples logs -l app = event-display --tail = 50 You should see something similar to: cloudevents.Event Validation: valid Context Attributes, specversion: 0 .3 type: dev.knative.samples.hifromknative source: knative/eventing/samples/hello-world id: 8a7384b9-8bbe-4634-bf0f-ead07e450b2a time: 2019 -10-04T22:53:39.844943931Z datacontenttype: application/json Extensions, knativearrivaltime: 2019 -10-04T22:53:39Z knativehistory: default-kn2-ingress-kn-channel.knative-samples.svc.cluster.local traceparent: 00 -4b01db030b9ea04bb150b77c8fa86509-2740816590a7604f-00 Data, { \"msg\" : \"Hi from helloworld- app!\" } Note: You could use the above approach to test your applications too.","title":"Verify reply from helloworld-python app"},{"location":"eventing/samples/helloworld/helloworld-python/#removing-the-sample-app-deployment","text":"To remove the sample app from your cluster, delete the service record: kubectl delete --filename sample-app.yaml","title":"Removing the sample app deployment"},{"location":"eventing/samples/helloworld/helloworld-python/_index/","text":"Macro Rendering Error \u00b6 TemplateSyntaxError : unexpected '%' Traceback (most recent call last): File \"/usr/local/lib/python3.9/site-packages/mkdocs_macros/plugin.py\", line 441, in render md_template = self.env.from_string(markdown) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 941, in from_string return cls.from_code(self, self.compile(source), globals, None) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 638, in compile self.handle_exception(source=source_hint) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 832, in handle_exception reraise(*rewrite_traceback_stack(source=source)) File \"/usr/local/lib/python3.9/site-packages/jinja2/_compat.py\", line 28, in reraise raise value.with_traceback(tb) File \"<unknown>\", line 3, in template jinja2.exceptions.TemplateSyntaxError: unexpected '%'","title":"Hello World - Python"},{"location":"eventing/samples/helloworld/helloworld-python/_index/#macro-rendering-error","text":"TemplateSyntaxError : unexpected '%' Traceback (most recent call last): File \"/usr/local/lib/python3.9/site-packages/mkdocs_macros/plugin.py\", line 441, in render md_template = self.env.from_string(markdown) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 941, in from_string return cls.from_code(self, self.compile(source), globals, None) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 638, in compile self.handle_exception(source=source_hint) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 832, in handle_exception reraise(*rewrite_traceback_stack(source=source)) File \"/usr/local/lib/python3.9/site-packages/jinja2/_compat.py\", line 28, in reraise raise value.with_traceback(tb) File \"<unknown>\", line 3, in template jinja2.exceptions.TemplateSyntaxError: unexpected '%'","title":"Macro Rendering Error"},{"location":"eventing/samples/iot-core/","text":"This sample shows how to bind a running service to an IoT core using GCP PubSub as the event source. With minor modifications, it can be used to bind a running service to anything that sends events via GCP PubSub. Note: All commands are given relative to the root of this repository. Deployment Steps \u00b6 Environment Variables \u00b6 To make the following commands easier, we are going to set the various variables here and use them later. Variables you must Change \u00b6 export IOTCORE_PROJECT = \"s9-demo\" Variables you may Change \u00b6 export IOTCORE_REGISTRY = \"iot-demo\" export IOTCORE_DEVICE = \"iot-demo-client\" export IOTCORE_REGION = \"us-central1\" export IOTCORE_TOPIC_DATA = \"iot-demo-pubsub-topic\" export IOTCORE_TOPIC_DEVICE = \"iot-demo-device-pubsub-topic\" Prerequisites \u00b6 Kubernetes \u00b6 Have a running Kubernetes cluster with kubectl pointing at it. GCP \u00b6 Create a Google Cloud Project . Have gcloud installed and pointing at that project. Enable the Cloud Pub/Sub API on that project. gcloud services enable pubsub.googleapis.com Create the two GCP PubSub topic s. gcloud pubsub topics create $IOTCORE_TOPIC_DATA gcloud pubsub topics create $IOTCORE_TOPIC_DEVICE Setup Knative Eventing . GCP PubSub Source \u00b6 Create a GCP Service Account . Determine the Service Account to use, or create a new one. Give that Service Account the 'Pub/Sub Editor' role on your GCP project. Download a new JSON private key for that Service Account. Create two secrets with the downloaded key (one for the Source, one for the Receive Adapter): kubectl --namespace knative-sources create secret generic gcppubsub-source-key --from-file = key.json = PATH_TO_KEY_FILE.json kubectl --namespace default create secret generic google-cloud-key --from-file = key.json = PATH_TO_KEY_FILE.json Deploy the GcpPubSubSource controller as part of eventing-source's controller. kubectl apply --filename https://github.com/knative/eventing-contrib/releases/download/v0.8.2/gcppubsub.yaml Deploying \u00b6 Broker \u00b6 Install the default Broker . kubectl create -f - <<EOF apiVersion: eventing.knative.dev/v1 kind: Broker metadata: name: default EOF GCP PubSub Source \u00b6 Deploy gcp-pubsub-source.yaml . sed -e \"s/PROJECT_ID/ $IOTCORE_PROJECT /\" \\ -e \"s/TOPIC_NAME/ $IOTCORE_TOPIC_DATA /\" \\ docs/eventing/samples/iot-core/gcp-pubsub-source.yaml | kubectl apply --filename - Trigger \u00b6 Even though the Source isn't completely ready yet, we can setup the Trigger for all events coming out of it. Deploy trigger.yaml . kubectl apply --filename docs/eventing/samples/iot-core/trigger.yaml This uses a very simple Knative Service to see that events are flowing. Feel free to replace it. IoT Core \u00b6 We now have everything setup on the Knative side. We will now setup the IoT Core. Create a device registry: gcloud iot registries create $IOTCORE_REGISTRY \\ --project = $IOTCORE_PROJECT \\ --region = $IOTCORE_REGION \\ --event-notification-config = topic = $IOTCORE_TOPIC_DATA \\ --state-pubsub-topic = $IOTCORE_TOPIC_DEVICE Create the certificates. openssl req -x509 -nodes -newkey rsa:2048 \\ -keyout device.key.pem \\ -out device.crt.pem \\ -days 365 \\ -subj \"/CN=unused\" curl https://pki.google.com/roots.pem > ./root-ca.pem Register a device using the generated certificates. gcloud iot devices create $IOTCORE_DEVICE \\ --project = $IOTCORE_PROJECT \\ --region = $IOTCORE_REGION \\ --registry = $IOTCORE_REGISTRY \\ --public-key path = ./device.crt.pem,type = rsa-x509-pem Running \u00b6 We now have everything installed and ready to go. We will generate events and see them in the subscriber. Run the following program to generate events: go run github.com/knative/docs/docs/eventing/samples/iot-core/generator \\ -project $IOTCORE_PROJECT \\ -region $IOTCORE_REGION \\ -registry $IOTCORE_REGISTRY \\ -device $IOTCORE_DEVICE \\ -ca \" $PWD /root-ca.pem\" \\ -key \" $PWD /device.key.pem\" \\ -src \"iot-core demo\" \\ -events 10 Inspect the logs of the subscriber: kubectl logs --selector serving.knative.dev/service = event-display -c user-container You should see something along the similar to: { \"ID\" : \"481014114648052\" , \"Data\" : \"eyJzb3VyY2VfaWQiOiJpb3QtY29yZSBkZW1vIiwiZXZlbnRfaWQiOiJlaWQtMzI3MjJiMzItZWU5Mi00YzZlLWEzOTgtNDlmYjRkYWYyNGE1IiwiZXZlbnRfdHMiOjE1NTM3MTczOTYsIm1ldHJpYyI6MC4xMzY1MjI5OH0=\" , \"Attributes\" : { \"deviceId\" : \"iot-demo-client\" , \"deviceNumId\" : \"2754785852315736\" , \"deviceRegistryId\" : \"iot-demo\" , \"deviceRegistryLocation\" : \"us-central1\" , \"projectId\" : \"s9-demo\" , \"subFolder\" : \"\" } , \"PublishTime\" : \"2019-03-27T20:09:56.685Z\" } Cleanup \u00b6 To cleanup the knative resources: Remove the GcpPubSubSource : sed -e \"s/PROJECT_ID/ $IOTCORE_PROJECT /\" \\ -e \"s/TOPIC_NAME/ $IOTCORE_TOPIC_DATA /\" \\ docs/eventing/samples/iot-core/gcp-pubsub-source.yaml | kubectl delete --filename - Remove the Trigger: kubectl delete --filename docs/eventing/samples/iot-core/trigger.yaml Remove the GcpPubSubSource controller: kubectl delete --filename https://github.com/knative/eventing-contrib/releases/download/v0.8.2/gcppubsub.yaml","title":"IoT Core"},{"location":"eventing/samples/iot-core/#deployment-steps","text":"","title":"Deployment Steps"},{"location":"eventing/samples/iot-core/#environment-variables","text":"To make the following commands easier, we are going to set the various variables here and use them later.","title":"Environment Variables"},{"location":"eventing/samples/iot-core/#variables-you-must-change","text":"export IOTCORE_PROJECT = \"s9-demo\"","title":"Variables you must Change"},{"location":"eventing/samples/iot-core/#variables-you-may-change","text":"export IOTCORE_REGISTRY = \"iot-demo\" export IOTCORE_DEVICE = \"iot-demo-client\" export IOTCORE_REGION = \"us-central1\" export IOTCORE_TOPIC_DATA = \"iot-demo-pubsub-topic\" export IOTCORE_TOPIC_DEVICE = \"iot-demo-device-pubsub-topic\"","title":"Variables you may Change"},{"location":"eventing/samples/iot-core/#prerequisites","text":"","title":"Prerequisites"},{"location":"eventing/samples/iot-core/#kubernetes","text":"Have a running Kubernetes cluster with kubectl pointing at it.","title":"Kubernetes"},{"location":"eventing/samples/iot-core/#gcp","text":"Create a Google Cloud Project . Have gcloud installed and pointing at that project. Enable the Cloud Pub/Sub API on that project. gcloud services enable pubsub.googleapis.com Create the two GCP PubSub topic s. gcloud pubsub topics create $IOTCORE_TOPIC_DATA gcloud pubsub topics create $IOTCORE_TOPIC_DEVICE Setup Knative Eventing .","title":"GCP"},{"location":"eventing/samples/iot-core/#gcp-pubsub-source","text":"Create a GCP Service Account . Determine the Service Account to use, or create a new one. Give that Service Account the 'Pub/Sub Editor' role on your GCP project. Download a new JSON private key for that Service Account. Create two secrets with the downloaded key (one for the Source, one for the Receive Adapter): kubectl --namespace knative-sources create secret generic gcppubsub-source-key --from-file = key.json = PATH_TO_KEY_FILE.json kubectl --namespace default create secret generic google-cloud-key --from-file = key.json = PATH_TO_KEY_FILE.json Deploy the GcpPubSubSource controller as part of eventing-source's controller. kubectl apply --filename https://github.com/knative/eventing-contrib/releases/download/v0.8.2/gcppubsub.yaml","title":"GCP PubSub Source"},{"location":"eventing/samples/iot-core/#deploying","text":"","title":"Deploying"},{"location":"eventing/samples/iot-core/#broker","text":"Install the default Broker . kubectl create -f - <<EOF apiVersion: eventing.knative.dev/v1 kind: Broker metadata: name: default EOF","title":"Broker"},{"location":"eventing/samples/iot-core/#gcp-pubsub-source_1","text":"Deploy gcp-pubsub-source.yaml . sed -e \"s/PROJECT_ID/ $IOTCORE_PROJECT /\" \\ -e \"s/TOPIC_NAME/ $IOTCORE_TOPIC_DATA /\" \\ docs/eventing/samples/iot-core/gcp-pubsub-source.yaml | kubectl apply --filename -","title":"GCP PubSub Source"},{"location":"eventing/samples/iot-core/#trigger","text":"Even though the Source isn't completely ready yet, we can setup the Trigger for all events coming out of it. Deploy trigger.yaml . kubectl apply --filename docs/eventing/samples/iot-core/trigger.yaml This uses a very simple Knative Service to see that events are flowing. Feel free to replace it.","title":"Trigger"},{"location":"eventing/samples/iot-core/#iot-core","text":"We now have everything setup on the Knative side. We will now setup the IoT Core. Create a device registry: gcloud iot registries create $IOTCORE_REGISTRY \\ --project = $IOTCORE_PROJECT \\ --region = $IOTCORE_REGION \\ --event-notification-config = topic = $IOTCORE_TOPIC_DATA \\ --state-pubsub-topic = $IOTCORE_TOPIC_DEVICE Create the certificates. openssl req -x509 -nodes -newkey rsa:2048 \\ -keyout device.key.pem \\ -out device.crt.pem \\ -days 365 \\ -subj \"/CN=unused\" curl https://pki.google.com/roots.pem > ./root-ca.pem Register a device using the generated certificates. gcloud iot devices create $IOTCORE_DEVICE \\ --project = $IOTCORE_PROJECT \\ --region = $IOTCORE_REGION \\ --registry = $IOTCORE_REGISTRY \\ --public-key path = ./device.crt.pem,type = rsa-x509-pem","title":"IoT Core"},{"location":"eventing/samples/iot-core/#running","text":"We now have everything installed and ready to go. We will generate events and see them in the subscriber. Run the following program to generate events: go run github.com/knative/docs/docs/eventing/samples/iot-core/generator \\ -project $IOTCORE_PROJECT \\ -region $IOTCORE_REGION \\ -registry $IOTCORE_REGISTRY \\ -device $IOTCORE_DEVICE \\ -ca \" $PWD /root-ca.pem\" \\ -key \" $PWD /device.key.pem\" \\ -src \"iot-core demo\" \\ -events 10 Inspect the logs of the subscriber: kubectl logs --selector serving.knative.dev/service = event-display -c user-container You should see something along the similar to: { \"ID\" : \"481014114648052\" , \"Data\" : \"eyJzb3VyY2VfaWQiOiJpb3QtY29yZSBkZW1vIiwiZXZlbnRfaWQiOiJlaWQtMzI3MjJiMzItZWU5Mi00YzZlLWEzOTgtNDlmYjRkYWYyNGE1IiwiZXZlbnRfdHMiOjE1NTM3MTczOTYsIm1ldHJpYyI6MC4xMzY1MjI5OH0=\" , \"Attributes\" : { \"deviceId\" : \"iot-demo-client\" , \"deviceNumId\" : \"2754785852315736\" , \"deviceRegistryId\" : \"iot-demo\" , \"deviceRegistryLocation\" : \"us-central1\" , \"projectId\" : \"s9-demo\" , \"subFolder\" : \"\" } , \"PublishTime\" : \"2019-03-27T20:09:56.685Z\" }","title":"Running"},{"location":"eventing/samples/iot-core/#cleanup","text":"To cleanup the knative resources: Remove the GcpPubSubSource : sed -e \"s/PROJECT_ID/ $IOTCORE_PROJECT /\" \\ -e \"s/TOPIC_NAME/ $IOTCORE_TOPIC_DATA /\" \\ docs/eventing/samples/iot-core/gcp-pubsub-source.yaml | kubectl delete --filename - Remove the Trigger: kubectl delete --filename docs/eventing/samples/iot-core/trigger.yaml Remove the GcpPubSubSource controller: kubectl delete --filename https://github.com/knative/eventing-contrib/releases/download/v0.8.2/gcppubsub.yaml","title":"Cleanup"},{"location":"eventing/samples/kafka/","text":"The following examples will help you understand how to use the different Apache Kafka components for Knative. Prerequisites \u00b6 All examples require: A Kubernetes cluster with Knative Eventing v0.9+ Knative Serving v0.9+ An Apache Kafka cluster Setting up Apache Kafka \u00b6 If you want to run the Apache Kafka cluster on Kubernetes, the simplest option is to install it by using Strimzi . Create a namespace for your Apache Kafka installation, like kafka : kubectl create namespace kafka Install the Strimzi operator, like: curl -L \"https://github.com/strimzi/strimzi-kafka-operator/releases/download/0.16.2/strimzi-cluster-operator-0.16.2.yaml\" \\ | sed 's/namespace: .*/namespace: kafka/' \\ | kubectl -n kafka apply -f - Describe the size of your Apache Kafka installation in kafka.yaml , like: apiVersion : kafka.strimzi.io/v1beta1 kind : Kafka metadata : name : my-cluster spec : kafka : version : 2.4.0 replicas : 1 listeners : plain : {} tls : {} config : offsets.topic.replication.factor : 1 transaction.state.log.replication.factor : 1 transaction.state.log.min.isr : 1 log.message.format.version : \"2.4\" storage : type : ephemeral zookeeper : replicas : 3 storage : type : ephemeral entityOperator : topicOperator : {} userOperator : {} Deploy the Apache Kafka cluster $ kubectl apply -n kafka -f kafka.yaml This will install a small, non-production, cluster of Apache Kafka. To verify your installation, check if the pods for Strimzi are all up, in the kafka namespace: $ kubectl get pods -n kafka NAME READY STATUS RESTARTS AGE my-cluster-entity-operator-65995cf856-ld2zp 3 /3 Running 0 102s my-cluster-kafka-0 2 /2 Running 0 2m8s my-cluster-zookeeper-0 2 /2 Running 0 2m39s my-cluster-zookeeper-1 2 /2 Running 0 2m49s my-cluster-zookeeper-2 2 /2 Running 0 2m59s strimzi-cluster-operator-77555d4b69-sbrt4 1 /1 Running 0 3m14s NOTE: For production ready installs check Strimzi . Installation script \u00b6 If you want to install the latest version of Strimzi, in just one step, we have a script for your convenience, which does exactly the same steps that are listed above: $ ./kafka_setup.sh Examples of Apache Kafka and Knative \u00b6 A number of different examples, showing the KafkaSource , KafkaChannel and KafkaBinding can be found here: KafkaSource to Service KafkaChannel and Broker KafkaBinding","title":"Overview"},{"location":"eventing/samples/kafka/#prerequisites","text":"All examples require: A Kubernetes cluster with Knative Eventing v0.9+ Knative Serving v0.9+ An Apache Kafka cluster","title":"Prerequisites"},{"location":"eventing/samples/kafka/#setting-up-apache-kafka","text":"If you want to run the Apache Kafka cluster on Kubernetes, the simplest option is to install it by using Strimzi . Create a namespace for your Apache Kafka installation, like kafka : kubectl create namespace kafka Install the Strimzi operator, like: curl -L \"https://github.com/strimzi/strimzi-kafka-operator/releases/download/0.16.2/strimzi-cluster-operator-0.16.2.yaml\" \\ | sed 's/namespace: .*/namespace: kafka/' \\ | kubectl -n kafka apply -f - Describe the size of your Apache Kafka installation in kafka.yaml , like: apiVersion : kafka.strimzi.io/v1beta1 kind : Kafka metadata : name : my-cluster spec : kafka : version : 2.4.0 replicas : 1 listeners : plain : {} tls : {} config : offsets.topic.replication.factor : 1 transaction.state.log.replication.factor : 1 transaction.state.log.min.isr : 1 log.message.format.version : \"2.4\" storage : type : ephemeral zookeeper : replicas : 3 storage : type : ephemeral entityOperator : topicOperator : {} userOperator : {} Deploy the Apache Kafka cluster $ kubectl apply -n kafka -f kafka.yaml This will install a small, non-production, cluster of Apache Kafka. To verify your installation, check if the pods for Strimzi are all up, in the kafka namespace: $ kubectl get pods -n kafka NAME READY STATUS RESTARTS AGE my-cluster-entity-operator-65995cf856-ld2zp 3 /3 Running 0 102s my-cluster-kafka-0 2 /2 Running 0 2m8s my-cluster-zookeeper-0 2 /2 Running 0 2m39s my-cluster-zookeeper-1 2 /2 Running 0 2m49s my-cluster-zookeeper-2 2 /2 Running 0 2m59s strimzi-cluster-operator-77555d4b69-sbrt4 1 /1 Running 0 3m14s NOTE: For production ready installs check Strimzi .","title":"Setting up Apache Kafka"},{"location":"eventing/samples/kafka/#installation-script","text":"If you want to install the latest version of Strimzi, in just one step, we have a script for your convenience, which does exactly the same steps that are listed above: $ ./kafka_setup.sh","title":"Installation script"},{"location":"eventing/samples/kafka/#examples-of-apache-kafka-and-knative","text":"A number of different examples, showing the KafkaSource , KafkaChannel and KafkaBinding can be found here: KafkaSource to Service KafkaChannel and Broker KafkaBinding","title":"Examples of Apache Kafka and Knative"},{"location":"eventing/samples/kafka/_index/","text":"Macro Rendering Error \u00b6 TemplateSyntaxError : unexpected '%' Traceback (most recent call last): File \"/usr/local/lib/python3.9/site-packages/mkdocs_macros/plugin.py\", line 441, in render md_template = self.env.from_string(markdown) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 941, in from_string return cls.from_code(self, self.compile(source), globals, None) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 638, in compile self.handle_exception(source=source_hint) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 832, in handle_exception reraise(*rewrite_traceback_stack(source=source)) File \"/usr/local/lib/python3.9/site-packages/jinja2/_compat.py\", line 28, in reraise raise value.with_traceback(tb) File \"<unknown>\", line 3, in template jinja2.exceptions.TemplateSyntaxError: unexpected '%'","title":"Apache Kafka examples"},{"location":"eventing/samples/kafka/_index/#macro-rendering-error","text":"TemplateSyntaxError : unexpected '%' Traceback (most recent call last): File \"/usr/local/lib/python3.9/site-packages/mkdocs_macros/plugin.py\", line 441, in render md_template = self.env.from_string(markdown) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 941, in from_string return cls.from_code(self, self.compile(source), globals, None) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 638, in compile self.handle_exception(source=source_hint) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 832, in handle_exception reraise(*rewrite_traceback_stack(source=source)) File \"/usr/local/lib/python3.9/site-packages/jinja2/_compat.py\", line 28, in reraise raise value.with_traceback(tb) File \"<unknown>\", line 3, in template jinja2.exceptions.TemplateSyntaxError: unexpected '%'","title":"Macro Rendering Error"},{"location":"eventing/samples/kafka/binding/","text":"KafkaBinding is responsible for injecting Kafka bootstrap connection information into a Kubernetes resource that embed a PodSpec (as spec.template.spec ). This enables easy bootstrapping of a Kafka client. Create a Job that uses KafkaBinding \u00b6 In the below example a Kubernetes Job will be using the KafkaBinding to produce messages on a Kafka Topic, which will be received by the Event Display service via Kafka Source Prerequisites \u00b6 You must ensure that you meet the prerequisites listed in the Apache Kafka overview . This feature is available from Knative Eventing 0.15+ Creating a KafkaSource source CRD \u00b6 Install the KafkaSource sub-component to your Knative cluster: kubectl apply -f https://storage.googleapis.com/knative-releases/eventing-contrib/latest/kafka-source.yaml Check that the kafka-controller-manager-0 pod is running. kubectl get pods --namespace knative-sources NAME READY STATUS RESTARTS AGE kafka-controller-manager-0 1/1 Running 0 42m Create the Event Display service \u00b6 (Optional) Source code for Event Display service Get the source code of Event Display container image from here Deploy the Event Display Service via kubectl: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : event-display spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing-contrib/cmd/event_display $ kubectl apply --filename event-display.yaml ... service.serving.knative.dev/event-display created (Optional) Deploy the Event Display Service via kn cli: Alternatively, you can create the knative service using the kn cli like below kn service create event-display --image=gcr.io/knative-releases/knative.dev/eventing-contrib/cmd/event_display Ensure that the Service pod is running. The pod name will be prefixed with event-display . $ kubectl get pods NAME READY STATUS RESTARTS AGE event-display-00001-deployment-5d5df6c7-gv2j4 2/2 Running 0 72s ... Apache Kafka Event Source \u00b6 Modify event-source.yaml accordingly with bootstrap servers, topics, etc...: apiVersion : sources.knative.dev/v1beta1 kind : KafkaSource metadata : name : kafka-source spec : consumerGroup : knative-group bootstrapServers : - my-cluster-kafka-bootstrap.kafka:9092 #note the kafka namespace topics : - logs sink : ref : apiVersion : serving.knative.dev/v1 kind : Service name : event-display Deploy the event source. $ kubectl apply -f event-source.yaml ... kafkasource.sources.knative.dev/kafka-source created Check that the event source pod is running. The pod name will be prefixed with kafka-source . $ kubectl get pods NAME READY STATUS RESTARTS AGE kafka-source-xlnhq-5544766765-dnl5s 1/1 Running 0 40m Kafka Binding Resource \u00b6 Create the KafkaBinding that will inject kafka bootstrap information into select Jobs : Modify kafka-binding.yaml accordingly with bootstrap servers etc...: apiVersion : bindings.knative.dev/v1beta1 kind : KafkaBinding metadata : name : kafka-binding-test spec : subject : apiVersion : batch/v1 kind : Job selector : matchLabels : kafka.topic : \"logs\" bootstrapServers : - my-cluster-kafka-bootstrap.kafka:9092 In this case, we will bind any Job with the labels kafka.topic: \"logs\" . Create Kubernetes Job \u00b6 Source code for kafka-publisher service Get the source code of kafka-publisher container image from here Now we will use the kafka-publisher container to send events to kafka topic when the Job runs. apiVersion : batch/v1 kind : Job metadata : labels : kafka.topic : \"logs\" name : kafka-publisher-job spec : backoffLimit : 1 completions : 1 parallelism : 1 template : metadata : annotations : sidecar.istio.io/inject : \"false\" spec : restartPolicy : Never containers : - image : docker.io/murugappans/kafka-publisher-1974f83e2ff7c8994707b5e8731528e8@sha256:fd79490514053c643617dc72a43097251fed139c966fd5d131134a0e424882de env : - name : KAFKA_TOPIC value : \"logs\" - name : KAFKA_KEY value : \"0\" - name : KAFKA_HEADERS value : \"content-type:application/json\" - name : KAFKA_VALUE value : '{\"msg\":\"This is a test!\"}' name : kafka-publisher 1. Check that the Job has run successfully. $ kubectl get jobs NAME COMPLETIONS DURATION AGE kafka-publisher-job 1/1 7s 7s Verify \u00b6 Ensure the Event Display received the message sent to it by the Event Source. $ kubectl logs --selector='serving.knative.dev/service=event-display' -c user-container \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1.0 type: dev.knative.kafka.event source: /apis/v1/namespaces/default/kafkasources/kafka-source#logs subject: partition:0#1 id: partition:0/offset:1 time: 2020-05-17T19:45:02.7Z datacontenttype: application/json Extensions, kafkaheadercontenttype: application/json key: 0 traceparent: 00-f383b779f512358b24ffbf6556a6d6da-cacdbe78ef9b5ad3-00 Data, { \"msg\": \"This is a test!\" } Connecting to a TLS enabled Kafka broker \u00b6 The KafkaBinding supports TLS and SASL authentication methods. For injecting TLS authentication, please have the below files CA Certificate Client Certificate and Key These files are expected to be in pem format, if it is in other format like jks , please convert to pem. Create the certificate files as secrets in the namespace where KafkaBinding is going to be set up $ kubectl create secret generic cacert --from-file=caroot.pem secret/cacert created $ kubectl create secret tls kafka-secret --cert=certificate.pem --key=key.pem secret/key created Apply the kafkabinding-tls.yaml, change bootstrapServers accordingly. apiVersion : sources.knative.dev/v1beta1 kind : KafkaBinding metadata : name : kafka-source-with-tls spec : subject : apiVersion : batch/v1 kind : Job selector : matchLabels : kafka.topic : \"logs\" net : tls : enable : true cert : secretKeyRef : key : tls.crt name : kafka-secret key : secretKeyRef : key : tls.key name : kafka-secret caCert : secretKeyRef : key : caroot.pem name : cacert consumerGroup : knative-group bootstrapServers : - my-secure-kafka-bootstrap.kafka:443","title":"Binding Example"},{"location":"eventing/samples/kafka/binding/#create-a-job-that-uses-kafkabinding","text":"In the below example a Kubernetes Job will be using the KafkaBinding to produce messages on a Kafka Topic, which will be received by the Event Display service via Kafka Source","title":"Create a Job that uses KafkaBinding"},{"location":"eventing/samples/kafka/binding/#prerequisites","text":"You must ensure that you meet the prerequisites listed in the Apache Kafka overview . This feature is available from Knative Eventing 0.15+","title":"Prerequisites"},{"location":"eventing/samples/kafka/binding/#creating-a-kafkasource-source-crd","text":"Install the KafkaSource sub-component to your Knative cluster: kubectl apply -f https://storage.googleapis.com/knative-releases/eventing-contrib/latest/kafka-source.yaml Check that the kafka-controller-manager-0 pod is running. kubectl get pods --namespace knative-sources NAME READY STATUS RESTARTS AGE kafka-controller-manager-0 1/1 Running 0 42m","title":"Creating a KafkaSource source CRD"},{"location":"eventing/samples/kafka/binding/#create-the-event-display-service","text":"(Optional) Source code for Event Display service Get the source code of Event Display container image from here Deploy the Event Display Service via kubectl: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : event-display spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing-contrib/cmd/event_display $ kubectl apply --filename event-display.yaml ... service.serving.knative.dev/event-display created (Optional) Deploy the Event Display Service via kn cli: Alternatively, you can create the knative service using the kn cli like below kn service create event-display --image=gcr.io/knative-releases/knative.dev/eventing-contrib/cmd/event_display Ensure that the Service pod is running. The pod name will be prefixed with event-display . $ kubectl get pods NAME READY STATUS RESTARTS AGE event-display-00001-deployment-5d5df6c7-gv2j4 2/2 Running 0 72s ...","title":"Create the Event Display service"},{"location":"eventing/samples/kafka/binding/#apache-kafka-event-source","text":"Modify event-source.yaml accordingly with bootstrap servers, topics, etc...: apiVersion : sources.knative.dev/v1beta1 kind : KafkaSource metadata : name : kafka-source spec : consumerGroup : knative-group bootstrapServers : - my-cluster-kafka-bootstrap.kafka:9092 #note the kafka namespace topics : - logs sink : ref : apiVersion : serving.knative.dev/v1 kind : Service name : event-display Deploy the event source. $ kubectl apply -f event-source.yaml ... kafkasource.sources.knative.dev/kafka-source created Check that the event source pod is running. The pod name will be prefixed with kafka-source . $ kubectl get pods NAME READY STATUS RESTARTS AGE kafka-source-xlnhq-5544766765-dnl5s 1/1 Running 0 40m","title":"Apache Kafka Event Source"},{"location":"eventing/samples/kafka/binding/#kafka-binding-resource","text":"Create the KafkaBinding that will inject kafka bootstrap information into select Jobs : Modify kafka-binding.yaml accordingly with bootstrap servers etc...: apiVersion : bindings.knative.dev/v1beta1 kind : KafkaBinding metadata : name : kafka-binding-test spec : subject : apiVersion : batch/v1 kind : Job selector : matchLabels : kafka.topic : \"logs\" bootstrapServers : - my-cluster-kafka-bootstrap.kafka:9092 In this case, we will bind any Job with the labels kafka.topic: \"logs\" .","title":"Kafka Binding Resource"},{"location":"eventing/samples/kafka/binding/#create-kubernetes-job","text":"Source code for kafka-publisher service Get the source code of kafka-publisher container image from here Now we will use the kafka-publisher container to send events to kafka topic when the Job runs. apiVersion : batch/v1 kind : Job metadata : labels : kafka.topic : \"logs\" name : kafka-publisher-job spec : backoffLimit : 1 completions : 1 parallelism : 1 template : metadata : annotations : sidecar.istio.io/inject : \"false\" spec : restartPolicy : Never containers : - image : docker.io/murugappans/kafka-publisher-1974f83e2ff7c8994707b5e8731528e8@sha256:fd79490514053c643617dc72a43097251fed139c966fd5d131134a0e424882de env : - name : KAFKA_TOPIC value : \"logs\" - name : KAFKA_KEY value : \"0\" - name : KAFKA_HEADERS value : \"content-type:application/json\" - name : KAFKA_VALUE value : '{\"msg\":\"This is a test!\"}' name : kafka-publisher 1. Check that the Job has run successfully. $ kubectl get jobs NAME COMPLETIONS DURATION AGE kafka-publisher-job 1/1 7s 7s","title":"Create Kubernetes Job"},{"location":"eventing/samples/kafka/binding/#verify","text":"Ensure the Event Display received the message sent to it by the Event Source. $ kubectl logs --selector='serving.knative.dev/service=event-display' -c user-container \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1.0 type: dev.knative.kafka.event source: /apis/v1/namespaces/default/kafkasources/kafka-source#logs subject: partition:0#1 id: partition:0/offset:1 time: 2020-05-17T19:45:02.7Z datacontenttype: application/json Extensions, kafkaheadercontenttype: application/json key: 0 traceparent: 00-f383b779f512358b24ffbf6556a6d6da-cacdbe78ef9b5ad3-00 Data, { \"msg\": \"This is a test!\" }","title":"Verify"},{"location":"eventing/samples/kafka/binding/#connecting-to-a-tls-enabled-kafka-broker","text":"The KafkaBinding supports TLS and SASL authentication methods. For injecting TLS authentication, please have the below files CA Certificate Client Certificate and Key These files are expected to be in pem format, if it is in other format like jks , please convert to pem. Create the certificate files as secrets in the namespace where KafkaBinding is going to be set up $ kubectl create secret generic cacert --from-file=caroot.pem secret/cacert created $ kubectl create secret tls kafka-secret --cert=certificate.pem --key=key.pem secret/key created Apply the kafkabinding-tls.yaml, change bootstrapServers accordingly. apiVersion : sources.knative.dev/v1beta1 kind : KafkaBinding metadata : name : kafka-source-with-tls spec : subject : apiVersion : batch/v1 kind : Job selector : matchLabels : kafka.topic : \"logs\" net : tls : enable : true cert : secretKeyRef : key : tls.crt name : kafka-secret key : secretKeyRef : key : tls.key name : kafka-secret caCert : secretKeyRef : key : caroot.pem name : cacert consumerGroup : knative-group bootstrapServers : - my-secure-kafka-bootstrap.kafka:443","title":"Connecting to a TLS enabled Kafka broker"},{"location":"eventing/samples/kafka/channel/","text":"You can install and configure the Apache Kafka CRD ( KafkaChannel ) as the default channel configuration in Knative Eventing. Prerequisites \u00b6 Ensure that you meet the prerequisites listed in the Apache Kafka overview . A Kubernetes cluster with Knative Kafka Channel installed . Creating a KafkaChannel channel CRD \u00b6 Create a new object by configuring the YAML file as follows: cat <<-EOF | kubectl apply -f - --- apiVersion: messaging.knative.dev/v1beta1 kind: KafkaChannel metadata: name: my-kafka-channel spec: numPartitions: 3 replicationFactor: 1 EOF Specifying the default channel configuration \u00b6 To configure the usage of the KafkaChannel CRD as the default channel configuration , edit the default-ch-webhook ConfigMap as follows: cat <<-EOF | kubectl apply -f - --- apiVersion: v1 kind: ConfigMap metadata: name: default-ch-webhook namespace: knative-eventing data: # Configuration for defaulting channels that do not specify CRD implementations. default-ch-config: | clusterDefault: apiVersion: messaging.knative.dev/v1beta1 kind: KafkaChannel spec: numPartitions: 3 replicationFactor: 1 EOF Creating an Apache Kafka channel using the default channel configuration \u00b6 Now that KafkaChannel is set as the default channel configuration, you can use the channels.messaging.knative.dev CRD to create a new Apache Kafka channel, using the generic Channel : cat <<-EOF | kubectl apply -f - --- apiVersion: messaging.knative.dev/v1 kind: Channel metadata: name: testchannel-one EOF Check Kafka for a testchannel topic. With Strimzi this can be done by using the command: kubectl -n kafka exec -it my-cluster-kafka-0 -- bin/kafka-topics.sh --zookeeper localhost:2181 --list The result is: ... knative-messaging-kafka.default.testchannel-one ... The Apache Kafka topic that is created by the channel implementation is prefixed with knative-messaging-kafka . This indicates it is an Apache Kafka channel from Knative. It contains the name of the namespace, default in this example, followed by the actual name of the channel. Configuring the Knative broker for Apache Kafka channels \u00b6 To setup a broker that will use the new default Kafka channels, you must create a new default broker, using the command: kubectl create -f - <<EOF apiVersion: eventing.knative.dev/v1 kind: Broker metadata: name: default EOF This will give you two pods, such as: default-broker-filter-64658fc79f-nf596 1/1 Running 0 15m default-broker-ingress-ff79755b6-vj9jt 1/1 Running 0 15m Inside the Apache Kafka cluster you should see two new topics, such as: ... knative-messaging-kafka.default.default-kn2-ingress knative-messaging-kafka.default.default-kn2-trigger ... Creating a service and trigger to use the Apache Kafka broker \u00b6 To use the Apache Kafka based broker, let's take a look at a simple demo. Use the ApiServerSource to publish events to the broker as well as the Trigger API, which then routes events to a Knative Service . Install ksvc , using the command: kubectl apply -f 000-ksvc.yaml Install a source that publishes to the default broker kubectl apply -f 020-k8s-events.yaml Create a trigger that routes the events to the ksvc : kubectl apply -f 030-trigger.yaml Verifying your Apache Kafka channel and broker \u00b6 Now that your Eventing cluster is configured for Apache Kafka, you can verify your configuration with the following options. Receive events via Knative \u00b6 Now you can see the events in the log of the ksvc using the command: kubectl logs --selector='serving.knative.dev/service=broker-kafka-display' -c user-container Authentication against an Apache Kafka \u00b6 In production environments it is common that the Apache Kafka cluster is secured using TLS or SASL . This section shows how to confiugure the KafkaChannel to work against a protected Apache Kafka cluster, with the two supported TLS and SASL authentication methods. TLS authentication \u00b6 To use TLS authentication you must create: A CA certificate A client certificate and key NOTE: Kafka channels require these files to be in .pem format. If your files are in a different format, you must convert them to .pem . Create the certificate files as secrets in your chosen namespace: $ kubectl create secret --namespace <namespace> generic <kafka-auth-secret> \\ --from-file=ca.crt=caroot.pem \\ --from-file=user.crt=certificate.pem \\ --from-file=user.key=key.pem ``` *NOTE:* It is important to use the same keys (`ca.crt`, `user.crt` and `user.key`). Reference your secret and the namespace of the secret in the `config-kafka` ConfigMap: apiVersion: v1 kind: ConfigMap metadata: name: config-kafka namespace: knative-eventing data: bootstrapServers: authSecretName: authSecretNamespace: ``` SASL authentication \u00b6 To use SASL authentication, you will need the following information: A username and password. The type of SASL mechanism you wish to use. For example; PLAIN , SCRAM-SHA-256 or SCRAM-SHA-512 . NOTE: It is recommended to also enable TLS. If you enable this, you will also need the ca.crt certificate as described in the previous section. Create the certificate files as secrets in your chosen namespace: $ kubectl create secret --namespace <namespace> generic <kafka-auth-secret> \\ --from-file=ca.crt=caroot.pem \\ --from-literal=password=\"SecretPassword\" \\ --from-literal=saslType=\"SCRAM-SHA-512\" \\ --from-literal=user=\"my-sasl-user\" NOTE: It is important to use the same keys; user , password and saslType . Reference your secret and the namespace of the secret in the config-kafka ConfigMap: apiVersion: v1 kind: ConfigMap metadata: name: config-kafka namespace: knative-eventing data: bootstrapServers: <bootstrap-servers> authSecretName: <kafka-auth-Secret> authSecretNamespace: <namespace>","title":"Channel Example"},{"location":"eventing/samples/kafka/channel/#prerequisites","text":"Ensure that you meet the prerequisites listed in the Apache Kafka overview . A Kubernetes cluster with Knative Kafka Channel installed .","title":"Prerequisites"},{"location":"eventing/samples/kafka/channel/#creating-a-kafkachannel-channel-crd","text":"Create a new object by configuring the YAML file as follows: cat <<-EOF | kubectl apply -f - --- apiVersion: messaging.knative.dev/v1beta1 kind: KafkaChannel metadata: name: my-kafka-channel spec: numPartitions: 3 replicationFactor: 1 EOF","title":"Creating a KafkaChannel channel CRD"},{"location":"eventing/samples/kafka/channel/#specifying-the-default-channel-configuration","text":"To configure the usage of the KafkaChannel CRD as the default channel configuration , edit the default-ch-webhook ConfigMap as follows: cat <<-EOF | kubectl apply -f - --- apiVersion: v1 kind: ConfigMap metadata: name: default-ch-webhook namespace: knative-eventing data: # Configuration for defaulting channels that do not specify CRD implementations. default-ch-config: | clusterDefault: apiVersion: messaging.knative.dev/v1beta1 kind: KafkaChannel spec: numPartitions: 3 replicationFactor: 1 EOF","title":"Specifying the default channel configuration"},{"location":"eventing/samples/kafka/channel/#creating-an-apache-kafka-channel-using-the-default-channel-configuration","text":"Now that KafkaChannel is set as the default channel configuration, you can use the channels.messaging.knative.dev CRD to create a new Apache Kafka channel, using the generic Channel : cat <<-EOF | kubectl apply -f - --- apiVersion: messaging.knative.dev/v1 kind: Channel metadata: name: testchannel-one EOF Check Kafka for a testchannel topic. With Strimzi this can be done by using the command: kubectl -n kafka exec -it my-cluster-kafka-0 -- bin/kafka-topics.sh --zookeeper localhost:2181 --list The result is: ... knative-messaging-kafka.default.testchannel-one ... The Apache Kafka topic that is created by the channel implementation is prefixed with knative-messaging-kafka . This indicates it is an Apache Kafka channel from Knative. It contains the name of the namespace, default in this example, followed by the actual name of the channel.","title":"Creating an Apache Kafka channel using the default channel configuration"},{"location":"eventing/samples/kafka/channel/#configuring-the-knative-broker-for-apache-kafka-channels","text":"To setup a broker that will use the new default Kafka channels, you must create a new default broker, using the command: kubectl create -f - <<EOF apiVersion: eventing.knative.dev/v1 kind: Broker metadata: name: default EOF This will give you two pods, such as: default-broker-filter-64658fc79f-nf596 1/1 Running 0 15m default-broker-ingress-ff79755b6-vj9jt 1/1 Running 0 15m Inside the Apache Kafka cluster you should see two new topics, such as: ... knative-messaging-kafka.default.default-kn2-ingress knative-messaging-kafka.default.default-kn2-trigger ...","title":"Configuring the Knative broker for Apache Kafka channels"},{"location":"eventing/samples/kafka/channel/#creating-a-service-and-trigger-to-use-the-apache-kafka-broker","text":"To use the Apache Kafka based broker, let's take a look at a simple demo. Use the ApiServerSource to publish events to the broker as well as the Trigger API, which then routes events to a Knative Service . Install ksvc , using the command: kubectl apply -f 000-ksvc.yaml Install a source that publishes to the default broker kubectl apply -f 020-k8s-events.yaml Create a trigger that routes the events to the ksvc : kubectl apply -f 030-trigger.yaml","title":"Creating a service and trigger to use the Apache Kafka broker"},{"location":"eventing/samples/kafka/channel/#verifying-your-apache-kafka-channel-and-broker","text":"Now that your Eventing cluster is configured for Apache Kafka, you can verify your configuration with the following options.","title":"Verifying your Apache Kafka channel and broker"},{"location":"eventing/samples/kafka/channel/#receive-events-via-knative","text":"Now you can see the events in the log of the ksvc using the command: kubectl logs --selector='serving.knative.dev/service=broker-kafka-display' -c user-container","title":"Receive events via Knative"},{"location":"eventing/samples/kafka/channel/#authentication-against-an-apache-kafka","text":"In production environments it is common that the Apache Kafka cluster is secured using TLS or SASL . This section shows how to confiugure the KafkaChannel to work against a protected Apache Kafka cluster, with the two supported TLS and SASL authentication methods.","title":"Authentication against an Apache Kafka"},{"location":"eventing/samples/kafka/channel/#tls-authentication","text":"To use TLS authentication you must create: A CA certificate A client certificate and key NOTE: Kafka channels require these files to be in .pem format. If your files are in a different format, you must convert them to .pem . Create the certificate files as secrets in your chosen namespace: $ kubectl create secret --namespace <namespace> generic <kafka-auth-secret> \\ --from-file=ca.crt=caroot.pem \\ --from-file=user.crt=certificate.pem \\ --from-file=user.key=key.pem ``` *NOTE:* It is important to use the same keys (`ca.crt`, `user.crt` and `user.key`). Reference your secret and the namespace of the secret in the `config-kafka` ConfigMap: apiVersion: v1 kind: ConfigMap metadata: name: config-kafka namespace: knative-eventing data: bootstrapServers: authSecretName: authSecretNamespace: ```","title":"TLS authentication"},{"location":"eventing/samples/kafka/channel/#sasl-authentication","text":"To use SASL authentication, you will need the following information: A username and password. The type of SASL mechanism you wish to use. For example; PLAIN , SCRAM-SHA-256 or SCRAM-SHA-512 . NOTE: It is recommended to also enable TLS. If you enable this, you will also need the ca.crt certificate as described in the previous section. Create the certificate files as secrets in your chosen namespace: $ kubectl create secret --namespace <namespace> generic <kafka-auth-secret> \\ --from-file=ca.crt=caroot.pem \\ --from-literal=password=\"SecretPassword\" \\ --from-literal=saslType=\"SCRAM-SHA-512\" \\ --from-literal=user=\"my-sasl-user\" NOTE: It is important to use the same keys; user , password and saslType . Reference your secret and the namespace of the secret in the config-kafka ConfigMap: apiVersion: v1 kind: ConfigMap metadata: name: config-kafka namespace: knative-eventing data: bootstrapServers: <bootstrap-servers> authSecretName: <kafka-auth-Secret> authSecretNamespace: <namespace>","title":"SASL authentication"},{"location":"eventing/samples/kafka/source/","text":"Tutorial on how to build and deploy a KafkaSource Eventing source using a Knative Serving Service . Prerequisites \u00b6 Ensure that you meet the prerequisites listed in the Apache Kafka overview . A Kubernetes cluster with Knative Kafka Source installed . Apache Kafka Topic (Optional) \u00b6 If using Strimzi, you can set a topic modifying source/kafka-topic.yaml with your desired: Topic Cluster Name Partitions Replicas apiVersion : kafka.strimzi.io/v1beta1 kind : KafkaTopic metadata : name : knative-demo-topic namespace : kafka labels : strimzi.io/cluster : my-cluster spec : partitions : 3 replicas : 1 config : retention.ms : 7200000 segment.bytes : 1073741824 Deploy the KafkaTopic $ kubectl apply -f strimzi-topic.yaml kafkatopic.kafka.strimzi.io/knative-demo-topic created Ensure the KafkaTopic is running. $ kubectl -n kafka get kafkatopics.kafka.strimzi.io NAME AGE knative-demo-topic 16s Create the Event Display service \u00b6 Download a copy of the code: git clone -b \"v0.21.x\" https://github.com/knative/docs knative-docs cd knative-docs/docs/eventing/samples/kafka/source Build the Event Display Service ( event-display.yaml ) apiVersion : serving.knative.dev/v1 kind : Service metadata : name : event-display namespace : default spec : template : spec : containers : - # This corresponds to # https://github.com/knative/eventing-contrib/tree/main/cmd/event_display/main.go image : gcr.io/knative-releases/knative.dev/eventing-contrib/cmd/event_display Deploy the Event Display Service $ kubectl apply --filename event-display.yaml ... service.serving.knative.dev/event-display created Ensure that the Service pod is running. The pod name will be prefixed with event-display . $ kubectl get pods NAME READY STATUS RESTARTS AGE event-display-00001-deployment-5d5df6c7-gv2j4 2/2 Running 0 72s ... Apache Kafka Event Source \u00b6 Modify source/event-source.yaml accordingly with bootstrap servers, topics, etc...: apiVersion : sources.knative.dev/v1beta1 kind : KafkaSource metadata : name : kafka-source spec : consumerGroup : knative-group bootstrapServers : - my-cluster-kafka-bootstrap.kafka:9092 # note the kafka namespace topics : - knative-demo-topic sink : ref : apiVersion : serving.knative.dev/v1 kind : Service name : event-display Deploy the event source. $ kubectl apply -f event-source.yaml ... kafkasource.sources.knative.dev/kafka-source created Check that the event source pod is running. The pod name will be prefixed with kafka-source . $ kubectl get pods NAME READY STATUS RESTARTS AGE kafka-source-xlnhq-5544766765-dnl5s 1/1 Running 0 40m Ensure the Apache Kafka Event Source started with the necessary configuration. $ kubectl logs --selector='knative-eventing-source-name=kafka-source' {\"level\":\"info\",\"ts\":\"2020-05-28T10:39:42.104Z\",\"caller\":\"adapter/adapter.go:81\",\"msg\":\"Starting with config: \",\"Topics\":\".\",\"ConsumerGroup\":\"...\",\"SinkURI\":\"...\",\"Name\":\".\",\"Namespace\":\".\"} Verify \u00b6 Produce a message ( {\"msg\": \"This is a test!\"} ) to the Apache Kafka topic, like shown below: kubectl -n kafka run kafka-producer -ti --image=strimzi/kafka:0.14.0-kafka-2.3.0 --rm=true --restart=Never -- bin/kafka-console-producer.sh --broker-list my-cluster-kafka-bootstrap:9092 --topic knative-demo-topic If you don't see a command prompt, try pressing enter. >{\"msg\": \"This is a test!\"} Check that the Apache Kafka Event Source consumed the message and sent it to its sink properly. Since these logs are captured in debug level, edit the key level of config-logging configmap in knative-sources namespace to look like this: data: loglevel.controller: info loglevel.webhook: info zap-logger-config: | { \"level\": \"debug\", \"development\": false, \"outputPaths\": [\"stdout\"], \"errorOutputPaths\": [\"stderr\"], \"encoding\": \"json\", \"encoderConfig\": { \"timeKey\": \"ts\", \"levelKey\": \"level\", \"nameKey\": \"logger\", \"callerKey\": \"caller\", \"messageKey\": \"msg\", \"stacktraceKey\": \"stacktrace\", \"lineEnding\": \"\", \"levelEncoder\": \"\", \"timeEncoder\": \"iso8601\", \"durationEncoder\": \"\", \"callerEncoder\": \"\" } } Now manually delete the kafkasource deployment and allow the kafka-controller-manager deployment running in knative-sources namespace to redeploy it. Debug level logs should be visible now. $ kubectl logs --selector='knative-eventing-source-name=kafka-source' ... {\"level\":\"debug\",\"ts\":\"2020-05-28T10:40:29.400Z\",\"caller\":\"kafka/consumer_handler.go:77\",\"msg\":\"Message claimed\",\"topic\":\".\",\"value\":\".\"} {\"level\":\"debug\",\"ts\":\"2020-05-28T10:40:31.722Z\",\"caller\":\"kafka/consumer_handler.go:89\",\"msg\":\"Message marked\",\"topic\":\".\",\"value\":\".\"} Ensure the Event Display received the message sent to it by the Event Source. $ kubectl logs --selector='serving.knative.dev/service=event-display' -c user-container \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1.0 type: dev.knative.kafka.event source: /apis/v1/namespaces/default/kafkasources/kafka-source#my-topic subject: partition:0#564 id: partition:0/offset:564 time: 2020-02-10T18:10:23.861866615Z datacontenttype: application/json Extensions, key: Data, { \"msg\": \"This is a test!\" } Teardown Steps \u00b6 Remove the Apache Kafka Event Source $ kubectl delete -f source/source.yaml kafkasource.sources.knative.dev \"kafka-source\" deleted Remove the Event Display $ kubectl delete -f source/event-display.yaml service.serving.knative.dev \"event-display\" deleted Remove the Apache Kafka Event Controller $ kubectl delete -f https://storage.googleapis.com/knative-releases/eventing-contrib/latest/kafka-source.yaml serviceaccount \"kafka-controller-manager\" deleted clusterrole.rbac.authorization.k8s.io \"eventing-sources-kafka-controller\" deleted clusterrolebinding.rbac.authorization.k8s.io \"eventing-sources-kafka-controller\" deleted customresourcedefinition.apiextensions.k8s.io \"kafkasources.sources.knative.dev\" deleted service \"kafka-controller\" deleted statefulset.apps \"kafka-controller-manager\" deleted (Optional) Remove the Apache Kafka Topic $ kubectl delete -f kafka-topic.yaml kafkatopic.kafka.strimzi.io \"knative-demo-topic\" deleted (Optional) Specify the key deserializer \u00b6 When KafkaSource receives a message from Kafka, it dumps the key in the Event extension called Key and dumps Kafka message headers in the extensions starting with kafkaheader . You can specify the key deserializer among four types: string (default) for UTF-8 encoded strings int for 32-bit & 64-bit signed integers float for 32-bit & 64-bit floating points byte-array for a Base64 encoded byte array To specify it, add the label kafkasources.sources.knative.dev/key-type to the KafkaSource definition like: apiVersion : sources.knative.dev/v1beta1 kind : KafkaSource metadata : name : kafka-source labels : kafkasources.sources.knative.dev/key-type : int spec : consumerGroup : knative-group bootstrapServers : - my-cluster-kafka-bootstrap.kafka:9092 # note the kafka namespace topics : - knative-demo-topic sink : ref : apiVersion : serving.knative.dev/v1 kind : Service name : event-display Connecting to a TLS enabled Kafka broker \u00b6 The KafkaSource supports TLS and SASL authentication methods. For enabling TLS authentication, please have the below files CA Certificate Client Certificate and Key KafkaSource expects these files to be in pem format, if it is in other format like jks, please convert to pem. Create the certificate files as secrets in the namespace where KafkaSource is going to be set up $ kubectl create secret generic cacert --from-file=caroot.pem secret/cacert created $ kubectl create secret tls kafka-secret --cert=certificate.pem --key=key.pem secret/key created Apply the KafkaSource, change bootstrapServers and topics accordingly. ```yaml apiVersion: sources.knative.dev/v1beta1 kind: KafkaSource metadata: name: kafka-source-with-tls spec: net: tls: enable: true cert: secretKeyRef: key: tls.crt name: kafka-secret key: secretKeyRef: key: tls.key name: kafka-secret caCert: secretKeyRef: key: caroot.pem name: cacert consumerGroup: knative-group bootstrapServers: my-secure-kafka-bootstrap.kafka:443 topics: knative-demo-topic sink: ref: apiVersion: serving.knative.dev/v1 kind: Service name: event-display ```","title":"Source Example"},{"location":"eventing/samples/kafka/source/#prerequisites","text":"Ensure that you meet the prerequisites listed in the Apache Kafka overview . A Kubernetes cluster with Knative Kafka Source installed .","title":"Prerequisites"},{"location":"eventing/samples/kafka/source/#apache-kafka-topic-optional","text":"If using Strimzi, you can set a topic modifying source/kafka-topic.yaml with your desired: Topic Cluster Name Partitions Replicas apiVersion : kafka.strimzi.io/v1beta1 kind : KafkaTopic metadata : name : knative-demo-topic namespace : kafka labels : strimzi.io/cluster : my-cluster spec : partitions : 3 replicas : 1 config : retention.ms : 7200000 segment.bytes : 1073741824 Deploy the KafkaTopic $ kubectl apply -f strimzi-topic.yaml kafkatopic.kafka.strimzi.io/knative-demo-topic created Ensure the KafkaTopic is running. $ kubectl -n kafka get kafkatopics.kafka.strimzi.io NAME AGE knative-demo-topic 16s","title":"Apache Kafka Topic (Optional)"},{"location":"eventing/samples/kafka/source/#create-the-event-display-service","text":"Download a copy of the code: git clone -b \"v0.21.x\" https://github.com/knative/docs knative-docs cd knative-docs/docs/eventing/samples/kafka/source Build the Event Display Service ( event-display.yaml ) apiVersion : serving.knative.dev/v1 kind : Service metadata : name : event-display namespace : default spec : template : spec : containers : - # This corresponds to # https://github.com/knative/eventing-contrib/tree/main/cmd/event_display/main.go image : gcr.io/knative-releases/knative.dev/eventing-contrib/cmd/event_display Deploy the Event Display Service $ kubectl apply --filename event-display.yaml ... service.serving.knative.dev/event-display created Ensure that the Service pod is running. The pod name will be prefixed with event-display . $ kubectl get pods NAME READY STATUS RESTARTS AGE event-display-00001-deployment-5d5df6c7-gv2j4 2/2 Running 0 72s ...","title":"Create the Event Display service"},{"location":"eventing/samples/kafka/source/#apache-kafka-event-source","text":"Modify source/event-source.yaml accordingly with bootstrap servers, topics, etc...: apiVersion : sources.knative.dev/v1beta1 kind : KafkaSource metadata : name : kafka-source spec : consumerGroup : knative-group bootstrapServers : - my-cluster-kafka-bootstrap.kafka:9092 # note the kafka namespace topics : - knative-demo-topic sink : ref : apiVersion : serving.knative.dev/v1 kind : Service name : event-display Deploy the event source. $ kubectl apply -f event-source.yaml ... kafkasource.sources.knative.dev/kafka-source created Check that the event source pod is running. The pod name will be prefixed with kafka-source . $ kubectl get pods NAME READY STATUS RESTARTS AGE kafka-source-xlnhq-5544766765-dnl5s 1/1 Running 0 40m Ensure the Apache Kafka Event Source started with the necessary configuration. $ kubectl logs --selector='knative-eventing-source-name=kafka-source' {\"level\":\"info\",\"ts\":\"2020-05-28T10:39:42.104Z\",\"caller\":\"adapter/adapter.go:81\",\"msg\":\"Starting with config: \",\"Topics\":\".\",\"ConsumerGroup\":\"...\",\"SinkURI\":\"...\",\"Name\":\".\",\"Namespace\":\".\"}","title":"Apache Kafka Event Source"},{"location":"eventing/samples/kafka/source/#verify","text":"Produce a message ( {\"msg\": \"This is a test!\"} ) to the Apache Kafka topic, like shown below: kubectl -n kafka run kafka-producer -ti --image=strimzi/kafka:0.14.0-kafka-2.3.0 --rm=true --restart=Never -- bin/kafka-console-producer.sh --broker-list my-cluster-kafka-bootstrap:9092 --topic knative-demo-topic If you don't see a command prompt, try pressing enter. >{\"msg\": \"This is a test!\"} Check that the Apache Kafka Event Source consumed the message and sent it to its sink properly. Since these logs are captured in debug level, edit the key level of config-logging configmap in knative-sources namespace to look like this: data: loglevel.controller: info loglevel.webhook: info zap-logger-config: | { \"level\": \"debug\", \"development\": false, \"outputPaths\": [\"stdout\"], \"errorOutputPaths\": [\"stderr\"], \"encoding\": \"json\", \"encoderConfig\": { \"timeKey\": \"ts\", \"levelKey\": \"level\", \"nameKey\": \"logger\", \"callerKey\": \"caller\", \"messageKey\": \"msg\", \"stacktraceKey\": \"stacktrace\", \"lineEnding\": \"\", \"levelEncoder\": \"\", \"timeEncoder\": \"iso8601\", \"durationEncoder\": \"\", \"callerEncoder\": \"\" } } Now manually delete the kafkasource deployment and allow the kafka-controller-manager deployment running in knative-sources namespace to redeploy it. Debug level logs should be visible now. $ kubectl logs --selector='knative-eventing-source-name=kafka-source' ... {\"level\":\"debug\",\"ts\":\"2020-05-28T10:40:29.400Z\",\"caller\":\"kafka/consumer_handler.go:77\",\"msg\":\"Message claimed\",\"topic\":\".\",\"value\":\".\"} {\"level\":\"debug\",\"ts\":\"2020-05-28T10:40:31.722Z\",\"caller\":\"kafka/consumer_handler.go:89\",\"msg\":\"Message marked\",\"topic\":\".\",\"value\":\".\"} Ensure the Event Display received the message sent to it by the Event Source. $ kubectl logs --selector='serving.knative.dev/service=event-display' -c user-container \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1.0 type: dev.knative.kafka.event source: /apis/v1/namespaces/default/kafkasources/kafka-source#my-topic subject: partition:0#564 id: partition:0/offset:564 time: 2020-02-10T18:10:23.861866615Z datacontenttype: application/json Extensions, key: Data, { \"msg\": \"This is a test!\" }","title":"Verify"},{"location":"eventing/samples/kafka/source/#teardown-steps","text":"Remove the Apache Kafka Event Source $ kubectl delete -f source/source.yaml kafkasource.sources.knative.dev \"kafka-source\" deleted Remove the Event Display $ kubectl delete -f source/event-display.yaml service.serving.knative.dev \"event-display\" deleted Remove the Apache Kafka Event Controller $ kubectl delete -f https://storage.googleapis.com/knative-releases/eventing-contrib/latest/kafka-source.yaml serviceaccount \"kafka-controller-manager\" deleted clusterrole.rbac.authorization.k8s.io \"eventing-sources-kafka-controller\" deleted clusterrolebinding.rbac.authorization.k8s.io \"eventing-sources-kafka-controller\" deleted customresourcedefinition.apiextensions.k8s.io \"kafkasources.sources.knative.dev\" deleted service \"kafka-controller\" deleted statefulset.apps \"kafka-controller-manager\" deleted (Optional) Remove the Apache Kafka Topic $ kubectl delete -f kafka-topic.yaml kafkatopic.kafka.strimzi.io \"knative-demo-topic\" deleted","title":"Teardown Steps"},{"location":"eventing/samples/kafka/source/#optional-specify-the-key-deserializer","text":"When KafkaSource receives a message from Kafka, it dumps the key in the Event extension called Key and dumps Kafka message headers in the extensions starting with kafkaheader . You can specify the key deserializer among four types: string (default) for UTF-8 encoded strings int for 32-bit & 64-bit signed integers float for 32-bit & 64-bit floating points byte-array for a Base64 encoded byte array To specify it, add the label kafkasources.sources.knative.dev/key-type to the KafkaSource definition like: apiVersion : sources.knative.dev/v1beta1 kind : KafkaSource metadata : name : kafka-source labels : kafkasources.sources.knative.dev/key-type : int spec : consumerGroup : knative-group bootstrapServers : - my-cluster-kafka-bootstrap.kafka:9092 # note the kafka namespace topics : - knative-demo-topic sink : ref : apiVersion : serving.knative.dev/v1 kind : Service name : event-display","title":"(Optional) Specify the key deserializer"},{"location":"eventing/samples/kafka/source/#connecting-to-a-tls-enabled-kafka-broker","text":"The KafkaSource supports TLS and SASL authentication methods. For enabling TLS authentication, please have the below files CA Certificate Client Certificate and Key KafkaSource expects these files to be in pem format, if it is in other format like jks, please convert to pem. Create the certificate files as secrets in the namespace where KafkaSource is going to be set up $ kubectl create secret generic cacert --from-file=caroot.pem secret/cacert created $ kubectl create secret tls kafka-secret --cert=certificate.pem --key=key.pem secret/key created Apply the KafkaSource, change bootstrapServers and topics accordingly. ```yaml apiVersion: sources.knative.dev/v1beta1 kind: KafkaSource metadata: name: kafka-source-with-tls spec: net: tls: enable: true cert: secretKeyRef: key: tls.crt name: kafka-secret key: secretKeyRef: key: tls.key name: kafka-secret caCert: secretKeyRef: key: caroot.pem name: cacert consumerGroup: knative-group bootstrapServers: my-secure-kafka-bootstrap.kafka:443 topics: knative-demo-topic sink: ref: apiVersion: serving.knative.dev/v1 kind: Service name: event-display ```","title":"Connecting to a TLS enabled Kafka broker"},{"location":"eventing/samples/kubernetes-event-source/","text":"This example shows how to wire Kubernetes cluster events , using the API Server Source, for consumption by a function that has been implemented as a Knative Service. Before you begin \u00b6 You must have a Knative cluster running both the Serving and Eventing components. To learn how to install the required components, see Installing Knative . You can follow the steps below to create new files, or you clone a copy from the repo by running: git clone -b \"release-0.13\" https://github.com/knative/docs knative-docs cd knative-docs/docs/eventing/samples/kubernetes-event-source Deployment Steps \u00b6 Broker \u00b6 These instructions assume the namespace default , which you can change to your preferred namespace. If you use a different namespace, you will need to modify all the YAML files deployed in this sample to point at that namespace. Create the default Broker in your namespace: kubectl create -f - <<EOF apiVersion: eventing.knative.dev/v1 kind: Broker metadata: name: default EOF Service Account \u00b6 Create a Service Account that the ApiServerSource runs as. The ApiServerSource watches for Kubernetes events and forwards them to the Knative Eventing Broker. Create a file named serviceaccount.yaml and copy the code block below into it. apiVersion : v1 kind : ServiceAccount metadata : name : events-sa namespace : default --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : event-watcher rules : - apiGroups : - \"\" resources : - events verbs : - get - list - watch --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : k8s-ra-event-watcher roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : event-watcher subjects : - kind : ServiceAccount name : events-sa namespace : default If you want to re-use an existing Service Account with the appropriate permissions, you need to modify the serviceaccount.yaml . Enter the following command to create the service account from serviceaccount.yaml : kubectl apply --filename serviceaccount.yaml Create Event Source for Kubernetes Events \u00b6 In order to receive events, you have to create a concrete Event Source for a specific namespace. Create a file named k8s-events.yaml and copy the code block below into it. apiVersion : sources.knative.dev/v1 kind : ApiServerSource metadata : name : testevents namespace : default spec : serviceAccountName : events-sa mode : Resource resources : - apiVersion : v1 kind : Event sink : ref : apiVersion : eventing.knative.dev/v1 kind : Broker name : default If you want to consume events from a different namespace or use a different Service Account , you need to modify k8s-events.yaml accordingly. Enter the following command to create the event source: kubectl apply --filename k8s-events.yaml Trigger \u00b6 In order to check the ApiServerSource is fully working, we will create a simple Knative Service that dumps incoming messages to its log and creates a Trigger from the Broker to that Knative Service. Create a file named trigger.yaml and copy the code block below into it. apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : testevents-trigger namespace : default spec : broker : default subscriber : ref : apiVersion : serving.knative.dev/v1 kind : Service name : event-display --- # This is a very simple Knative Service that writes the input request to its log. apiVersion : serving.knative.dev/v1 kind : Service metadata : name : event-display namespace : default spec : template : spec : containers : - # This corresponds to # https://github.com/knative/eventing-contrib/tree/main/cmd/event_display/main.go image : gcr.io/knative-releases/knative.dev/eventing-contrib/cmd/event_display If the deployed ApiServerSource is pointing at a Broker other than default , modify trigger.yaml by adding spec.broker with the Broker 's name. Deploy trigger.yaml : kubectl apply --filename trigger.yaml Create Events \u00b6 Create events by launching a pod in the default namespace. Create a busybox container and immediately delete it: kubectl run busybox --image = busybox --restart = Never -- ls kubectl delete pod busybox Verify \u00b6 We will verify that the Kubernetes events were sent into the Knative eventing system by looking at our message dumper function logs. If you deployed the Trigger , continue using this section. If not, you will need to look downstream yourself: kubectl get pods kubectl logs -l serving.knative.dev/service = event-display -c user-container You should see log lines similar to: \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1.0 type: dev.knative.apiserver.resource.update source: https://10.96.0.1:443 subject: /apis/v1/namespaces/default/events/testevents.15dd3050eb1e6f50 id: e0447eb7-36b5-443b-9d37-faf4fe5c62f0 time: 2020-07-28T16:35:14.172979816Z datacontenttype: application/json Extensions, kind: Event knativearrivaltime: 2020-07-28T16:35:14.173381505Z knativehistory: default-kne-trigger-kn-channel.default.svc.cluster.local name: busybox.1625f7cfa4cd12f8 namespace: default Data, { \"apiVersion\": \"v1\", \"count\": 1, \"eventTime\": null, \"firstTimestamp\": \"2020-07-28T16:35:14Z\", \"involvedObject\": { \"apiVersion\": \"v1\", \"fieldPath\": \"spec.containers{busybox}\", \"kind\": \"Pod\", \"name\": \"busybox\", \"namespace\": \"default\", \"resourceVersion\": \"28987493\", \"uid\": \"1efb342a-737b-11e9-a6c5-42010a8a00ed\" }, \"kind\": \"Event\", \"lastTimestamp\": \"2020-07-28T16:35:14Z\", \"message\": \"Started container\", \"metadata\": { \"creationTimestamp\": \"2020-07-28T16:35:14Z\", \"name\": \"busybox.1625f7cfa4cd12f8\", \"namespace\": \"default\", \"resourceVersion\": \"506088\", \"selfLink\": \"/api/v1/namespaces/default/events/busybox.1625f7cfa4cd12f8\", \"uid\": \"7f841049-7979-48db-9cbc-93ed2346a1b5\", }, \"reason\": \"Started\", \"reportingComponent\": \"\", \"reportingInstance\": \"\", \"source\": { \"component\": \"kubelet\", \"host\": \"gke-knative-auto-cluster-default-pool-23c23c4f-xdj0\" }, \"type\": \"Normal\" } Cleanup \u00b6 You can remove the ServiceAccount , ClusterRoles , ClusterRoleBinding , ApiServerSource , Service and Trigger using: kubectl --namespace default delete --filename serviceaccount.yaml kubectl --namespace default delete --filename k8s-events.yaml kubectl --namespace default delete --filename trigger.yaml","title":"Kubernetes Event Source"},{"location":"eventing/samples/kubernetes-event-source/#before-you-begin","text":"You must have a Knative cluster running both the Serving and Eventing components. To learn how to install the required components, see Installing Knative . You can follow the steps below to create new files, or you clone a copy from the repo by running: git clone -b \"release-0.13\" https://github.com/knative/docs knative-docs cd knative-docs/docs/eventing/samples/kubernetes-event-source","title":"Before you begin"},{"location":"eventing/samples/kubernetes-event-source/#deployment-steps","text":"","title":"Deployment Steps"},{"location":"eventing/samples/kubernetes-event-source/#broker","text":"These instructions assume the namespace default , which you can change to your preferred namespace. If you use a different namespace, you will need to modify all the YAML files deployed in this sample to point at that namespace. Create the default Broker in your namespace: kubectl create -f - <<EOF apiVersion: eventing.knative.dev/v1 kind: Broker metadata: name: default EOF","title":"Broker"},{"location":"eventing/samples/kubernetes-event-source/#service-account","text":"Create a Service Account that the ApiServerSource runs as. The ApiServerSource watches for Kubernetes events and forwards them to the Knative Eventing Broker. Create a file named serviceaccount.yaml and copy the code block below into it. apiVersion : v1 kind : ServiceAccount metadata : name : events-sa namespace : default --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : event-watcher rules : - apiGroups : - \"\" resources : - events verbs : - get - list - watch --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : k8s-ra-event-watcher roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : event-watcher subjects : - kind : ServiceAccount name : events-sa namespace : default If you want to re-use an existing Service Account with the appropriate permissions, you need to modify the serviceaccount.yaml . Enter the following command to create the service account from serviceaccount.yaml : kubectl apply --filename serviceaccount.yaml","title":"Service Account"},{"location":"eventing/samples/kubernetes-event-source/#create-event-source-for-kubernetes-events","text":"In order to receive events, you have to create a concrete Event Source for a specific namespace. Create a file named k8s-events.yaml and copy the code block below into it. apiVersion : sources.knative.dev/v1 kind : ApiServerSource metadata : name : testevents namespace : default spec : serviceAccountName : events-sa mode : Resource resources : - apiVersion : v1 kind : Event sink : ref : apiVersion : eventing.knative.dev/v1 kind : Broker name : default If you want to consume events from a different namespace or use a different Service Account , you need to modify k8s-events.yaml accordingly. Enter the following command to create the event source: kubectl apply --filename k8s-events.yaml","title":"Create Event Source for Kubernetes Events"},{"location":"eventing/samples/kubernetes-event-source/#trigger","text":"In order to check the ApiServerSource is fully working, we will create a simple Knative Service that dumps incoming messages to its log and creates a Trigger from the Broker to that Knative Service. Create a file named trigger.yaml and copy the code block below into it. apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : testevents-trigger namespace : default spec : broker : default subscriber : ref : apiVersion : serving.knative.dev/v1 kind : Service name : event-display --- # This is a very simple Knative Service that writes the input request to its log. apiVersion : serving.knative.dev/v1 kind : Service metadata : name : event-display namespace : default spec : template : spec : containers : - # This corresponds to # https://github.com/knative/eventing-contrib/tree/main/cmd/event_display/main.go image : gcr.io/knative-releases/knative.dev/eventing-contrib/cmd/event_display If the deployed ApiServerSource is pointing at a Broker other than default , modify trigger.yaml by adding spec.broker with the Broker 's name. Deploy trigger.yaml : kubectl apply --filename trigger.yaml","title":"Trigger"},{"location":"eventing/samples/kubernetes-event-source/#create-events","text":"Create events by launching a pod in the default namespace. Create a busybox container and immediately delete it: kubectl run busybox --image = busybox --restart = Never -- ls kubectl delete pod busybox","title":"Create Events"},{"location":"eventing/samples/kubernetes-event-source/#verify","text":"We will verify that the Kubernetes events were sent into the Knative eventing system by looking at our message dumper function logs. If you deployed the Trigger , continue using this section. If not, you will need to look downstream yourself: kubectl get pods kubectl logs -l serving.knative.dev/service = event-display -c user-container You should see log lines similar to: \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1.0 type: dev.knative.apiserver.resource.update source: https://10.96.0.1:443 subject: /apis/v1/namespaces/default/events/testevents.15dd3050eb1e6f50 id: e0447eb7-36b5-443b-9d37-faf4fe5c62f0 time: 2020-07-28T16:35:14.172979816Z datacontenttype: application/json Extensions, kind: Event knativearrivaltime: 2020-07-28T16:35:14.173381505Z knativehistory: default-kne-trigger-kn-channel.default.svc.cluster.local name: busybox.1625f7cfa4cd12f8 namespace: default Data, { \"apiVersion\": \"v1\", \"count\": 1, \"eventTime\": null, \"firstTimestamp\": \"2020-07-28T16:35:14Z\", \"involvedObject\": { \"apiVersion\": \"v1\", \"fieldPath\": \"spec.containers{busybox}\", \"kind\": \"Pod\", \"name\": \"busybox\", \"namespace\": \"default\", \"resourceVersion\": \"28987493\", \"uid\": \"1efb342a-737b-11e9-a6c5-42010a8a00ed\" }, \"kind\": \"Event\", \"lastTimestamp\": \"2020-07-28T16:35:14Z\", \"message\": \"Started container\", \"metadata\": { \"creationTimestamp\": \"2020-07-28T16:35:14Z\", \"name\": \"busybox.1625f7cfa4cd12f8\", \"namespace\": \"default\", \"resourceVersion\": \"506088\", \"selfLink\": \"/api/v1/namespaces/default/events/busybox.1625f7cfa4cd12f8\", \"uid\": \"7f841049-7979-48db-9cbc-93ed2346a1b5\", }, \"reason\": \"Started\", \"reportingComponent\": \"\", \"reportingInstance\": \"\", \"source\": { \"component\": \"kubelet\", \"host\": \"gke-knative-auto-cluster-default-pool-23c23c4f-xdj0\" }, \"type\": \"Normal\" }","title":"Verify"},{"location":"eventing/samples/kubernetes-event-source/#cleanup","text":"You can remove the ServiceAccount , ClusterRoles , ClusterRoleBinding , ApiServerSource , Service and Trigger using: kubectl --namespace default delete --filename serviceaccount.yaml kubectl --namespace default delete --filename k8s-events.yaml kubectl --namespace default delete --filename trigger.yaml","title":"Cleanup"},{"location":"eventing/samples/parallel/","text":"The following examples will help you understand how to use Parallel to describe various flows. Prerequisites \u00b6 All examples require: A Kubernetes cluster with Knative Eventing Knative Serving All examples are using the default channel template . Examples \u00b6 For each of these examples below, we'll use PingSource as the source of events. We also use simple functions to perform trivial filtering, transformation and routing of the incoming events. The examples are: Parallel with multiple branches and global reply Parallel with mutually exclusive cases","title":"Overview"},{"location":"eventing/samples/parallel/#prerequisites","text":"All examples require: A Kubernetes cluster with Knative Eventing Knative Serving All examples are using the default channel template .","title":"Prerequisites"},{"location":"eventing/samples/parallel/#examples","text":"For each of these examples below, we'll use PingSource as the source of events. We also use simple functions to perform trivial filtering, transformation and routing of the incoming events. The examples are: Parallel with multiple branches and global reply Parallel with mutually exclusive cases","title":"Examples"},{"location":"eventing/samples/parallel/_index/","text":"Macro Rendering Error \u00b6 TemplateSyntaxError : unexpected '%' Traceback (most recent call last): File \"/usr/local/lib/python3.9/site-packages/mkdocs_macros/plugin.py\", line 441, in render md_template = self.env.from_string(markdown) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 941, in from_string return cls.from_code(self, self.compile(source), globals, None) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 638, in compile self.handle_exception(source=source_hint) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 832, in handle_exception reraise(*rewrite_traceback_stack(source=source)) File \"/usr/local/lib/python3.9/site-packages/jinja2/_compat.py\", line 28, in reraise raise value.with_traceback(tb) File \"<unknown>\", line 3, in template jinja2.exceptions.TemplateSyntaxError: unexpected '%'","title":"Parallel Example"},{"location":"eventing/samples/parallel/_index/#macro-rendering-error","text":"TemplateSyntaxError : unexpected '%' Traceback (most recent call last): File \"/usr/local/lib/python3.9/site-packages/mkdocs_macros/plugin.py\", line 441, in render md_template = self.env.from_string(markdown) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 941, in from_string return cls.from_code(self, self.compile(source), globals, None) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 638, in compile self.handle_exception(source=source_hint) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 832, in handle_exception reraise(*rewrite_traceback_stack(source=source)) File \"/usr/local/lib/python3.9/site-packages/jinja2/_compat.py\", line 28, in reraise raise value.with_traceback(tb) File \"<unknown>\", line 3, in template jinja2.exceptions.TemplateSyntaxError: unexpected '%'","title":"Macro Rendering Error"},{"location":"eventing/samples/parallel/multiple-branches/","text":"We are going to create a Parallel with two branches: the first branch accepts events with a time that is is even the second branch accepts events with a time that is is odd The events produced by each branch are then sent to the event-display service. Prerequisites \u00b6 Please refer to the sample overview for the prerequisites . Create the Knative Services \u00b6 Let's first create the filter and transformer services that we will use in our Parallel. apiVersion : serving.knative.dev/v1 kind : Service metadata : name : even-filter spec : template : spec : containers : - image : villardl/filter-nodejs:0.1 env : - name : FILTER value : | Math.round(Date.parse(event.time) / 60000) % 2 === 0 --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : odd-filter spec : template : spec : containers : - image : villardl/filter-nodejs:0.1 env : - name : FILTER value : | Math.round(Date.parse(event.time) / 60000) % 2 !== 0 --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : even-transformer spec : template : spec : containers : - image : villardl/transformer-nodejs env : - name : TRANSFORMER value : | ({\"message\": \"we are even!\"}) --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : odd-transformer spec : template : spec : containers : - image : villardl/transformer-nodejs:0.1 env : - name : TRANSFORMER value : | ({\"message\": \"this is odd!\"}) . kubectl create -f ./filters.yaml -f ./transformers.yaml Create the Service displaying the events created by Sequence \u00b6 apiVersion : serving.knative.dev/v1 kind : Service metadata : name : event-display spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing-contrib/cmd/event_display Change default below to create the Sequence in the Namespace where you want your resources to be created. kubectl -n default create -f ./event-display.yaml Create the Parallel \u00b6 The parallel.yaml file contains the specifications for creating the Parallel. apiVersion : flows.knative.dev/v1 kind : Parallel metadata : name : odd-even-parallel spec : channelTemplate : apiVersion : messaging.knative.dev/v1 kind : InMemoryChannel branches : - filter : ref : apiVersion : serving.knative.dev/v1 kind : Service name : even-filter subscriber : ref : apiVersion : serving.knative.dev/v1 kind : Service name : even-transformer - filter : ref : apiVersion : serving.knative.dev/v1 kind : Service name : odd-filter subscriber : ref : apiVersion : serving.knative.dev/v1 kind : Service name : odd-transformer reply : ref : apiVersion : serving.knative.dev/v1 kind : Service name : event-display kubectl create -f ./parallel.yaml Create the PingSource targeting the Parallel \u00b6 This will create a PingSource which will send a CloudEvent with {\"message\": \"Even or odd?\"} as the data payload every minute. apiVersion : sources.knative.dev/v1beta2 kind : PingSource metadata : name : ping-source spec : schedule : \"*/1 * * * *\" contentType : \"application/json\" data : '{\"message\": \"Even or odd?\"}' sink : ref : apiVersion : flows.knative.dev/v1 kind : Parallel name : odd-even-parallel kubectl create -f ./ping-source.yaml Inspecting the results \u00b6 You can now see the final output by inspecting the logs of the event-display pods. Note that since we set the PingSource to emit every minute, it might take some time for the events to show up in the logs. Let's look at the event-display log: kubectl logs -l serving.knative.dev/service = event-display --tail = 30 -c user-container \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1 .0 type: dev.knative.sources.ping source: /apis/v1/namespaces/default/pingsources/ping-source id: 015a4cf4-8a43-44a9-8702-3d4171d27ba5 time: 2020 -03-03T21:24:00.0007254Z datacontenttype: application/json ; charset = utf-8 Extensions, knativehistory: odd-even-parallel-kn-parallel-kn-channel.default.svc.cluster.local ; odd-even-parallel-kn-parallel-0-kn-channel.default.svc.cluster.local traceparent: 00 -41a139bf073f3cfcba7bb7ce7f1488fc-68a891ace985221a-00 Data, { \"message\" : \"we are even!\" } \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1 .0 type: dev.knative.sources.ping source: /apis/v1/namespaces/default/pingsources/ping-source id: 52e6b097-f914-4b5a-8539-165650e85bcd time: 2020 -03-03T21:23:00.0004662Z datacontenttype: application/json ; charset = utf-8 Extensions, knativehistory: odd-even-parallel-kn-parallel-kn-channel.default.svc.cluster.local ; odd-even-parallel-kn-parallel-1-kn-channel.default.svc.cluster.local traceparent: 00 -58d371410d7daf2033be226860b4ee5d-05d686ee90c3226f-00 Data, { \"message\" : \"this is odd!\" }","title":"Multiple Cases"},{"location":"eventing/samples/parallel/multiple-branches/#prerequisites","text":"Please refer to the sample overview for the prerequisites .","title":"Prerequisites"},{"location":"eventing/samples/parallel/multiple-branches/#create-the-knative-services","text":"Let's first create the filter and transformer services that we will use in our Parallel. apiVersion : serving.knative.dev/v1 kind : Service metadata : name : even-filter spec : template : spec : containers : - image : villardl/filter-nodejs:0.1 env : - name : FILTER value : | Math.round(Date.parse(event.time) / 60000) % 2 === 0 --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : odd-filter spec : template : spec : containers : - image : villardl/filter-nodejs:0.1 env : - name : FILTER value : | Math.round(Date.parse(event.time) / 60000) % 2 !== 0 --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : even-transformer spec : template : spec : containers : - image : villardl/transformer-nodejs env : - name : TRANSFORMER value : | ({\"message\": \"we are even!\"}) --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : odd-transformer spec : template : spec : containers : - image : villardl/transformer-nodejs:0.1 env : - name : TRANSFORMER value : | ({\"message\": \"this is odd!\"}) . kubectl create -f ./filters.yaml -f ./transformers.yaml","title":"Create the Knative Services"},{"location":"eventing/samples/parallel/multiple-branches/#create-the-service-displaying-the-events-created-by-sequence","text":"apiVersion : serving.knative.dev/v1 kind : Service metadata : name : event-display spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing-contrib/cmd/event_display Change default below to create the Sequence in the Namespace where you want your resources to be created. kubectl -n default create -f ./event-display.yaml","title":"Create the Service displaying the events created by Sequence"},{"location":"eventing/samples/parallel/multiple-branches/#create-the-parallel","text":"The parallel.yaml file contains the specifications for creating the Parallel. apiVersion : flows.knative.dev/v1 kind : Parallel metadata : name : odd-even-parallel spec : channelTemplate : apiVersion : messaging.knative.dev/v1 kind : InMemoryChannel branches : - filter : ref : apiVersion : serving.knative.dev/v1 kind : Service name : even-filter subscriber : ref : apiVersion : serving.knative.dev/v1 kind : Service name : even-transformer - filter : ref : apiVersion : serving.knative.dev/v1 kind : Service name : odd-filter subscriber : ref : apiVersion : serving.knative.dev/v1 kind : Service name : odd-transformer reply : ref : apiVersion : serving.knative.dev/v1 kind : Service name : event-display kubectl create -f ./parallel.yaml","title":"Create the Parallel"},{"location":"eventing/samples/parallel/multiple-branches/#create-the-pingsource-targeting-the-parallel","text":"This will create a PingSource which will send a CloudEvent with {\"message\": \"Even or odd?\"} as the data payload every minute. apiVersion : sources.knative.dev/v1beta2 kind : PingSource metadata : name : ping-source spec : schedule : \"*/1 * * * *\" contentType : \"application/json\" data : '{\"message\": \"Even or odd?\"}' sink : ref : apiVersion : flows.knative.dev/v1 kind : Parallel name : odd-even-parallel kubectl create -f ./ping-source.yaml","title":"Create the PingSource targeting the Parallel"},{"location":"eventing/samples/parallel/multiple-branches/#inspecting-the-results","text":"You can now see the final output by inspecting the logs of the event-display pods. Note that since we set the PingSource to emit every minute, it might take some time for the events to show up in the logs. Let's look at the event-display log: kubectl logs -l serving.knative.dev/service = event-display --tail = 30 -c user-container \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1 .0 type: dev.knative.sources.ping source: /apis/v1/namespaces/default/pingsources/ping-source id: 015a4cf4-8a43-44a9-8702-3d4171d27ba5 time: 2020 -03-03T21:24:00.0007254Z datacontenttype: application/json ; charset = utf-8 Extensions, knativehistory: odd-even-parallel-kn-parallel-kn-channel.default.svc.cluster.local ; odd-even-parallel-kn-parallel-0-kn-channel.default.svc.cluster.local traceparent: 00 -41a139bf073f3cfcba7bb7ce7f1488fc-68a891ace985221a-00 Data, { \"message\" : \"we are even!\" } \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1 .0 type: dev.knative.sources.ping source: /apis/v1/namespaces/default/pingsources/ping-source id: 52e6b097-f914-4b5a-8539-165650e85bcd time: 2020 -03-03T21:23:00.0004662Z datacontenttype: application/json ; charset = utf-8 Extensions, knativehistory: odd-even-parallel-kn-parallel-kn-channel.default.svc.cluster.local ; odd-even-parallel-kn-parallel-1-kn-channel.default.svc.cluster.local traceparent: 00 -58d371410d7daf2033be226860b4ee5d-05d686ee90c3226f-00 Data, { \"message\" : \"this is odd!\" }","title":"Inspecting the results"},{"location":"eventing/samples/parallel/multiple-branches/_index/","text":"Macro Rendering Error \u00b6 TemplateSyntaxError : unexpected '%' Traceback (most recent call last): File \"/usr/local/lib/python3.9/site-packages/mkdocs_macros/plugin.py\", line 441, in render md_template = self.env.from_string(markdown) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 941, in from_string return cls.from_code(self, self.compile(source), globals, None) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 638, in compile self.handle_exception(source=source_hint) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 832, in handle_exception reraise(*rewrite_traceback_stack(source=source)) File \"/usr/local/lib/python3.9/site-packages/jinja2/_compat.py\", line 28, in reraise raise value.with_traceback(tb) File \"<unknown>\", line 3, in template jinja2.exceptions.TemplateSyntaxError: unexpected '%'","title":"Multiple Cases"},{"location":"eventing/samples/parallel/multiple-branches/_index/#macro-rendering-error","text":"TemplateSyntaxError : unexpected '%' Traceback (most recent call last): File \"/usr/local/lib/python3.9/site-packages/mkdocs_macros/plugin.py\", line 441, in render md_template = self.env.from_string(markdown) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 941, in from_string return cls.from_code(self, self.compile(source), globals, None) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 638, in compile self.handle_exception(source=source_hint) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 832, in handle_exception reraise(*rewrite_traceback_stack(source=source)) File \"/usr/local/lib/python3.9/site-packages/jinja2/_compat.py\", line 28, in reraise raise value.with_traceback(tb) File \"<unknown>\", line 3, in template jinja2.exceptions.TemplateSyntaxError: unexpected '%'","title":"Macro Rendering Error"},{"location":"eventing/samples/parallel/mutual-exclusivity/","text":"In this example, we are going to see how we can create a Parallel with mutually exclusive branches. This example is the same as the multiple branches example except that we are now going to rely on the Knative switch function to provide a soft mutual exclusivity guarantee. NOTE: this example must be deployed in the default namespace. Prerequisites \u00b6 Please refer to the sample overview for the prerequisites . Create the Knative Services \u00b6 Let's first create the switcher and transformer services that we will use in our Parallel. apiVersion : serving.knative.dev/v1 kind : Service metadata : name : me-even-odd-switcher spec : template : spec : containers : - image : villardl/switcher-nodejs:0.1 env : - name : EXPRESSION value : Math.round(Date.parse(event.time) / 60000) % 2 - name : CASES value : '[0, 1]' --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : even-transformer spec : template : spec : containers : - image : villardl/transformer-nodejs:0.1 env : - name : TRANSFORMER value : | ({\"message\": \"we are even!\"}) --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : odd-transformer spec : template : spec : containers : - image : villardl/transformer-nodejs:0.1 env : - name : TRANSFORMER value : | ({\"message\": \"this is odd!\"}) . kubectl create -f ./switcher.yaml -f ./transformers.yaml Create the Service displaying the events created by Parallel \u00b6 apiVersion : serving.knative.dev/v1 kind : Service metadata : name : event-display spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing-contrib/cmd/event_display kubectl -n default create -f ./event-display.yaml Create the Parallel object \u00b6 The parallel.yaml file contains the specifications for creating the Parallel object. apiVersion : flows.knative.dev/v1 kind : Parallel metadata : name : me-odd-even-parallel spec : channelTemplate : apiVersion : messaging.knative.dev/v1 kind : InMemoryChannel branches : - filter : uri : \"http://me-even-odd-switcher.default.svc.cluster.local/0\" subscriber : ref : apiVersion : serving.knative.dev/v1 kind : Service name : me-even-transformer - filter : uri : \"http://me-even-odd-switcher.default.svc.cluster.local/1\" subscriber : ref : apiVersion : serving.knative.dev/v1 kind : Service name : me-odd-transformer reply : ref : apiVersion : serving.knative.dev/v1 kind : Service name : me-event-display kubectl create -f ./parallel.yaml Create the PingSource targeting the Parallel object \u00b6 This will create a PingSource which will send a CloudEvent with {\"message\": \"Even or odd?\"} as the data payload every minute. apiVersion : sources.knative.dev/v1beta2 kind : PingSource metadata : name : me-ping-source spec : schedule : \"*/1 * * * *\" contentType : \"application/json\" data : '{\"message\": \"Even or odd?\"}' sink : ref : apiVersion : flows.knative.dev/v1 kind : Parallel name : me-odd-even-parallel kubectl create -f ./ping-source.yaml Inspecting the results \u00b6 You can now see the final output by inspecting the logs of the me-event-display pods. Note that since we set the PingSource to emit every minute, it might take some time for the events to show up in the logs. Let's look at the me-event-display log: kubectl logs -l serving.knative.dev/service = me-event-display --tail = 50 -c user-container \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1 .0 type: dev.knative.sources.ping source: /apis/v1/namespaces/default/pingsources/me-ping-source id: 0b0db15c-9b36-4388-8eaa-8c23a9dc2707 time: 2020 -03-03T21:30:00.0007292Z datacontenttype: application/json ; charset = utf-8 Extensions, knativehistory: me-odd-even-parallel-kn-parallel-kn-channel.default.svc.cluster.local ; me-odd-even-parallel-kn-parallel-0-kn-channel.default.svc.cluster.local traceparent: 00 -e8b17109cd21d4fa66a86633b5a614c7-ba96d220fe13211c-00 Data, { \"message\" : \"we are even!\" } \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1 .0 type: dev.knative.sources.ping source: /apis/v1/namespaces/default/pingsources/me-ping-source id: 321170d1-dea7-4b18-9290-28adb1de089b time: 2020 -03-03T21:31:00.0007847Z datacontenttype: application/json ; charset = utf-8 Extensions, knativehistory: me-odd-even-parallel-kn-parallel-kn-channel.default.svc.cluster.local ; me-odd-even-parallel-kn-parallel-1-kn-channel.default.svc.cluster.local traceparent: 00 -78d8b044d23c85789f0a13fd3679ac24-1d69ddde56d620c7-00 Data, { \"message\" : \"this is odd!\" }","title":"Mutual Exclusivity"},{"location":"eventing/samples/parallel/mutual-exclusivity/#prerequisites","text":"Please refer to the sample overview for the prerequisites .","title":"Prerequisites"},{"location":"eventing/samples/parallel/mutual-exclusivity/#create-the-knative-services","text":"Let's first create the switcher and transformer services that we will use in our Parallel. apiVersion : serving.knative.dev/v1 kind : Service metadata : name : me-even-odd-switcher spec : template : spec : containers : - image : villardl/switcher-nodejs:0.1 env : - name : EXPRESSION value : Math.round(Date.parse(event.time) / 60000) % 2 - name : CASES value : '[0, 1]' --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : even-transformer spec : template : spec : containers : - image : villardl/transformer-nodejs:0.1 env : - name : TRANSFORMER value : | ({\"message\": \"we are even!\"}) --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : odd-transformer spec : template : spec : containers : - image : villardl/transformer-nodejs:0.1 env : - name : TRANSFORMER value : | ({\"message\": \"this is odd!\"}) . kubectl create -f ./switcher.yaml -f ./transformers.yaml","title":"Create the Knative Services"},{"location":"eventing/samples/parallel/mutual-exclusivity/#create-the-service-displaying-the-events-created-by-parallel","text":"apiVersion : serving.knative.dev/v1 kind : Service metadata : name : event-display spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing-contrib/cmd/event_display kubectl -n default create -f ./event-display.yaml","title":"Create the Service displaying the events created by Parallel"},{"location":"eventing/samples/parallel/mutual-exclusivity/#create-the-parallel-object","text":"The parallel.yaml file contains the specifications for creating the Parallel object. apiVersion : flows.knative.dev/v1 kind : Parallel metadata : name : me-odd-even-parallel spec : channelTemplate : apiVersion : messaging.knative.dev/v1 kind : InMemoryChannel branches : - filter : uri : \"http://me-even-odd-switcher.default.svc.cluster.local/0\" subscriber : ref : apiVersion : serving.knative.dev/v1 kind : Service name : me-even-transformer - filter : uri : \"http://me-even-odd-switcher.default.svc.cluster.local/1\" subscriber : ref : apiVersion : serving.knative.dev/v1 kind : Service name : me-odd-transformer reply : ref : apiVersion : serving.knative.dev/v1 kind : Service name : me-event-display kubectl create -f ./parallel.yaml","title":"Create the Parallel object"},{"location":"eventing/samples/parallel/mutual-exclusivity/#create-the-pingsource-targeting-the-parallel-object","text":"This will create a PingSource which will send a CloudEvent with {\"message\": \"Even or odd?\"} as the data payload every minute. apiVersion : sources.knative.dev/v1beta2 kind : PingSource metadata : name : me-ping-source spec : schedule : \"*/1 * * * *\" contentType : \"application/json\" data : '{\"message\": \"Even or odd?\"}' sink : ref : apiVersion : flows.knative.dev/v1 kind : Parallel name : me-odd-even-parallel kubectl create -f ./ping-source.yaml","title":"Create the PingSource targeting the Parallel object"},{"location":"eventing/samples/parallel/mutual-exclusivity/#inspecting-the-results","text":"You can now see the final output by inspecting the logs of the me-event-display pods. Note that since we set the PingSource to emit every minute, it might take some time for the events to show up in the logs. Let's look at the me-event-display log: kubectl logs -l serving.knative.dev/service = me-event-display --tail = 50 -c user-container \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1 .0 type: dev.knative.sources.ping source: /apis/v1/namespaces/default/pingsources/me-ping-source id: 0b0db15c-9b36-4388-8eaa-8c23a9dc2707 time: 2020 -03-03T21:30:00.0007292Z datacontenttype: application/json ; charset = utf-8 Extensions, knativehistory: me-odd-even-parallel-kn-parallel-kn-channel.default.svc.cluster.local ; me-odd-even-parallel-kn-parallel-0-kn-channel.default.svc.cluster.local traceparent: 00 -e8b17109cd21d4fa66a86633b5a614c7-ba96d220fe13211c-00 Data, { \"message\" : \"we are even!\" } \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1 .0 type: dev.knative.sources.ping source: /apis/v1/namespaces/default/pingsources/me-ping-source id: 321170d1-dea7-4b18-9290-28adb1de089b time: 2020 -03-03T21:31:00.0007847Z datacontenttype: application/json ; charset = utf-8 Extensions, knativehistory: me-odd-even-parallel-kn-parallel-kn-channel.default.svc.cluster.local ; me-odd-even-parallel-kn-parallel-1-kn-channel.default.svc.cluster.local traceparent: 00 -78d8b044d23c85789f0a13fd3679ac24-1d69ddde56d620c7-00 Data, { \"message\" : \"this is odd!\" }","title":"Inspecting the results"},{"location":"eventing/samples/parallel/mutual-exclusivity/_index/","text":"Macro Rendering Error \u00b6 TemplateSyntaxError : unexpected '%' Traceback (most recent call last): File \"/usr/local/lib/python3.9/site-packages/mkdocs_macros/plugin.py\", line 441, in render md_template = self.env.from_string(markdown) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 941, in from_string return cls.from_code(self, self.compile(source), globals, None) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 638, in compile self.handle_exception(source=source_hint) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 832, in handle_exception reraise(*rewrite_traceback_stack(source=source)) File \"/usr/local/lib/python3.9/site-packages/jinja2/_compat.py\", line 28, in reraise raise value.with_traceback(tb) File \"<unknown>\", line 3, in template jinja2.exceptions.TemplateSyntaxError: unexpected '%'","title":"Mutual Exclusive Cases"},{"location":"eventing/samples/parallel/mutual-exclusivity/_index/#macro-rendering-error","text":"TemplateSyntaxError : unexpected '%' Traceback (most recent call last): File \"/usr/local/lib/python3.9/site-packages/mkdocs_macros/plugin.py\", line 441, in render md_template = self.env.from_string(markdown) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 941, in from_string return cls.from_code(self, self.compile(source), globals, None) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 638, in compile self.handle_exception(source=source_hint) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 832, in handle_exception reraise(*rewrite_traceback_stack(source=source)) File \"/usr/local/lib/python3.9/site-packages/jinja2/_compat.py\", line 28, in reraise raise value.with_traceback(tb) File \"<unknown>\", line 3, in template jinja2.exceptions.TemplateSyntaxError: unexpected '%'","title":"Macro Rendering Error"},{"location":"eventing/samples/ping-source/","text":"PingSource example \u00b6 This example shows how to configure PingSource as an event source targeting a Knative Service. Before you begin \u00b6 Set up Knative Serving . Set up Knative Eventing . Create a Knative Service \u00b6 To verify that PingSource is working, create a simple Knative Service that dumps incoming messages to its log. Kubectl Use following command to create the service from STDIN: cat <<EOF | kubectl create -f - apiVersion: serving.knative.dev/v1 kind: Service metadata: name: event-display spec: template: spec: containers: - image: gcr.io/knative-releases/knative.dev/eventing-contrib/cmd/event_display EOF Kn Use following command to create the service using the kn cli: kn service create event-display --image gcr.io/knative-releases/knative.dev/eventing-contrib/cmd/event_display Create a PingSource \u00b6 For each set of ping events that you want to request, create an Event Source in the same namespace as the destination. Kubectl Use following command to create the event source from STDIN: cat <<EOF | kubectl create -f - apiVersion: sources.knative.dev/v1beta2 kind: PingSource metadata: name: test-ping-source spec: schedule: \"*/2 * * * *\" contentType: \"application/json\" data: '{\"message\": \"Hello world!\"}' sink: ref: apiVersion: serving.knative.dev/v1 kind: Service name: event-display EOF Kn Use following command to create the event source from the ping-source.yaml file: kn source ping create test-ping-source \\ --schedule \"*/2 * * * *\" --data \\ '{ \"message\": \"Hello world!\" }' \\ --sink ksvc:event-display (Optional) Create a PingSource with binary data \u00b6 Sometimes you may want to send binary data, which cannot be directly serialized in yaml, to downstream. This can be achieved by using dataBase64 as the payload. As the name suggests, dataBase64 should carry data that is base64 encoded. Please note that data and dataBase64 cannot co-exist. Use the following command to create the event source with binary data from STDIN: cat <<EOF | kubectl create -f - apiVersion: sources.knative.dev/v1beta2 kind: PingSource metadata: name: test-ping-source-binary spec: schedule: \"*/2 * * * *\" contentType: \"text/plain\" dataBase64: \"ZGF0YQ==\" sink: ref: apiVersion: serving.knative.dev/v1 kind: Service name: event-display EOF Verify \u00b6 Verify that the message was sent to the Knative eventing system by looking at message dumper logs. Kubectl Use following command to view the logs of the event-display service: kubectl logs -l serving.knative.dev/service = event-display -c user-container --since = 10m Kail You can also use kail instead of kubectl logs to tail the logs of the subscriber. kail -l serving.knative.dev/service = event-display -c user-container --since = 10m You should see log lines showing the request headers and body from the source: \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1 .0 type: dev.knative.sources.ping source: /apis/v1/namespaces/default/pingsources/test-ping-source id: d8e761eb-30c7-49a3-a421-cd5895239f2d time: 2019 -12-04T14:24:00.000702251Z datacontenttype: application/json Data, { \"message\" : \"Hello world!\" } If you created a PingSource with binary data, you should also see the following: \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1 .0 type: dev.knative.sources.ping source: /apis/v1/namespaces/default/pingsources/test-ping-source-binary id: a195be33-ff65-49af-9045-0e0711d05e94 time: 2020 -11-17T19:48:00.48334181Z datacontenttype: text/plain Data, ZGF0YQ == Cleanup \u00b6 You can delete the PingSource instance by entering the following command: Kubectl kubectl delete pingsources.sources.knative.dev test-ping-source kubectl delete pingsources.sources.knative.dev test-ping-source-binary Kn kn source ping delete test-ping-source kn source ping delete test-ping-source-binary Similarly, you can delete the Service instance via: Kubectl kubectl delete service.serving.knative.dev event-display Kn kn service delete event-display","title":"Ping Source"},{"location":"eventing/samples/ping-source/#pingsource-example","text":"This example shows how to configure PingSource as an event source targeting a Knative Service.","title":"PingSource example"},{"location":"eventing/samples/ping-source/#before-you-begin","text":"Set up Knative Serving . Set up Knative Eventing .","title":"Before you begin"},{"location":"eventing/samples/ping-source/#create-a-knative-service","text":"To verify that PingSource is working, create a simple Knative Service that dumps incoming messages to its log. Kubectl Use following command to create the service from STDIN: cat <<EOF | kubectl create -f - apiVersion: serving.knative.dev/v1 kind: Service metadata: name: event-display spec: template: spec: containers: - image: gcr.io/knative-releases/knative.dev/eventing-contrib/cmd/event_display EOF Kn Use following command to create the service using the kn cli: kn service create event-display --image gcr.io/knative-releases/knative.dev/eventing-contrib/cmd/event_display","title":"Create a Knative Service"},{"location":"eventing/samples/ping-source/#create-a-pingsource","text":"For each set of ping events that you want to request, create an Event Source in the same namespace as the destination. Kubectl Use following command to create the event source from STDIN: cat <<EOF | kubectl create -f - apiVersion: sources.knative.dev/v1beta2 kind: PingSource metadata: name: test-ping-source spec: schedule: \"*/2 * * * *\" contentType: \"application/json\" data: '{\"message\": \"Hello world!\"}' sink: ref: apiVersion: serving.knative.dev/v1 kind: Service name: event-display EOF Kn Use following command to create the event source from the ping-source.yaml file: kn source ping create test-ping-source \\ --schedule \"*/2 * * * *\" --data \\ '{ \"message\": \"Hello world!\" }' \\ --sink ksvc:event-display","title":"Create a PingSource"},{"location":"eventing/samples/ping-source/#optional-create-a-pingsource-with-binary-data","text":"Sometimes you may want to send binary data, which cannot be directly serialized in yaml, to downstream. This can be achieved by using dataBase64 as the payload. As the name suggests, dataBase64 should carry data that is base64 encoded. Please note that data and dataBase64 cannot co-exist. Use the following command to create the event source with binary data from STDIN: cat <<EOF | kubectl create -f - apiVersion: sources.knative.dev/v1beta2 kind: PingSource metadata: name: test-ping-source-binary spec: schedule: \"*/2 * * * *\" contentType: \"text/plain\" dataBase64: \"ZGF0YQ==\" sink: ref: apiVersion: serving.knative.dev/v1 kind: Service name: event-display EOF","title":"(Optional) Create a PingSource with binary data"},{"location":"eventing/samples/ping-source/#verify","text":"Verify that the message was sent to the Knative eventing system by looking at message dumper logs. Kubectl Use following command to view the logs of the event-display service: kubectl logs -l serving.knative.dev/service = event-display -c user-container --since = 10m Kail You can also use kail instead of kubectl logs to tail the logs of the subscriber. kail -l serving.knative.dev/service = event-display -c user-container --since = 10m You should see log lines showing the request headers and body from the source: \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1 .0 type: dev.knative.sources.ping source: /apis/v1/namespaces/default/pingsources/test-ping-source id: d8e761eb-30c7-49a3-a421-cd5895239f2d time: 2019 -12-04T14:24:00.000702251Z datacontenttype: application/json Data, { \"message\" : \"Hello world!\" } If you created a PingSource with binary data, you should also see the following: \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1 .0 type: dev.knative.sources.ping source: /apis/v1/namespaces/default/pingsources/test-ping-source-binary id: a195be33-ff65-49af-9045-0e0711d05e94 time: 2020 -11-17T19:48:00.48334181Z datacontenttype: text/plain Data, ZGF0YQ ==","title":"Verify"},{"location":"eventing/samples/ping-source/#cleanup","text":"You can delete the PingSource instance by entering the following command: Kubectl kubectl delete pingsources.sources.knative.dev test-ping-source kubectl delete pingsources.sources.knative.dev test-ping-source-binary Kn kn source ping delete test-ping-source kn source ping delete test-ping-source-binary Similarly, you can delete the Service instance via: Kubectl kubectl delete service.serving.knative.dev event-display Kn kn service delete event-display","title":"Cleanup"},{"location":"eventing/samples/sequence/_index/","text":"","title":"Sequence Example"},{"location":"eventing/samples/sequence/sequence-reply-to-event-display/","text":"We are going to create the following logical configuration. We create a PingSource, feeding events to a Sequence , then taking the output of that Sequence and displaying the resulting output. The functions used in these examples live in https://github.com/knative/eventing-contrib/blob/main/cmd/appender/main.go . Prerequisites \u00b6 For this example, we'll assume you have set up an InMemoryChannel as well as Knative Serving (for our functions). The examples use default namespace, again, if you want to deploy to another Namespace, you will need to modify the examples to reflect this. If you want to use different type of Channel , you will have to modify the Sequence.Spec.ChannelTemplate to create the appropriate Channel resources. Setup \u00b6 Create the Knative Services \u00b6 Change default below to create the steps in the Namespace where you want resources created. apiVersion : serving.knative.dev/v1 kind : Service metadata : name : first spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing-contrib/cmd/appender env : - name : MESSAGE value : \" - Handled by 0\" --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : second spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing-contrib/cmd/appender env : - name : MESSAGE value : \" - Handled by 1\" --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : third spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing-contrib/cmd/appender env : - name : MESSAGE value : \" - Handled by 2\" --- kubectl -n default create -f ./steps.yaml Create the Sequence \u00b6 The sequence.yaml file contains the specifications for creating the Sequence. If you are using a different type of Channel, you need to change the spec.channelTemplate to point to your desired Channel. apiVersion : flows.knative.dev/v1 kind : Sequence metadata : name : sequence spec : channelTemplate : apiVersion : messaging.knative.dev/v1 kind : InMemoryChannel steps : - ref : apiVersion : serving.knative.dev/v1 kind : Service name : first - ref : apiVersion : serving.knative.dev/v1 kind : Service name : second - ref : apiVersion : serving.knative.dev/v1 kind : Service name : third reply : ref : kind : Service apiVersion : serving.knative.dev/v1 name : event-display Change default below to create the Sequence in the Namespace where you want the resources to be created. kubectl -n default create -f ./sequence.yaml Create the Service displaying the events created by Sequence \u00b6 apiVersion : serving.knative.dev/v1 kind : Service metadata : name : event-display spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing-contrib/cmd/event_display Change default below to create the Sequence in the Namespace where you want your resources to be created. kubectl -n default create -f ./event-display.yaml Create the PingSource targeting the Sequence \u00b6 This will create a PingSource which will send a CloudEvent with {\"message\": \"Hello world!\"} as the data payload every 2 minutes. apiVersion : sources.knative.dev/v1beta2 kind : PingSource metadata : name : ping-source spec : schedule : \"*/2 * * * *\" contentType : \"application/json\" data : '{\"message\": \"Hello world!\"}' sink : ref : apiVersion : flows.knative.dev/v1 kind : Sequence name : sequence kubectl -n default create -f ./ping-source.yaml Inspecting the results \u00b6 You can now see the final output by inspecting the logs of the event-display pods. kubectl -n default get pods Wait a bit and then look at the logs for the event-display pod: kubectl -n default logs -l serving.knative.dev/service = event-display -c user-container --tail = -1 \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1 .0 type: samples.http.mode3 source: /apis/v1/namespaces/default/pingsources/ping-source id: e8fa7906-ab62-4e61-9c13-a9406e2130a9 time: 2020 -03-02T20:52:00.0004957Z datacontenttype: application/json Extensions, knativehistory: sequence-kn-sequence-0-kn-channel.default.svc.cluster.local ; sequence-kn-sequence-1-kn-channel.default.svc.cluster.local ; sequence-kn-sequence-2-kn-channel.default.svc.cluster.local traceparent: 00 -6e2947379387f35ddc933b9190af16ad-de3db0bc4e442394-00 Data, { \"id\" : 0 , \"message\" : \"Hello world! - Handled by 0 - Handled by 1 - Handled by 2\" } And you can see that the initial PingSource message (\"Hello World!\") has been appended to it by each of the steps in the Sequence.","title":"Displaying Sequence Output"},{"location":"eventing/samples/sequence/sequence-reply-to-event-display/#prerequisites","text":"For this example, we'll assume you have set up an InMemoryChannel as well as Knative Serving (for our functions). The examples use default namespace, again, if you want to deploy to another Namespace, you will need to modify the examples to reflect this. If you want to use different type of Channel , you will have to modify the Sequence.Spec.ChannelTemplate to create the appropriate Channel resources.","title":"Prerequisites"},{"location":"eventing/samples/sequence/sequence-reply-to-event-display/#setup","text":"","title":"Setup"},{"location":"eventing/samples/sequence/sequence-reply-to-event-display/#create-the-knative-services","text":"Change default below to create the steps in the Namespace where you want resources created. apiVersion : serving.knative.dev/v1 kind : Service metadata : name : first spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing-contrib/cmd/appender env : - name : MESSAGE value : \" - Handled by 0\" --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : second spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing-contrib/cmd/appender env : - name : MESSAGE value : \" - Handled by 1\" --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : third spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing-contrib/cmd/appender env : - name : MESSAGE value : \" - Handled by 2\" --- kubectl -n default create -f ./steps.yaml","title":"Create the Knative Services"},{"location":"eventing/samples/sequence/sequence-reply-to-event-display/#create-the-sequence","text":"The sequence.yaml file contains the specifications for creating the Sequence. If you are using a different type of Channel, you need to change the spec.channelTemplate to point to your desired Channel. apiVersion : flows.knative.dev/v1 kind : Sequence metadata : name : sequence spec : channelTemplate : apiVersion : messaging.knative.dev/v1 kind : InMemoryChannel steps : - ref : apiVersion : serving.knative.dev/v1 kind : Service name : first - ref : apiVersion : serving.knative.dev/v1 kind : Service name : second - ref : apiVersion : serving.knative.dev/v1 kind : Service name : third reply : ref : kind : Service apiVersion : serving.knative.dev/v1 name : event-display Change default below to create the Sequence in the Namespace where you want the resources to be created. kubectl -n default create -f ./sequence.yaml","title":"Create the Sequence"},{"location":"eventing/samples/sequence/sequence-reply-to-event-display/#create-the-service-displaying-the-events-created-by-sequence","text":"apiVersion : serving.knative.dev/v1 kind : Service metadata : name : event-display spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing-contrib/cmd/event_display Change default below to create the Sequence in the Namespace where you want your resources to be created. kubectl -n default create -f ./event-display.yaml","title":"Create the Service displaying the events created by Sequence"},{"location":"eventing/samples/sequence/sequence-reply-to-event-display/#create-the-pingsource-targeting-the-sequence","text":"This will create a PingSource which will send a CloudEvent with {\"message\": \"Hello world!\"} as the data payload every 2 minutes. apiVersion : sources.knative.dev/v1beta2 kind : PingSource metadata : name : ping-source spec : schedule : \"*/2 * * * *\" contentType : \"application/json\" data : '{\"message\": \"Hello world!\"}' sink : ref : apiVersion : flows.knative.dev/v1 kind : Sequence name : sequence kubectl -n default create -f ./ping-source.yaml","title":"Create the PingSource targeting the Sequence"},{"location":"eventing/samples/sequence/sequence-reply-to-event-display/#inspecting-the-results","text":"You can now see the final output by inspecting the logs of the event-display pods. kubectl -n default get pods Wait a bit and then look at the logs for the event-display pod: kubectl -n default logs -l serving.knative.dev/service = event-display -c user-container --tail = -1 \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1 .0 type: samples.http.mode3 source: /apis/v1/namespaces/default/pingsources/ping-source id: e8fa7906-ab62-4e61-9c13-a9406e2130a9 time: 2020 -03-02T20:52:00.0004957Z datacontenttype: application/json Extensions, knativehistory: sequence-kn-sequence-0-kn-channel.default.svc.cluster.local ; sequence-kn-sequence-1-kn-channel.default.svc.cluster.local ; sequence-kn-sequence-2-kn-channel.default.svc.cluster.local traceparent: 00 -6e2947379387f35ddc933b9190af16ad-de3db0bc4e442394-00 Data, { \"id\" : 0 , \"message\" : \"Hello world! - Handled by 0 - Handled by 1 - Handled by 2\" } And you can see that the initial PingSource message (\"Hello World!\") has been appended to it by each of the steps in the Sequence.","title":"Inspecting the results"},{"location":"eventing/samples/sequence/sequence-reply-to-sequence/","text":"We are going to create the following logical configuration. We create a PingSource, feeding events to a Sequence , then taking the output of that Sequence and sending it to a second Sequence and finally displaying the resulting output. The functions used in these examples live in https://github.com/knative/eventing-contrib/blob/main/cmd/appender/main.go . Prerequisites \u00b6 For this example, we'll assume you have set up an InMemoryChannel as well as Knative Serving (for our functions). The examples use default namespace, again, if you want to deploy to another namespace, you will need to modify the examples to reflect this. If you want to use different type of Channel , you will have to modify the Sequence.Spec.ChannelTemplate to create the appropriate Channel resources. Setup \u00b6 Create the Knative Services \u00b6 Change default below to create the steps in the Namespace where you want resources created. apiVersion : serving.knative.dev/v1 kind : Service metadata : name : first spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing-contrib/cmd/appender env : - name : MESSAGE value : \" - Handled by 0\" --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : second spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing-contrib/cmd/appender env : - name : MESSAGE value : \" - Handled by 1\" --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : third spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing-contrib/cmd/appender env : - name : MESSAGE value : \" - Handled by 2\" --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : fourth spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing-contrib/cmd/appender env : - name : MESSAGE value : \" - Handled by 3\" --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : fifth spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing-contrib/cmd/appender env : - name : MESSAGE value : \" - Handled by 4\" --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : sixth spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing-contrib/cmd/appender env : - name : MESSAGE value : \" - Handled by 5\" --- kubectl -n default create -f ./steps.yaml Create the first Sequence \u00b6 The sequence1.yaml file contains the specifications for creating the Sequence. If you are using a different type of Channel, you need to change the spec.channelTemplate to point to your desired Channel. apiVersion : flows.knative.dev/v1 kind : Sequence metadata : name : first-sequence spec : channelTemplate : apiVersion : messaging.knative.dev/v1 kind : InMemoryChannel steps : - ref : apiVersion : serving.knative.dev/v1 kind : Service name : first - ref : apiVersion : serving.knative.dev/v1 kind : Service name : second - ref : apiVersion : serving.knative.dev/v1 kind : Service name : third reply : ref : kind : Sequence apiVersion : flows.knative.dev/v1 name : second-sequence Change default below to create the Sequence in the Namespace where you want your resources created. kubectl -n default create -f ./sequence1.yaml Create the second Sequence \u00b6 The sequence2.yaml file contains the specifications for creating the Sequence. If you are using a different type of Channel, you need to change the spec.channelTemplate to point to your desired Channel. apiVersion : flows.knative.dev/v1 kind : Sequence metadata : name : second-sequence spec : channelTemplate : apiVersion : messaging.knative.dev/v1 kind : InMemoryChannel steps : - ref : apiVersion : serving.knative.dev/v1 kind : Service name : fourth - ref : apiVersion : serving.knative.dev/v1 kind : Service name : fifth - ref : apiVersion : serving.knative.dev/v1 kind : Service name : sixth reply : ref : kind : Service apiVersion : serving.knative.dev/v1 name : event-display kubectl -n default create -f ./sequence2.yaml Create the Service displaying the events created by Sequence \u00b6 apiVersion : serving.knative.dev/v1 kind : Service metadata : name : event-display spec : template : spec : containerers : - image : gcr.io/knative-releases/knative.dev/eventing-contrib/cmd/event_display Change default below to create the Sequence in the Namespace where you want your resources created. kubectl -n default create -f ./event-display.yaml Create the PingSource targeting the first Sequence \u00b6 This will create a PingSource which will send a CloudEvent with {\"message\": \"Hello world!\"} as the data payload every 2 minutes. apiVersion : sources.knative.dev/v1beta2 kind : PingSource metadata : name : ping-source spec : schedule : \"*/2 * * * *\" contentType : \"application/json\" data : '{\"message\": \"Hello world!\"}' sink : ref : apiVersion : flows.knative.dev/v1 kind : Sequence name : first-sequence kubectl -n default create -f ./ping-source.yaml Inspecting the results \u00b6 You can now see the final output by inspecting the logs of the event-display pods. kubectl -n default get pods Then look at the logs for the event-display pod: kubectl -n default logs -l serving.knative.dev/service = event-display -c user-container --tail = -1 \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1 .0 type: dev.knative.sources.ping source: /apis/v1/namespaces/default/pingsources/ping-source id: 29d531df-78d8-4d11-9ffd-ba24045241a9 time: 2020 -03-02T21:18:00.0011708Z datacontenttype: application/json Extensions, knativehistory: first-sequence-kn-sequence-0-kn-channel.default.svc.cluster.local ; first-sequence-kn-sequence-1-kn-channel.default.svc.cluster.local ; first-sequence-kn-sequence-2-kn-channel.default.svc.cluster.local ; second-sequence-kn-sequence-0-kn-channel.default.svc.cluster.local ; second-sequence-kn-sequence-1-kn-channel.default.svc.cluster.local ; second-sequence-kn-sequence-2-kn-channel.default.svc.cluster.local traceparent: 00 -e5abc9de525a89ead80560b8f328de5c-fc12b64a6296f541-00 Data, { \"id\" : 0 , \"message\" : \"Hello world! - Handled by 0 - Handled by 1 - Handled by 2 - Handled by 3 - Handled by 4 - Handled by 5\" } And you can see that the initial PingSource message (\"Hello World!\") has been appended to it by each of the steps in the Sequence.","title":"Using Sequences in Series"},{"location":"eventing/samples/sequence/sequence-reply-to-sequence/#prerequisites","text":"For this example, we'll assume you have set up an InMemoryChannel as well as Knative Serving (for our functions). The examples use default namespace, again, if you want to deploy to another namespace, you will need to modify the examples to reflect this. If you want to use different type of Channel , you will have to modify the Sequence.Spec.ChannelTemplate to create the appropriate Channel resources.","title":"Prerequisites"},{"location":"eventing/samples/sequence/sequence-reply-to-sequence/#setup","text":"","title":"Setup"},{"location":"eventing/samples/sequence/sequence-reply-to-sequence/#create-the-knative-services","text":"Change default below to create the steps in the Namespace where you want resources created. apiVersion : serving.knative.dev/v1 kind : Service metadata : name : first spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing-contrib/cmd/appender env : - name : MESSAGE value : \" - Handled by 0\" --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : second spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing-contrib/cmd/appender env : - name : MESSAGE value : \" - Handled by 1\" --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : third spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing-contrib/cmd/appender env : - name : MESSAGE value : \" - Handled by 2\" --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : fourth spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing-contrib/cmd/appender env : - name : MESSAGE value : \" - Handled by 3\" --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : fifth spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing-contrib/cmd/appender env : - name : MESSAGE value : \" - Handled by 4\" --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : sixth spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing-contrib/cmd/appender env : - name : MESSAGE value : \" - Handled by 5\" --- kubectl -n default create -f ./steps.yaml","title":"Create the Knative Services"},{"location":"eventing/samples/sequence/sequence-reply-to-sequence/#create-the-first-sequence","text":"The sequence1.yaml file contains the specifications for creating the Sequence. If you are using a different type of Channel, you need to change the spec.channelTemplate to point to your desired Channel. apiVersion : flows.knative.dev/v1 kind : Sequence metadata : name : first-sequence spec : channelTemplate : apiVersion : messaging.knative.dev/v1 kind : InMemoryChannel steps : - ref : apiVersion : serving.knative.dev/v1 kind : Service name : first - ref : apiVersion : serving.knative.dev/v1 kind : Service name : second - ref : apiVersion : serving.knative.dev/v1 kind : Service name : third reply : ref : kind : Sequence apiVersion : flows.knative.dev/v1 name : second-sequence Change default below to create the Sequence in the Namespace where you want your resources created. kubectl -n default create -f ./sequence1.yaml","title":"Create the first Sequence"},{"location":"eventing/samples/sequence/sequence-reply-to-sequence/#create-the-second-sequence","text":"The sequence2.yaml file contains the specifications for creating the Sequence. If you are using a different type of Channel, you need to change the spec.channelTemplate to point to your desired Channel. apiVersion : flows.knative.dev/v1 kind : Sequence metadata : name : second-sequence spec : channelTemplate : apiVersion : messaging.knative.dev/v1 kind : InMemoryChannel steps : - ref : apiVersion : serving.knative.dev/v1 kind : Service name : fourth - ref : apiVersion : serving.knative.dev/v1 kind : Service name : fifth - ref : apiVersion : serving.knative.dev/v1 kind : Service name : sixth reply : ref : kind : Service apiVersion : serving.knative.dev/v1 name : event-display kubectl -n default create -f ./sequence2.yaml","title":"Create the second Sequence"},{"location":"eventing/samples/sequence/sequence-reply-to-sequence/#create-the-service-displaying-the-events-created-by-sequence","text":"apiVersion : serving.knative.dev/v1 kind : Service metadata : name : event-display spec : template : spec : containerers : - image : gcr.io/knative-releases/knative.dev/eventing-contrib/cmd/event_display Change default below to create the Sequence in the Namespace where you want your resources created. kubectl -n default create -f ./event-display.yaml","title":"Create the Service displaying the events created by Sequence"},{"location":"eventing/samples/sequence/sequence-reply-to-sequence/#create-the-pingsource-targeting-the-first-sequence","text":"This will create a PingSource which will send a CloudEvent with {\"message\": \"Hello world!\"} as the data payload every 2 minutes. apiVersion : sources.knative.dev/v1beta2 kind : PingSource metadata : name : ping-source spec : schedule : \"*/2 * * * *\" contentType : \"application/json\" data : '{\"message\": \"Hello world!\"}' sink : ref : apiVersion : flows.knative.dev/v1 kind : Sequence name : first-sequence kubectl -n default create -f ./ping-source.yaml","title":"Create the PingSource targeting the first Sequence"},{"location":"eventing/samples/sequence/sequence-reply-to-sequence/#inspecting-the-results","text":"You can now see the final output by inspecting the logs of the event-display pods. kubectl -n default get pods Then look at the logs for the event-display pod: kubectl -n default logs -l serving.knative.dev/service = event-display -c user-container --tail = -1 \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1 .0 type: dev.knative.sources.ping source: /apis/v1/namespaces/default/pingsources/ping-source id: 29d531df-78d8-4d11-9ffd-ba24045241a9 time: 2020 -03-02T21:18:00.0011708Z datacontenttype: application/json Extensions, knativehistory: first-sequence-kn-sequence-0-kn-channel.default.svc.cluster.local ; first-sequence-kn-sequence-1-kn-channel.default.svc.cluster.local ; first-sequence-kn-sequence-2-kn-channel.default.svc.cluster.local ; second-sequence-kn-sequence-0-kn-channel.default.svc.cluster.local ; second-sequence-kn-sequence-1-kn-channel.default.svc.cluster.local ; second-sequence-kn-sequence-2-kn-channel.default.svc.cluster.local traceparent: 00 -e5abc9de525a89ead80560b8f328de5c-fc12b64a6296f541-00 Data, { \"id\" : 0 , \"message\" : \"Hello world! - Handled by 0 - Handled by 1 - Handled by 2 - Handled by 3 - Handled by 4 - Handled by 5\" } And you can see that the initial PingSource message (\"Hello World!\") has been appended to it by each of the steps in the Sequence.","title":"Inspecting the results"},{"location":"eventing/samples/sequence/sequence-terminal/","text":"We are going to create the following logical configuration. We create a PingSource, feeding events to a Sequence . Sequence can then do either external work, or out of band create additional events. The functions used in these examples live in https://github.com/knative/eventing-contrib/blob/main/cmd/appender/main.go . Prerequisites \u00b6 For this example, we'll assume you have set up an InMemoryChannel as well as Knative Serving (for our functions). The examples use default namespace, again, if you want to deploy to another Namespace, you will need to modify the examples to reflect this. If you want to use different type of Channel , you will have to modify the Sequence.Spec.ChannelTemplate to create the appropriate Channel resources. Setup \u00b6 Create the Knative Services \u00b6 First create the 3 steps that will be referenced in the Steps. apiVersion : serving.knative.dev/v1 kind : Service metadata : name : first spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing-contrib/cmd/appender env : - name : MESSAGE value : \" - Handled by 0\" --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : second spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing-contrib/cmd/appender env : - name : MESSAGE value : \" - Handled by 1\" --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : third spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing-contrib/cmd/appender env : - name : MESSAGE value : \" - Handled by 2\" --- kubectl -n default create -f ./steps.yaml Create the Sequence \u00b6 The sequence.yaml file contains the specifications for creating the Sequence. If you are using a different type of Channel, you need to change the spec.channelTemplate to point to your desired Channel. apiVersion : flows.knative.dev/v1 kind : Sequence metadata : name : sequence spec : channelTemplate : apiVersion : messaging.knative.dev/v1 kind : InMemoryChannel steps : - ref : apiVersion : serving.knative.dev/v1 kind : Service name : first - ref : apiVersion : serving.knative.dev/v1 kind : Service name : second - ref : apiVersion : serving.knative.dev/v1 kind : Service name : third Change default below to create the Sequence in the Namespace where you want the resources to be created. Here, if you are using different type of Channel, you need to change the spec.channelTemplate to point to your desired Channel. kubectl -n default create -f ./sequence.yaml Create the PingSource targeting the Sequence \u00b6 This will create a PingSource which will send a CloudEvent with {\"message\": \"Hello world!\"} as the data payload every 2 minutes. apiVersion : sources.knative.dev/v1beta2 kind : PingSource metadata : name : ping-source spec : schedule : \"*/2 * * * *\" contentType : \"application/json\" data : '{\"message\": \"Hello world!\"}' sink : ref : apiVersion : flows.knative.dev/v1 kind : Sequence name : sequence kubectl -n default create -f ./ping-source.yaml Inspecting the results \u00b6 You can now see the final output by inspecting the logs of the event-display pods. Note that since we set the PingSource to emit every 2 minutes, it might take some time for the events to show up in the logs. kubectl -n default get pods Let's look at the logs for the first Step in the Sequence : kubectl -n default logs -l serving.knative.dev/service = first -c user-container --tail = -1 2020 /03/02 21 :28:00 listening on 8080 , appending \" - Handled by 0\" to events 2020 /03/02 21 :28:01 Received a new event: 2020 /03/02 21 :28:01 [ 2020 -03-02T21:28:00.0010247Z ] /apis/v1/namespaces/default/pingsources/ping-source dev.knative.sources.ping: & { Sequence:0 Message:Hello world! } 2020 /03/02 21 :28:01 Transform the event to: 2020 /03/02 21 :28:01 [ 2020 -03-02T21:28:00.0010247Z ] /apis/v1/namespaces/default/pingsources/ping-source dev.knative.sources.ping: & { Sequence:0 Message:Hello world! - Handled by 0 } And you can see that the initial PingSource message (\"Hello World!\") has now been modified by the first step in the Sequence to include \" - Handled by 0\". Exciting :) Then we can look at the output of the second Step in the Sequence : kubectl -n default logs -l serving.knative.dev/service = second -c user-container --tail = -1 2020 /03/02 21 :28:02 listening on 8080 , appending \" - Handled by 1\" to events 2020 /03/02 21 :28:02 Received a new event: 2020 /03/02 21 :28:02 [ 2020 -03-02T21:28:00.0010247Z ] /apis/v1/namespaces/default/pingsources/ping-source dev.knative.sources.ping: & { Sequence:0 Message:Hello world! - Handled by 0 } 2020 /03/02 21 :28:02 Transform the event to: 2020 /03/02 21 :28:02 [ 2020 -03-02T21:28:00.0010247Z ] /apis/v1/namespaces/default/pingsources/ping-source dev.knative.sources.ping: & { Sequence:0 Message:Hello world! - Handled by 0 - Handled by 1 } And as expected it's now been handled by both the first and second Step as reflected by the Message being now: \"Hello world! - Handled by 0 - Handled by 1\" Then we can look at the output of the last Step in the Sequence : kubectl -n default logs -l serving.knative.dev/service = third -c user-container --tail = -1 2020 /03/02 21 :28:03 listening on 8080 , appending \" - Handled by 2\" to events 2020 /03/02 21 :28:03 Received a new event: 2020 /03/02 21 :28:03 [ 2020 -03-02T21:28:00.0010247Z ] /apis/v1/namespaces/default/pingsources/ping-source dev.knative.sources.ping: & { Sequence:0 Message:Hello world! - Handled by 0 - Handled by 1 } 2020 /03/02 21 :28:03 Transform the event to: 2020 /03/02 21 :28:03 [ 2020 -03-02T21:28:00.0010247Z ] /apis/v1/namespaces/default/pingsources/ping-source dev.knative.sources.ping: & { Sequence:0 Message:Hello world! - Handled by 0 - Handled by 1 - Handled by 2 }","title":"Create Additional Events"},{"location":"eventing/samples/sequence/sequence-terminal/#prerequisites","text":"For this example, we'll assume you have set up an InMemoryChannel as well as Knative Serving (for our functions). The examples use default namespace, again, if you want to deploy to another Namespace, you will need to modify the examples to reflect this. If you want to use different type of Channel , you will have to modify the Sequence.Spec.ChannelTemplate to create the appropriate Channel resources.","title":"Prerequisites"},{"location":"eventing/samples/sequence/sequence-terminal/#setup","text":"","title":"Setup"},{"location":"eventing/samples/sequence/sequence-terminal/#create-the-knative-services","text":"First create the 3 steps that will be referenced in the Steps. apiVersion : serving.knative.dev/v1 kind : Service metadata : name : first spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing-contrib/cmd/appender env : - name : MESSAGE value : \" - Handled by 0\" --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : second spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing-contrib/cmd/appender env : - name : MESSAGE value : \" - Handled by 1\" --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : third spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing-contrib/cmd/appender env : - name : MESSAGE value : \" - Handled by 2\" --- kubectl -n default create -f ./steps.yaml","title":"Create the Knative Services"},{"location":"eventing/samples/sequence/sequence-terminal/#create-the-sequence","text":"The sequence.yaml file contains the specifications for creating the Sequence. If you are using a different type of Channel, you need to change the spec.channelTemplate to point to your desired Channel. apiVersion : flows.knative.dev/v1 kind : Sequence metadata : name : sequence spec : channelTemplate : apiVersion : messaging.knative.dev/v1 kind : InMemoryChannel steps : - ref : apiVersion : serving.knative.dev/v1 kind : Service name : first - ref : apiVersion : serving.knative.dev/v1 kind : Service name : second - ref : apiVersion : serving.knative.dev/v1 kind : Service name : third Change default below to create the Sequence in the Namespace where you want the resources to be created. Here, if you are using different type of Channel, you need to change the spec.channelTemplate to point to your desired Channel. kubectl -n default create -f ./sequence.yaml","title":"Create the Sequence"},{"location":"eventing/samples/sequence/sequence-terminal/#create-the-pingsource-targeting-the-sequence","text":"This will create a PingSource which will send a CloudEvent with {\"message\": \"Hello world!\"} as the data payload every 2 minutes. apiVersion : sources.knative.dev/v1beta2 kind : PingSource metadata : name : ping-source spec : schedule : \"*/2 * * * *\" contentType : \"application/json\" data : '{\"message\": \"Hello world!\"}' sink : ref : apiVersion : flows.knative.dev/v1 kind : Sequence name : sequence kubectl -n default create -f ./ping-source.yaml","title":"Create the PingSource targeting the Sequence"},{"location":"eventing/samples/sequence/sequence-terminal/#inspecting-the-results","text":"You can now see the final output by inspecting the logs of the event-display pods. Note that since we set the PingSource to emit every 2 minutes, it might take some time for the events to show up in the logs. kubectl -n default get pods Let's look at the logs for the first Step in the Sequence : kubectl -n default logs -l serving.knative.dev/service = first -c user-container --tail = -1 2020 /03/02 21 :28:00 listening on 8080 , appending \" - Handled by 0\" to events 2020 /03/02 21 :28:01 Received a new event: 2020 /03/02 21 :28:01 [ 2020 -03-02T21:28:00.0010247Z ] /apis/v1/namespaces/default/pingsources/ping-source dev.knative.sources.ping: & { Sequence:0 Message:Hello world! } 2020 /03/02 21 :28:01 Transform the event to: 2020 /03/02 21 :28:01 [ 2020 -03-02T21:28:00.0010247Z ] /apis/v1/namespaces/default/pingsources/ping-source dev.knative.sources.ping: & { Sequence:0 Message:Hello world! - Handled by 0 } And you can see that the initial PingSource message (\"Hello World!\") has now been modified by the first step in the Sequence to include \" - Handled by 0\". Exciting :) Then we can look at the output of the second Step in the Sequence : kubectl -n default logs -l serving.knative.dev/service = second -c user-container --tail = -1 2020 /03/02 21 :28:02 listening on 8080 , appending \" - Handled by 1\" to events 2020 /03/02 21 :28:02 Received a new event: 2020 /03/02 21 :28:02 [ 2020 -03-02T21:28:00.0010247Z ] /apis/v1/namespaces/default/pingsources/ping-source dev.knative.sources.ping: & { Sequence:0 Message:Hello world! - Handled by 0 } 2020 /03/02 21 :28:02 Transform the event to: 2020 /03/02 21 :28:02 [ 2020 -03-02T21:28:00.0010247Z ] /apis/v1/namespaces/default/pingsources/ping-source dev.knative.sources.ping: & { Sequence:0 Message:Hello world! - Handled by 0 - Handled by 1 } And as expected it's now been handled by both the first and second Step as reflected by the Message being now: \"Hello world! - Handled by 0 - Handled by 1\" Then we can look at the output of the last Step in the Sequence : kubectl -n default logs -l serving.knative.dev/service = third -c user-container --tail = -1 2020 /03/02 21 :28:03 listening on 8080 , appending \" - Handled by 2\" to events 2020 /03/02 21 :28:03 Received a new event: 2020 /03/02 21 :28:03 [ 2020 -03-02T21:28:00.0010247Z ] /apis/v1/namespaces/default/pingsources/ping-source dev.knative.sources.ping: & { Sequence:0 Message:Hello world! - Handled by 0 - Handled by 1 } 2020 /03/02 21 :28:03 Transform the event to: 2020 /03/02 21 :28:03 [ 2020 -03-02T21:28:00.0010247Z ] /apis/v1/namespaces/default/pingsources/ping-source dev.knative.sources.ping: & { Sequence:0 Message:Hello world! - Handled by 0 - Handled by 1 - Handled by 2 }","title":"Inspecting the results"},{"location":"eventing/samples/sequence/sequence-with-broker-trigger/","text":"We are going to create the following logical configuration. We create a PingSource, feeding events into the Broker, then we create a Filter that wires those events into a Sequence consisting of 3 steps. Then we take the end of the Sequence and feed newly minted events back into the Broker and create another Trigger which will then display those events. Prerequisites \u00b6 Knative Serving InMemoryChannel NOTE: The examples use the default namespace. If you want to use different type of Channel , you will have to modify the Sequence.Spec.ChannelTemplate to create the appropriate Channel resources. The functions used in these examples live in https://github.com/knative/eventing-contrib/blob/main/cmd/appender/main.go . Setup \u00b6 Creating the Broker \u00b6 To create the cluster default Broker type: kubectl create -f - <<EOF apiVersion: eventing.knative.dev/v1 kind: Broker metadata: name: default EOF Create the Knative Services \u00b6 apiVersion : serving.knative.dev/v1 kind : Service metadata : name : first spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing-contrib/cmd/appender env : - name : MESSAGE value : \" - Handled by 0\" --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : second spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing-contrib/cmd/appender env : - name : MESSAGE value : \" - Handled by 1\" --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : third spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing-contrib/cmd/appender env : - name : MESSAGE value : \" - Handled by 2\" - name : TYPE value : \"samples.http.mod3\" --- Change the default namespace below to create the services in the namespace where you have configured your broker. kubectl -n default create -f ./steps.yaml Create the Sequence \u00b6 The sequence.yaml file contains the specifications for creating the Sequence. If you are using a different type of Channel, you need to change the spec.channelTemplate to point to your desired Channel. Also, change the spec.reply.name to point to your Broker apiVersion : flows.knative.dev/v1 kind : Sequence metadata : name : sequence spec : channelTemplate : apiVersion : messaging.knative.dev/v1 kind : InMemoryChannel steps : - ref : apiVersion : serving.knative.dev/v1 kind : Service name : first - ref : apiVersion : serving.knative.dev/v1 kind : Service name : second - ref : apiVersion : serving.knative.dev/v1 kind : Service name : third reply : ref : kind : Broker apiVersion : eventing.knative.dev/v1 name : default Change the default namespace below to create the sequence in the namespace where you have configured your broker. kubectl -n default create -f ./sequence.yaml Create the PingSource targeting the Broker \u00b6 This will create a PingSource which will send a CloudEvent with {\"message\": \"Hello world!\"} as the data payload every 2 minutes. apiVersion : sources.knative.dev/v1beta2 kind : PingSource metadata : name : ping-source spec : schedule : \"*/2 * * * *\" contentType : \"application/json\" data : '{\"message\": \"Hello world!\"}' sink : ref : apiVersion : eventing.knative.dev/v1 kind : Broker name : default Change the default namespace below to create the PingSource in the namespace where you have configured your broker and sequence. kubectl -n default create -f ./ping-source.yaml Create the Trigger targeting the Sequence \u00b6 apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : sequence-trigger spec : broker : default filter : attributes : type : dev.knative.sources.ping subscriber : ref : apiVersion : flows.knative.dev/v1 kind : Sequence name : sequence Change the default namespace below to create the trigger in the namespace where you have configured your broker and sequence. kubectl -n default create -f ./trigger.yaml Create the Service and Trigger displaying the events created by Sequence \u00b6 apiVersion : serving.knative.dev/v1 kind : Service metadata : name : sequence-display spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing-contrib/cmd/appender --- apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : display-trigger spec : broker : default filter : attributes : type : samples.http.mod3 subscriber : ref : apiVersion : serving.knative.dev/v1 kind : Service name : sequence-display --- Change default namespace below to create the service and trigger in the namespace where you have configured your broker. kubectl -n default create -f ./display-trigger.yaml Inspecting the results \u00b6 You can now see the final output by inspecting the logs of the sequence-display pods. kubectl -n default get pods View the logs for the sequence-display pod: kubectl -n default logs -l serving.knative.dev/service = sequence-display -c user-container --tail = -1 \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1 .0 type: samples.http.mod3 source: /apis/v1/namespaces/default/pingsources/ping-source id: 159bba01-054a-4ae7-b7be-d4e7c5f773d2 time: 2020 -03-03T14:56:00.000652027Z datacontenttype: application/json Extensions, knativearrivaltime: 2020 -03-03T14:56:00.018390608Z knativehistory: default-kne-trigger-kn-channel.default.svc.cluster.local ; sequence-kn-sequence-0-kn-channel.default.svc.cluster.local ; sequence-kn-sequence-1-kn-channel.default.svc.cluster.local ; sequence-kn-sequence-2-kn-channel.default.svc.cluster.local ; default-kne-trigger-kn-channel.default.svc.cluster.local traceparent: 00 -e893412106ff417a90a5695e53ffd9cc-5829ae45a14ed462-00 Data, { \"id\" : 0 , \"message\" : \"Hello world! - Handled by 0 - Handled by 1 - Handled by 2\" } And you can see that the initial PingSource message {\"Hello World!\"} has been appended to it by each of the steps in the Sequence.","title":"Using Broker and Trigger"},{"location":"eventing/samples/sequence/sequence-with-broker-trigger/#prerequisites","text":"Knative Serving InMemoryChannel NOTE: The examples use the default namespace. If you want to use different type of Channel , you will have to modify the Sequence.Spec.ChannelTemplate to create the appropriate Channel resources. The functions used in these examples live in https://github.com/knative/eventing-contrib/blob/main/cmd/appender/main.go .","title":"Prerequisites"},{"location":"eventing/samples/sequence/sequence-with-broker-trigger/#setup","text":"","title":"Setup"},{"location":"eventing/samples/sequence/sequence-with-broker-trigger/#creating-the-broker","text":"To create the cluster default Broker type: kubectl create -f - <<EOF apiVersion: eventing.knative.dev/v1 kind: Broker metadata: name: default EOF","title":"Creating the Broker"},{"location":"eventing/samples/sequence/sequence-with-broker-trigger/#create-the-knative-services","text":"apiVersion : serving.knative.dev/v1 kind : Service metadata : name : first spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing-contrib/cmd/appender env : - name : MESSAGE value : \" - Handled by 0\" --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : second spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing-contrib/cmd/appender env : - name : MESSAGE value : \" - Handled by 1\" --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : third spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing-contrib/cmd/appender env : - name : MESSAGE value : \" - Handled by 2\" - name : TYPE value : \"samples.http.mod3\" --- Change the default namespace below to create the services in the namespace where you have configured your broker. kubectl -n default create -f ./steps.yaml","title":"Create the Knative Services"},{"location":"eventing/samples/sequence/sequence-with-broker-trigger/#create-the-sequence","text":"The sequence.yaml file contains the specifications for creating the Sequence. If you are using a different type of Channel, you need to change the spec.channelTemplate to point to your desired Channel. Also, change the spec.reply.name to point to your Broker apiVersion : flows.knative.dev/v1 kind : Sequence metadata : name : sequence spec : channelTemplate : apiVersion : messaging.knative.dev/v1 kind : InMemoryChannel steps : - ref : apiVersion : serving.knative.dev/v1 kind : Service name : first - ref : apiVersion : serving.knative.dev/v1 kind : Service name : second - ref : apiVersion : serving.knative.dev/v1 kind : Service name : third reply : ref : kind : Broker apiVersion : eventing.knative.dev/v1 name : default Change the default namespace below to create the sequence in the namespace where you have configured your broker. kubectl -n default create -f ./sequence.yaml","title":"Create the Sequence"},{"location":"eventing/samples/sequence/sequence-with-broker-trigger/#create-the-pingsource-targeting-the-broker","text":"This will create a PingSource which will send a CloudEvent with {\"message\": \"Hello world!\"} as the data payload every 2 minutes. apiVersion : sources.knative.dev/v1beta2 kind : PingSource metadata : name : ping-source spec : schedule : \"*/2 * * * *\" contentType : \"application/json\" data : '{\"message\": \"Hello world!\"}' sink : ref : apiVersion : eventing.knative.dev/v1 kind : Broker name : default Change the default namespace below to create the PingSource in the namespace where you have configured your broker and sequence. kubectl -n default create -f ./ping-source.yaml","title":"Create the PingSource targeting the Broker"},{"location":"eventing/samples/sequence/sequence-with-broker-trigger/#create-the-trigger-targeting-the-sequence","text":"apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : sequence-trigger spec : broker : default filter : attributes : type : dev.knative.sources.ping subscriber : ref : apiVersion : flows.knative.dev/v1 kind : Sequence name : sequence Change the default namespace below to create the trigger in the namespace where you have configured your broker and sequence. kubectl -n default create -f ./trigger.yaml","title":"Create the Trigger targeting the Sequence"},{"location":"eventing/samples/sequence/sequence-with-broker-trigger/#create-the-service-and-trigger-displaying-the-events-created-by-sequence","text":"apiVersion : serving.knative.dev/v1 kind : Service metadata : name : sequence-display spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing-contrib/cmd/appender --- apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : display-trigger spec : broker : default filter : attributes : type : samples.http.mod3 subscriber : ref : apiVersion : serving.knative.dev/v1 kind : Service name : sequence-display --- Change default namespace below to create the service and trigger in the namespace where you have configured your broker. kubectl -n default create -f ./display-trigger.yaml","title":"Create the Service and Trigger displaying the events created by Sequence"},{"location":"eventing/samples/sequence/sequence-with-broker-trigger/#inspecting-the-results","text":"You can now see the final output by inspecting the logs of the sequence-display pods. kubectl -n default get pods View the logs for the sequence-display pod: kubectl -n default logs -l serving.knative.dev/service = sequence-display -c user-container --tail = -1 \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1 .0 type: samples.http.mod3 source: /apis/v1/namespaces/default/pingsources/ping-source id: 159bba01-054a-4ae7-b7be-d4e7c5f773d2 time: 2020 -03-03T14:56:00.000652027Z datacontenttype: application/json Extensions, knativearrivaltime: 2020 -03-03T14:56:00.018390608Z knativehistory: default-kne-trigger-kn-channel.default.svc.cluster.local ; sequence-kn-sequence-0-kn-channel.default.svc.cluster.local ; sequence-kn-sequence-1-kn-channel.default.svc.cluster.local ; sequence-kn-sequence-2-kn-channel.default.svc.cluster.local ; default-kne-trigger-kn-channel.default.svc.cluster.local traceparent: 00 -e893412106ff417a90a5695e53ffd9cc-5829ae45a14ed462-00 Data, { \"id\" : 0 , \"message\" : \"Hello world! - Handled by 0 - Handled by 1 - Handled by 2\" } And you can see that the initial PingSource message {\"Hello World!\"} has been appended to it by each of the steps in the Sequence.","title":"Inspecting the results"},{"location":"eventing/sink/","text":"A sink is an Addressable resource that acts as a link between the Eventing mesh and an entity or system. We can connect any source to a sink, such as PingSource and KafkaSink objects: apiVersion : sources.knative.dev/v1beta1 kind : PingSource metadata : name : test-ping-source spec : schedule : \"*/1 * * * *\" jsonData : '{\"message\": \"Hello world!\"}' sink : ref : apiVersion : eventing.knative.dev/v1alpha1 kind : KafkaSink name : my-kafka-sink We can connect a Trigger object to a sink, so that we can filter events, before sending them to a sink: apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : my-service-trigger spec : broker : default filter : attributes : type : dev.knative.foo.bar myextension : my-extension-value subscriber : ref : apiVersion : eventing.knative.dev/v1alpha1 kind : KafkaSink name : my-kafka-sink Knative Sinks \u00b6 Name Maintainer Description KafkaSink Knative Send events to a Kafka topic RedisSink Knative Send events to a Redis Stream","title":"Overview"},{"location":"eventing/sink/#knative-sinks","text":"Name Maintainer Description KafkaSink Knative Send events to a Kafka topic RedisSink Knative Send events to a Redis Stream","title":"Knative Sinks"},{"location":"eventing/sink/_index/","text":"Macro Rendering Error \u00b6 TemplateSyntaxError : unexpected '%' Traceback (most recent call last): File \"/usr/local/lib/python3.9/site-packages/mkdocs_macros/plugin.py\", line 441, in render md_template = self.env.from_string(markdown) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 941, in from_string return cls.from_code(self, self.compile(source), globals, None) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 638, in compile self.handle_exception(source=source_hint) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 832, in handle_exception reraise(*rewrite_traceback_stack(source=source)) File \"/usr/local/lib/python3.9/site-packages/jinja2/_compat.py\", line 28, in reraise raise value.with_traceback(tb) File \"<unknown>\", line 3, in template jinja2.exceptions.TemplateSyntaxError: unexpected '%'","title":"Sink"},{"location":"eventing/sink/_index/#macro-rendering-error","text":"TemplateSyntaxError : unexpected '%' Traceback (most recent call last): File \"/usr/local/lib/python3.9/site-packages/mkdocs_macros/plugin.py\", line 441, in render md_template = self.env.from_string(markdown) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 941, in from_string return cls.from_code(self, self.compile(source), globals, None) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 638, in compile self.handle_exception(source=source_hint) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 832, in handle_exception reraise(*rewrite_traceback_stack(source=source)) File \"/usr/local/lib/python3.9/site-packages/jinja2/_compat.py\", line 28, in reraise raise value.with_traceback(tb) File \"<unknown>\", line 3, in template jinja2.exceptions.TemplateSyntaxError: unexpected '%'","title":"Macro Rendering Error"},{"location":"eventing/sink/kafka-sink/","text":"Apache Kafka Sink \u00b6 This page shows how to install and configure Apache Kafka Sink. Prerequisites \u00b6 Installing Eventing using YAML files . Installation \u00b6 Install the Kafka controller: kubectl apply --filename http://github.com/knative-sandbox/eventing-kafka-broker/releases/download/v0.21.0/eventing-kafka-controller.yaml Install the Kafka Sink data plane: kubectl apply --filename http://github.com/knative-sandbox/eventing-kafka-broker/releases/download/v0.21.0/eventing-kafka-sink.yaml Verify that kafka-controller and kafka-sink-receiver are running: kubectl get deployments.apps -n knative-eventing Example output: NAME READY UP-TO-DATE AVAILABLE AGE eventing-controller 1 /1 1 1 10s eventing-webhook 1 /1 1 1 9s kafka-controller 1 /1 1 1 3s kafka-sink-receiver 1 /1 1 1 5s Kafka Sink \u00b6 A KafkaSink object looks like this: apiVersion : eventing.knative.dev/v1alpha1 kind : KafkaSink metadata : name : my-kafka-sink namespace : default spec : topic : mytopic bootstrapServers : - my-cluster-kafka-bootstrap.kafka:9092 Security \u00b6 Apache Kafka supports different security features, Knative supports the followings: Authentication using SASL without encryption Authentication using SASL and encryption using SSL Authentication and encryption using SSL Encryption using SSL without client authentication To enable security features, in the KafkaSink spec, we can reference a Secret : apiVersion : eventing.knative.dev/v1alpha1 kind : KafkaSink metadata : name : my-kafka-sink namespace : default spec : topic : mytopic bootstrapServers : - my-cluster-kafka-bootstrap.kafka:9092 auth.secret.ref.name : my_secret The Secret my_secret must exist in the same namespace of the KafkaSink , in this case: default . Note: Certificates and keys must be in PEM format . Authentication using SASL \u00b6 Knative supports the following SASL mechanisms: PLAIN SCRAM-SHA-256 SCRAM-SHA-512 To use a specific SASL mechanism replace <sasl_mechanism> with the mechanism of your choice. Authentication using SASL without encryption \u00b6 kubectl create secret --namespace <namespace> generic <my_secret> \\ --from-literal = protocol = SASL_PLAINTEXT \\ --from-literal = sasl.mechanism = <sasl_mechanism> \\ --from-literal = user = <my_user> \\ --from-literal = password = <my_password> Authentication using SASL and encryption using SSL \u00b6 kubectl create secret --namespace <namespace> generic <my_secret> \\ --from-literal = protocol = SASL_SSL \\ --from-literal = sasl.mechanism = <sasl_mechanism> \\ --from-file = ca.crt = caroot.pem \\ --from-literal = user = <my_user> \\ --from-literal = password = <my_password> Encryption using SSL without client authentication \u00b6 kubectl create secret --namespace <namespace> generic <my_secret> \\ --from-literal = protocol = SSL \\ --from-file = ca.crt = <my_caroot.pem_file_path> \\ --from-literal = user.skip = true Authentication and encryption using SSL \u00b6 kubectl create secret --namespace <namespace> generic <my_secret> \\ --from-literal = protocol = SSL \\ --from-file = ca.crt = <my_caroot.pem_file_path> \\ --from-file = user.crt = <my_cert.pem_file_path> \\ --from-file = user.key = <my_key.pem_file_path> NOTE: ca.crt can be omitted to fallback to use system's root CA set. Kafka Producer configurations \u00b6 A Kafka Producer is the component responsible for sending events to the Apache Kafka cluster. Knative exposes all available Kafka Producer configurations that can be modified to suit your workloads. You can change these configurations by modifying the config-kafka-sink-data-plane config map in the knative-eventing namespace. Documentation for the settings available in this config map is available on the Apache Kafka website , in particular, Producer configurations . Enable debug logging for data plane components \u00b6 To enable debug logging for data plane components change the logging level to DEBUG in the kafka-config-logging config map. Apply the following kafka-config-logging config map: apiVersion : v1 kind : ConfigMap metadata : name : kafka-config-logging namespace : knative-eventing data : config.xml : | <configuration> <appender name=\"jsonConsoleAppender\" class=\"ch.qos.logback.core.ConsoleAppender\"> <encoder class=\"net.logstash.logback.encoder.LogstashEncoder\"/> </appender> <root level=\"DEBUG\"> <appender-ref ref=\"jsonConsoleAppender\"/> </root> </configuration> Restart the kafka-sink-receiver : kubectl rollout restart deployment -n knative-eventing kafka-sink-receiver Additional information \u00b6 To report bugs or add feature requests, open an issue in the eventing-kafka-broker repository .","title":"Apache Kafka Sink"},{"location":"eventing/sink/kafka-sink/#apache-kafka-sink","text":"This page shows how to install and configure Apache Kafka Sink.","title":"Apache Kafka Sink"},{"location":"eventing/sink/kafka-sink/#prerequisites","text":"Installing Eventing using YAML files .","title":"Prerequisites"},{"location":"eventing/sink/kafka-sink/#installation","text":"Install the Kafka controller: kubectl apply --filename http://github.com/knative-sandbox/eventing-kafka-broker/releases/download/v0.21.0/eventing-kafka-controller.yaml Install the Kafka Sink data plane: kubectl apply --filename http://github.com/knative-sandbox/eventing-kafka-broker/releases/download/v0.21.0/eventing-kafka-sink.yaml Verify that kafka-controller and kafka-sink-receiver are running: kubectl get deployments.apps -n knative-eventing Example output: NAME READY UP-TO-DATE AVAILABLE AGE eventing-controller 1 /1 1 1 10s eventing-webhook 1 /1 1 1 9s kafka-controller 1 /1 1 1 3s kafka-sink-receiver 1 /1 1 1 5s","title":"Installation"},{"location":"eventing/sink/kafka-sink/#kafka-sink","text":"A KafkaSink object looks like this: apiVersion : eventing.knative.dev/v1alpha1 kind : KafkaSink metadata : name : my-kafka-sink namespace : default spec : topic : mytopic bootstrapServers : - my-cluster-kafka-bootstrap.kafka:9092","title":"Kafka Sink"},{"location":"eventing/sink/kafka-sink/#security","text":"Apache Kafka supports different security features, Knative supports the followings: Authentication using SASL without encryption Authentication using SASL and encryption using SSL Authentication and encryption using SSL Encryption using SSL without client authentication To enable security features, in the KafkaSink spec, we can reference a Secret : apiVersion : eventing.knative.dev/v1alpha1 kind : KafkaSink metadata : name : my-kafka-sink namespace : default spec : topic : mytopic bootstrapServers : - my-cluster-kafka-bootstrap.kafka:9092 auth.secret.ref.name : my_secret The Secret my_secret must exist in the same namespace of the KafkaSink , in this case: default . Note: Certificates and keys must be in PEM format .","title":"Security"},{"location":"eventing/sink/kafka-sink/#authentication-using-sasl","text":"Knative supports the following SASL mechanisms: PLAIN SCRAM-SHA-256 SCRAM-SHA-512 To use a specific SASL mechanism replace <sasl_mechanism> with the mechanism of your choice.","title":"Authentication using SASL"},{"location":"eventing/sink/kafka-sink/#authentication-using-sasl-without-encryption","text":"kubectl create secret --namespace <namespace> generic <my_secret> \\ --from-literal = protocol = SASL_PLAINTEXT \\ --from-literal = sasl.mechanism = <sasl_mechanism> \\ --from-literal = user = <my_user> \\ --from-literal = password = <my_password>","title":"Authentication using SASL without encryption"},{"location":"eventing/sink/kafka-sink/#authentication-using-sasl-and-encryption-using-ssl","text":"kubectl create secret --namespace <namespace> generic <my_secret> \\ --from-literal = protocol = SASL_SSL \\ --from-literal = sasl.mechanism = <sasl_mechanism> \\ --from-file = ca.crt = caroot.pem \\ --from-literal = user = <my_user> \\ --from-literal = password = <my_password>","title":"Authentication using SASL and encryption using SSL"},{"location":"eventing/sink/kafka-sink/#encryption-using-ssl-without-client-authentication","text":"kubectl create secret --namespace <namespace> generic <my_secret> \\ --from-literal = protocol = SSL \\ --from-file = ca.crt = <my_caroot.pem_file_path> \\ --from-literal = user.skip = true","title":"Encryption using SSL without client authentication"},{"location":"eventing/sink/kafka-sink/#authentication-and-encryption-using-ssl","text":"kubectl create secret --namespace <namespace> generic <my_secret> \\ --from-literal = protocol = SSL \\ --from-file = ca.crt = <my_caroot.pem_file_path> \\ --from-file = user.crt = <my_cert.pem_file_path> \\ --from-file = user.key = <my_key.pem_file_path> NOTE: ca.crt can be omitted to fallback to use system's root CA set.","title":"Authentication and encryption using SSL"},{"location":"eventing/sink/kafka-sink/#kafka-producer-configurations","text":"A Kafka Producer is the component responsible for sending events to the Apache Kafka cluster. Knative exposes all available Kafka Producer configurations that can be modified to suit your workloads. You can change these configurations by modifying the config-kafka-sink-data-plane config map in the knative-eventing namespace. Documentation for the settings available in this config map is available on the Apache Kafka website , in particular, Producer configurations .","title":"Kafka Producer configurations"},{"location":"eventing/sink/kafka-sink/#enable-debug-logging-for-data-plane-components","text":"To enable debug logging for data plane components change the logging level to DEBUG in the kafka-config-logging config map. Apply the following kafka-config-logging config map: apiVersion : v1 kind : ConfigMap metadata : name : kafka-config-logging namespace : knative-eventing data : config.xml : | <configuration> <appender name=\"jsonConsoleAppender\" class=\"ch.qos.logback.core.ConsoleAppender\"> <encoder class=\"net.logstash.logback.encoder.LogstashEncoder\"/> </appender> <root level=\"DEBUG\"> <appender-ref ref=\"jsonConsoleAppender\"/> </root> </configuration> Restart the kafka-sink-receiver : kubectl rollout restart deployment -n knative-eventing kafka-sink-receiver","title":"Enable debug logging for data plane components"},{"location":"eventing/sink/kafka-sink/#additional-information","text":"To report bugs or add feature requests, open an issue in the eventing-kafka-broker repository .","title":"Additional information"},{"location":"eventing/sources/","text":"Event sources \u00b6 An event source is a Kubernetes custom resource (CR), created by a developer or cluster administrator, that acts as a link between an event producer and an event sink . A sink can be a k8s service, including Knative Services, a Channel, or a Broker that receives events from an event source. Event sources are created by instantiating a CR from a Source object. The Source object defines the arguments and parameters needed to instantiate a CR. All Sources are part of the sources category. You can list existing event sources on your cluster by entering the command: Kubectl kubectl get sources Kn kn source list Knative Sources \u00b6 Name API Version Maintainer Description APIServerSource v1 Knative Brings Kubernetes API server events into Knative. The APIServerSource fires a new event each time a Kubernetes resource is created, updated or deleted. AWS SQS v1alpha1 Knative Brings AWS Simple Queue Service messages into Knative. The AwsSqsSource fires a new event each time an event is published on an AWS SQS topic . Apache Camel v1alpha1 Knative Enables use of Apache Camel components for pushing events into Knative. A CamelSource is an event source that can represent any existing Apache Camel component , that provides a consumer side, and enables publishing events to an addressable endpoint. Each Camel endpoint has the form of a URI where the scheme is the ID of the component to use. CamelSource requires Camel-K to be installed into the current namespace. See the CamelSource example. Apache CouchDB v1alpha1 Knative Brings Apache CouchDB messages into Knative. Apache Kafka v1beta1 Knative Brings Apache Kafka messages into Knative. The KafkaSource reads events from an Apache Kafka Cluster, and passes these events to a sink so that they can be consumed. See the Kafka Source example for more details. Container Source v1 Knative The ContainerSource will instantiate container image(s) that can generate events until the ContainerSource is deleted. This may be used, for example, to poll an FTP server for new files or generate events at a set time interval. Given a spec.template with at least a container image specified, ContainerSource will keep a Pod running with the specified image(s). K_SINK (destination address) and KE_CE_OVERRIDES (JSON CloudEvents attributes) environment variables are injected into the running image(s). It is used by multiple other Sources as underlying infrastructure. Refer to the Container Source example for more details. GitHub v1alpha1 Knative Registers for events of the specified types on the specified GitHub organization or repository, and brings those events into Knative. The GitHubSource fires a new event for selected GitHub event types . See the GitHub Source example for more details. GitLab v1alpha1 Knative Registers for events of the specified types on the specified GitLab repository, and brings those events into Knative. The GitLabSource creates a webhooks for specified event types , listens for incoming events, and passes them to a consumer. See the GitLab Source example for more details. Heartbeats N/A Knative Uses an in-memory timer to produce events at the specified interval. PingSource v1beta2 Knative Produces events with a fixed payload on a specified Cron schedule. See the Ping Source example for more details. RabbitMQ Active development None Brings RabbitMQ messages into Knative. SinkBinding v1 Knative The SinkBinding can be used to author new event sources using any of the familiar compute abstractions that Kubernetes makes available (e.g. Deployment, Job, DaemonSet, StatefulSet), or Knative abstractions (e.g. Service, Configuration). SinkBinding provides a framework for injecting K_SINK (destination address) and K_CE_OVERRIDES (JSON cloudevents attributes) environment variables into any Kubernetes resource which has a spec.template that looks like a Pod (aka PodSpecable). See the SinkBinding example for more details. WebSocket N/A Knative Opens a WebSocket to the specified source and packages each received message as a Knative event. Third-Party Sources \u00b6 Name API Version Maintainer Description Auto Container Source Proof of Concept None AutoContainerSource is a controller that allows the Source CRDs without needing a controller. It notices CRDs with a specific label and starts controlling resources of that type. It utilizes Container Source as underlying infrastructure. Amazon CloudWatch Supported TriggerMesh Collects metrics from Amazon CloudWatch . Amazon CloudWatch Logs Supported TriggerMesh Subscribes to log events from an Amazon CloudWatch Logs stream. Amazon CodeCommit Supported TriggerMesh Registers for events emitted by an Amazon CodeCommit source code repository. Amazon Cognito Identity Supported TriggerMesh Registers for events from Amazon Cognito identity pools. Amazon Cognito User Supported TriggerMesh Registers for events from Amazon Cognito user pools. Amazon DynamoDB Supported TriggerMesh Reads records from an Amazon DynamoDB stream. Amazon Kinesis Supported TriggerMesh Reads records from an Amazon Kinesis stream. Amazon SNS Supported TriggerMesh Subscribes to messages from an Amazon SNS topic. Amazon SQS Supported TriggerMesh Consumes messages from an Amazon SQS queue. BitBucket Proof of Concept None Registers for events of the specified types on the specified BitBucket organization/repository. Brings those events into Knative. CloudAuditLogsSource v1 Google Registers for events of the specified types on the specified Google Cloud Audit Logs . Brings those events into Knative. Refer to the CloudAuditLogsSource example for more details. CloudPubSubSource v1 Google Brings Cloud Pub/Sub messages into Knative. The CloudPubSubSource fires a new event each time a message is published on a Google Cloud Platform PubSub topic . See the CloudPubSubSource example for more details. CloudSchedulerSource v1 Google Create, update, and delete Google Cloud Scheduler Jobs. When those jobs are triggered, receive the event inside Knative. See the CloudSchedulerSource example for further details. CloudStorageSource v1 Google Registers for events of the specified types on the specified Google Cloud Storage bucket and optional object prefix. Brings those events into Knative. See the CloudStorageSource example. DockerHubSource v1alpha1 None Retrieves events from Docker Hub Webhooks and transforms them into CloudEvents for consumption in Knative. FTP / SFTP Proof of concept None Watches for files being uploaded into a FTP/SFTP and generates events for those. Heartbeat Proof of Concept None Uses an in-memory timer to produce events as the specified interval. Uses AutoContainerSource for underlying infrastructure. Konnek Active Development None Retrieves events from cloud platforms (like AWS and GCP) and transforms them into CloudEvents for consumption in Knative. K8s Proof of Concept None Brings Kubernetes cluster events into Knative. Uses AutoContainerSource for underlying infrastructure. RedisSource v1alpha1 None Brings Redis Stream into Knative. Slack v1alpha1 TriggerMesh Subscribes to events from Slack. VMware Active Development None Brings vSphere events into Knative. Zendesk v1alpha1 TriggerMesh Subscribes to events from Zendesk. Additional resources \u00b6 For information about creating your own Source type, see the tutorial on writing a Source with a Receive Adapter . If your code needs to send events as part of its business logic and doesn't fit the model of a Source, consider feeding events directly to a Broker . For more information about using kn Source related commands, see the kn source reference documentation .","title":"Overview"},{"location":"eventing/sources/#event-sources","text":"An event source is a Kubernetes custom resource (CR), created by a developer or cluster administrator, that acts as a link between an event producer and an event sink . A sink can be a k8s service, including Knative Services, a Channel, or a Broker that receives events from an event source. Event sources are created by instantiating a CR from a Source object. The Source object defines the arguments and parameters needed to instantiate a CR. All Sources are part of the sources category. You can list existing event sources on your cluster by entering the command: Kubectl kubectl get sources Kn kn source list","title":"Event sources"},{"location":"eventing/sources/#knative-sources","text":"Name API Version Maintainer Description APIServerSource v1 Knative Brings Kubernetes API server events into Knative. The APIServerSource fires a new event each time a Kubernetes resource is created, updated or deleted. AWS SQS v1alpha1 Knative Brings AWS Simple Queue Service messages into Knative. The AwsSqsSource fires a new event each time an event is published on an AWS SQS topic . Apache Camel v1alpha1 Knative Enables use of Apache Camel components for pushing events into Knative. A CamelSource is an event source that can represent any existing Apache Camel component , that provides a consumer side, and enables publishing events to an addressable endpoint. Each Camel endpoint has the form of a URI where the scheme is the ID of the component to use. CamelSource requires Camel-K to be installed into the current namespace. See the CamelSource example. Apache CouchDB v1alpha1 Knative Brings Apache CouchDB messages into Knative. Apache Kafka v1beta1 Knative Brings Apache Kafka messages into Knative. The KafkaSource reads events from an Apache Kafka Cluster, and passes these events to a sink so that they can be consumed. See the Kafka Source example for more details. Container Source v1 Knative The ContainerSource will instantiate container image(s) that can generate events until the ContainerSource is deleted. This may be used, for example, to poll an FTP server for new files or generate events at a set time interval. Given a spec.template with at least a container image specified, ContainerSource will keep a Pod running with the specified image(s). K_SINK (destination address) and KE_CE_OVERRIDES (JSON CloudEvents attributes) environment variables are injected into the running image(s). It is used by multiple other Sources as underlying infrastructure. Refer to the Container Source example for more details. GitHub v1alpha1 Knative Registers for events of the specified types on the specified GitHub organization or repository, and brings those events into Knative. The GitHubSource fires a new event for selected GitHub event types . See the GitHub Source example for more details. GitLab v1alpha1 Knative Registers for events of the specified types on the specified GitLab repository, and brings those events into Knative. The GitLabSource creates a webhooks for specified event types , listens for incoming events, and passes them to a consumer. See the GitLab Source example for more details. Heartbeats N/A Knative Uses an in-memory timer to produce events at the specified interval. PingSource v1beta2 Knative Produces events with a fixed payload on a specified Cron schedule. See the Ping Source example for more details. RabbitMQ Active development None Brings RabbitMQ messages into Knative. SinkBinding v1 Knative The SinkBinding can be used to author new event sources using any of the familiar compute abstractions that Kubernetes makes available (e.g. Deployment, Job, DaemonSet, StatefulSet), or Knative abstractions (e.g. Service, Configuration). SinkBinding provides a framework for injecting K_SINK (destination address) and K_CE_OVERRIDES (JSON cloudevents attributes) environment variables into any Kubernetes resource which has a spec.template that looks like a Pod (aka PodSpecable). See the SinkBinding example for more details. WebSocket N/A Knative Opens a WebSocket to the specified source and packages each received message as a Knative event.","title":"Knative Sources"},{"location":"eventing/sources/#third-party-sources","text":"Name API Version Maintainer Description Auto Container Source Proof of Concept None AutoContainerSource is a controller that allows the Source CRDs without needing a controller. It notices CRDs with a specific label and starts controlling resources of that type. It utilizes Container Source as underlying infrastructure. Amazon CloudWatch Supported TriggerMesh Collects metrics from Amazon CloudWatch . Amazon CloudWatch Logs Supported TriggerMesh Subscribes to log events from an Amazon CloudWatch Logs stream. Amazon CodeCommit Supported TriggerMesh Registers for events emitted by an Amazon CodeCommit source code repository. Amazon Cognito Identity Supported TriggerMesh Registers for events from Amazon Cognito identity pools. Amazon Cognito User Supported TriggerMesh Registers for events from Amazon Cognito user pools. Amazon DynamoDB Supported TriggerMesh Reads records from an Amazon DynamoDB stream. Amazon Kinesis Supported TriggerMesh Reads records from an Amazon Kinesis stream. Amazon SNS Supported TriggerMesh Subscribes to messages from an Amazon SNS topic. Amazon SQS Supported TriggerMesh Consumes messages from an Amazon SQS queue. BitBucket Proof of Concept None Registers for events of the specified types on the specified BitBucket organization/repository. Brings those events into Knative. CloudAuditLogsSource v1 Google Registers for events of the specified types on the specified Google Cloud Audit Logs . Brings those events into Knative. Refer to the CloudAuditLogsSource example for more details. CloudPubSubSource v1 Google Brings Cloud Pub/Sub messages into Knative. The CloudPubSubSource fires a new event each time a message is published on a Google Cloud Platform PubSub topic . See the CloudPubSubSource example for more details. CloudSchedulerSource v1 Google Create, update, and delete Google Cloud Scheduler Jobs. When those jobs are triggered, receive the event inside Knative. See the CloudSchedulerSource example for further details. CloudStorageSource v1 Google Registers for events of the specified types on the specified Google Cloud Storage bucket and optional object prefix. Brings those events into Knative. See the CloudStorageSource example. DockerHubSource v1alpha1 None Retrieves events from Docker Hub Webhooks and transforms them into CloudEvents for consumption in Knative. FTP / SFTP Proof of concept None Watches for files being uploaded into a FTP/SFTP and generates events for those. Heartbeat Proof of Concept None Uses an in-memory timer to produce events as the specified interval. Uses AutoContainerSource for underlying infrastructure. Konnek Active Development None Retrieves events from cloud platforms (like AWS and GCP) and transforms them into CloudEvents for consumption in Knative. K8s Proof of Concept None Brings Kubernetes cluster events into Knative. Uses AutoContainerSource for underlying infrastructure. RedisSource v1alpha1 None Brings Redis Stream into Knative. Slack v1alpha1 TriggerMesh Subscribes to events from Slack. VMware Active Development None Brings vSphere events into Knative. Zendesk v1alpha1 TriggerMesh Subscribes to events from Zendesk.","title":"Third-Party Sources"},{"location":"eventing/sources/#additional-resources","text":"For information about creating your own Source type, see the tutorial on writing a Source with a Receive Adapter . If your code needs to send events as part of its business logic and doesn't fit the model of a Source, consider feeding events directly to a Broker . For more information about using kn Source related commands, see the kn source reference documentation .","title":"Additional resources"},{"location":"eventing/sources/apiserversource/","text":"APIServerSource \u00b6 An APIServerSource brings Kubernetes API server events into Knative. Installation \u00b6 The APIServerSource source type is enabled by default when you install Knative Eventing. Example \u00b6 This example shows how to create an APIServerSource that listens to Kubernetes Events and send CloudEvents to the Event Display Service. Creating a namespace \u00b6 Create a new namespace called apiserversource-example by entering the following command: kubectl create namespace apiserversource-example Creating the Event Display Service \u00b6 In this step, you create one event consumer, event-display to verify that APIServerSource is properly working. To deploy the event-display consumer to your cluster, run the following command: kubectl -n apiserversource-example apply -f - << EOF apiVersion: apps/v1 kind: Deployment metadata: name: event-display spec: replicas: 1 selector: matchLabels: &labels app: event-display template: metadata: labels: *labels spec: containers: - name: event-display image: gcr.io/knative-releases/knative.dev/eventing-contrib/cmd/event_display --- kind: Service apiVersion: v1 metadata: name: event-display spec: selector: app: event-display ports: - protocol: TCP port: 80 targetPort: 8080 EOF Creating a Service Account \u00b6 Create a Service Account that the ApiServerSource runs as. The ApiServerSource watches for Kubernetes events and forwards them to the Knative Eventing Broker. kubectl -n apiserversource-example apply -f - << EOF apiVersion: v1 kind: ServiceAccount metadata: name: events-sa namespace: apiserversource-example --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: event-watcher rules: - apiGroups: - \"\" resources: - events verbs: - get - list - watch --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: k8s-ra-event-watcher roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: event-watcher subjects: - kind: ServiceAccount name: events-sa namespace: apiserversource-example EOF Creating the APIServerSource \u00b6 In order to receive kubernetes events, you need to create a concrete APIServerSource for the namespace. YAML kubectl -n apiserversource-example apply -f - << EOF apiVersion: sources.knative.dev/v1 kind: ApiServerSource metadata: name: testevents namespace: apiserversource-example spec: serviceAccountName: events-sa mode: Resource resources: - apiVersion: v1 kind: Event sink: ref: apiVersion: v1 kind: Service name: event-display EOF Kn kn source apiserver create testevents \\ --namespace apiserversource-example \\ --mode \"Resource\" \\ --resource \"Event:v1\" \\ --service-account events-sa \\ --sink --sink http://event-display.svc.cluster.local Creating Events \u00b6 Create events by launching a pod in the default namespace. Create a busybox container and immediately delete it: kubectl -n apiserversource-example run busybox --image = busybox --restart = Never -- ls kubectl -n apiserversource-example delete pod busybox Verify \u00b6 We will verify that the Kubernetes events were sent into the Knative eventing system by looking at our message dumper function logs. kubectl -n apiserversource-example logs -l app = event-display --tail = 100 You should see log lines similar to: \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1.0 type: dev.knative.apiserver.resource.update source: https://10.96.0.1:443 subject: /apis/v1/namespaces/apiserversource-example/events/testevents.15dd3050eb1e6f50 id: e0447eb7-36b5-443b-9d37-faf4fe5c62f0 time: 2020-07-28T19:14:54.719501054Z datacontenttype: application/json Extensions, kind: Event name: busybox.1626008649e617e3 namespace: apiserversource-example Data, { \"apiVersion\": \"v1\", \"count\": 1, \"eventTime\": null, \"firstTimestamp\": \"2020-07-28T19:14:54Z\", \"involvedObject\": { \"apiVersion\": \"v1\", \"fieldPath\": \"spec.containers{busybox}\", \"kind\": \"Pod\", \"name\": \"busybox\", \"namespace\": \"apiserversource-example\", \"resourceVersion\": \"28987493\", \"uid\": \"1efb342a-737b-11e9-a6c5-42010a8a00ed\" }, \"kind\": \"Event\", \"lastTimestamp\": \"2020-07-28T19:14:54Z\", \"message\": \"Started container\", \"metadata\": { \"creationTimestamp\": \"2020-07-28T19:14:54Z\", \"name\": \"busybox.1626008649e617e3\", \"namespace\": \"default\", \"resourceVersion\": \"506088\", \"selfLink\": \"/api/v1/namespaces/apiserversource-example/events/busybox.1626008649e617e3\", \"uid\": \"2005af47-737b-11e9-a6c5-42010a8a00ed\" }, \"reason\": \"Started\", \"reportingComponent\": \"\", \"reportingInstance\": \"\", \"source\": { \"component\": \"kubelet\", \"host\": \"gke-knative-auto-cluster-default-pool-23c23c4f-xdj0\" }, \"type\": \"Normal\" } Cleanup \u00b6 Delete the apiserversource-example namespace and all of its resources from your cluster by entering the following command: kubectl delete namespace apiserversource-example Reference Documentation \u00b6 See the APIServerSource specification . Contact \u00b6 For any inquiries about this source, please reach out on to the Knative users group .","title":"APIServerSource"},{"location":"eventing/sources/apiserversource/#apiserversource","text":"An APIServerSource brings Kubernetes API server events into Knative.","title":"APIServerSource"},{"location":"eventing/sources/apiserversource/#installation","text":"The APIServerSource source type is enabled by default when you install Knative Eventing.","title":"Installation"},{"location":"eventing/sources/apiserversource/#example","text":"This example shows how to create an APIServerSource that listens to Kubernetes Events and send CloudEvents to the Event Display Service.","title":"Example"},{"location":"eventing/sources/apiserversource/#creating-a-namespace","text":"Create a new namespace called apiserversource-example by entering the following command: kubectl create namespace apiserversource-example","title":"Creating a namespace"},{"location":"eventing/sources/apiserversource/#creating-the-event-display-service","text":"In this step, you create one event consumer, event-display to verify that APIServerSource is properly working. To deploy the event-display consumer to your cluster, run the following command: kubectl -n apiserversource-example apply -f - << EOF apiVersion: apps/v1 kind: Deployment metadata: name: event-display spec: replicas: 1 selector: matchLabels: &labels app: event-display template: metadata: labels: *labels spec: containers: - name: event-display image: gcr.io/knative-releases/knative.dev/eventing-contrib/cmd/event_display --- kind: Service apiVersion: v1 metadata: name: event-display spec: selector: app: event-display ports: - protocol: TCP port: 80 targetPort: 8080 EOF","title":"Creating the Event Display Service"},{"location":"eventing/sources/apiserversource/#creating-a-service-account","text":"Create a Service Account that the ApiServerSource runs as. The ApiServerSource watches for Kubernetes events and forwards them to the Knative Eventing Broker. kubectl -n apiserversource-example apply -f - << EOF apiVersion: v1 kind: ServiceAccount metadata: name: events-sa namespace: apiserversource-example --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: event-watcher rules: - apiGroups: - \"\" resources: - events verbs: - get - list - watch --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: k8s-ra-event-watcher roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: event-watcher subjects: - kind: ServiceAccount name: events-sa namespace: apiserversource-example EOF","title":"Creating a Service Account"},{"location":"eventing/sources/apiserversource/#creating-the-apiserversource","text":"In order to receive kubernetes events, you need to create a concrete APIServerSource for the namespace. YAML kubectl -n apiserversource-example apply -f - << EOF apiVersion: sources.knative.dev/v1 kind: ApiServerSource metadata: name: testevents namespace: apiserversource-example spec: serviceAccountName: events-sa mode: Resource resources: - apiVersion: v1 kind: Event sink: ref: apiVersion: v1 kind: Service name: event-display EOF Kn kn source apiserver create testevents \\ --namespace apiserversource-example \\ --mode \"Resource\" \\ --resource \"Event:v1\" \\ --service-account events-sa \\ --sink --sink http://event-display.svc.cluster.local","title":"Creating the APIServerSource"},{"location":"eventing/sources/apiserversource/#creating-events","text":"Create events by launching a pod in the default namespace. Create a busybox container and immediately delete it: kubectl -n apiserversource-example run busybox --image = busybox --restart = Never -- ls kubectl -n apiserversource-example delete pod busybox","title":"Creating Events"},{"location":"eventing/sources/apiserversource/#verify","text":"We will verify that the Kubernetes events were sent into the Knative eventing system by looking at our message dumper function logs. kubectl -n apiserversource-example logs -l app = event-display --tail = 100 You should see log lines similar to: \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1.0 type: dev.knative.apiserver.resource.update source: https://10.96.0.1:443 subject: /apis/v1/namespaces/apiserversource-example/events/testevents.15dd3050eb1e6f50 id: e0447eb7-36b5-443b-9d37-faf4fe5c62f0 time: 2020-07-28T19:14:54.719501054Z datacontenttype: application/json Extensions, kind: Event name: busybox.1626008649e617e3 namespace: apiserversource-example Data, { \"apiVersion\": \"v1\", \"count\": 1, \"eventTime\": null, \"firstTimestamp\": \"2020-07-28T19:14:54Z\", \"involvedObject\": { \"apiVersion\": \"v1\", \"fieldPath\": \"spec.containers{busybox}\", \"kind\": \"Pod\", \"name\": \"busybox\", \"namespace\": \"apiserversource-example\", \"resourceVersion\": \"28987493\", \"uid\": \"1efb342a-737b-11e9-a6c5-42010a8a00ed\" }, \"kind\": \"Event\", \"lastTimestamp\": \"2020-07-28T19:14:54Z\", \"message\": \"Started container\", \"metadata\": { \"creationTimestamp\": \"2020-07-28T19:14:54Z\", \"name\": \"busybox.1626008649e617e3\", \"namespace\": \"default\", \"resourceVersion\": \"506088\", \"selfLink\": \"/api/v1/namespaces/apiserversource-example/events/busybox.1626008649e617e3\", \"uid\": \"2005af47-737b-11e9-a6c5-42010a8a00ed\" }, \"reason\": \"Started\", \"reportingComponent\": \"\", \"reportingInstance\": \"\", \"source\": { \"component\": \"kubelet\", \"host\": \"gke-knative-auto-cluster-default-pool-23c23c4f-xdj0\" }, \"type\": \"Normal\" }","title":"Verify"},{"location":"eventing/sources/apiserversource/#cleanup","text":"Delete the apiserversource-example namespace and all of its resources from your cluster by entering the following command: kubectl delete namespace apiserversource-example","title":"Cleanup"},{"location":"eventing/sources/apiserversource/#reference-documentation","text":"See the APIServerSource specification .","title":"Reference Documentation"},{"location":"eventing/sources/apiserversource/#contact","text":"For any inquiries about this source, please reach out on to the Knative users group .","title":"Contact"},{"location":"eventing/sources/containersource/","text":"ContainerSource \u00b6 ContainerSource will start a container image which will generate events under certain situations and send messages to a sink URI. It also can be an easy way to support your own event sources in Knative. This guide shows how to configure ContainerSource as an event source for functions and summarizes guidelines for creating your own event source as a ContainerSource. Prerequisites \u00b6 Install ko Set KO_DOCKER_REPO (e.g. gcr.io/[gcloud-project] or docker.io/<username> ) Authenticated with your KO_DOCKER_REPO Install docker Installation \u00b6 The ContainerSource source type is enabled by default when you install Knative Eventing. Example \u00b6 This example shows how the heartbeats container sends events to the Event Display Service. Preparing the heartbeats image \u00b6 Knative event-sources has a sample of heartbeats event source. You could clone the source code by git clone -b \"v0.21.x\" https://github.com/knative/eventing-contrib.git And then build a heartbeats image and publish to your image repo with ko publish knative.dev/eventing-contrib/cmd/heartbeats Creating a namespace \u00b6 Create a new namespace called containersource-example by entering the following command: kubectl create namespace containersource-example Creating the Event Display Service \u00b6 In order to verify ContainerSource is working, we will create a Event Display Service that dumps incoming messages to its log. kubectl -n containersource-example apply -f - << EOF apiVersion: apps/v1 kind: Deployment metadata: name: event-display spec: replicas: 1 selector: matchLabels: &labels app: event-display template: metadata: labels: *labels spec: containers: - name: event-display image: gcr.io/knative-releases/knative.dev/eventing-contrib/cmd/event_display --- kind: Service apiVersion: v1 metadata: name: event-display spec: selector: app: event-display ports: - protocol: TCP port: 80 targetPort: 8080 EOF Creating the ContainerSource using the heartbeats image \u00b6 In order to run the heartbeats container as an event source, you have to create a concrete ContainerSource with specific arguments and environment settings. Be sure to replace heartbeats_image_uri with a valid uri for your heartbeats image you published in the previous step. Note that arguments and environment variables are set and will be passed to the container. kubectl -n containersource-example apply -f - << EOF apiVersion: sources.knative.dev/v1 kind: ContainerSource metadata: name: test-heartbeats spec: template: spec: containers: # This corresponds to a heartbeats image uri you build and publish in the previous step # e.g. gcr.io/[gcloud-project]/knative.dev/eventing-contrib/cmd/heartbeats - image: <heartbeats_image_uri> name: heartbeats args: - --period=1 env: - name: POD_NAME value: \"mypod\" - name: POD_NAMESPACE value: \"event-test\" sink: ref: apiVersion: v1 kind: Service name: event-display EOF Verify \u00b6 View the logs for the event-display event consumer by entering the following command: kubectl -n containersource-example logs -l app = event-display --tail = 200 This returns the Attributes and Data of the events that the ContainerSource sent to the event-display Service: \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1.0 type: dev.knative.eventing.samples.heartbeat source: https://knative.dev/eventing-contrib/cmd/heartbeats/#event-test/mypod id: 2b72d7bf-c38f-4a98-a433-608fbcdd2596 time: 2019-10-18T15:23:20.809775386Z contenttype: application/json Extensions, beats: true heart: yes the: 42 Data, { \"id\": 2, \"label\": \"\" } Cleanup \u00b6 Delete the containersource-example namespace and all of its resources from your cluster by entering the following command: kubectl delete namespace containersource-example Reference Documentation \u00b6 See the ContainerSource specification . Contact \u00b6 For any inquiries about this source, please reach out on to the Knative users group .","title":"ContainerSource"},{"location":"eventing/sources/containersource/#containersource","text":"ContainerSource will start a container image which will generate events under certain situations and send messages to a sink URI. It also can be an easy way to support your own event sources in Knative. This guide shows how to configure ContainerSource as an event source for functions and summarizes guidelines for creating your own event source as a ContainerSource.","title":"ContainerSource"},{"location":"eventing/sources/containersource/#prerequisites","text":"Install ko Set KO_DOCKER_REPO (e.g. gcr.io/[gcloud-project] or docker.io/<username> ) Authenticated with your KO_DOCKER_REPO Install docker","title":"Prerequisites"},{"location":"eventing/sources/containersource/#installation","text":"The ContainerSource source type is enabled by default when you install Knative Eventing.","title":"Installation"},{"location":"eventing/sources/containersource/#example","text":"This example shows how the heartbeats container sends events to the Event Display Service.","title":"Example"},{"location":"eventing/sources/containersource/#preparing-the-heartbeats-image","text":"Knative event-sources has a sample of heartbeats event source. You could clone the source code by git clone -b \"v0.21.x\" https://github.com/knative/eventing-contrib.git And then build a heartbeats image and publish to your image repo with ko publish knative.dev/eventing-contrib/cmd/heartbeats","title":"Preparing the heartbeats image"},{"location":"eventing/sources/containersource/#creating-a-namespace","text":"Create a new namespace called containersource-example by entering the following command: kubectl create namespace containersource-example","title":"Creating a namespace"},{"location":"eventing/sources/containersource/#creating-the-event-display-service","text":"In order to verify ContainerSource is working, we will create a Event Display Service that dumps incoming messages to its log. kubectl -n containersource-example apply -f - << EOF apiVersion: apps/v1 kind: Deployment metadata: name: event-display spec: replicas: 1 selector: matchLabels: &labels app: event-display template: metadata: labels: *labels spec: containers: - name: event-display image: gcr.io/knative-releases/knative.dev/eventing-contrib/cmd/event_display --- kind: Service apiVersion: v1 metadata: name: event-display spec: selector: app: event-display ports: - protocol: TCP port: 80 targetPort: 8080 EOF","title":"Creating the Event Display Service"},{"location":"eventing/sources/containersource/#creating-the-containersource-using-the-heartbeats-image","text":"In order to run the heartbeats container as an event source, you have to create a concrete ContainerSource with specific arguments and environment settings. Be sure to replace heartbeats_image_uri with a valid uri for your heartbeats image you published in the previous step. Note that arguments and environment variables are set and will be passed to the container. kubectl -n containersource-example apply -f - << EOF apiVersion: sources.knative.dev/v1 kind: ContainerSource metadata: name: test-heartbeats spec: template: spec: containers: # This corresponds to a heartbeats image uri you build and publish in the previous step # e.g. gcr.io/[gcloud-project]/knative.dev/eventing-contrib/cmd/heartbeats - image: <heartbeats_image_uri> name: heartbeats args: - --period=1 env: - name: POD_NAME value: \"mypod\" - name: POD_NAMESPACE value: \"event-test\" sink: ref: apiVersion: v1 kind: Service name: event-display EOF","title":"Creating the ContainerSource using the heartbeats image"},{"location":"eventing/sources/containersource/#verify","text":"View the logs for the event-display event consumer by entering the following command: kubectl -n containersource-example logs -l app = event-display --tail = 200 This returns the Attributes and Data of the events that the ContainerSource sent to the event-display Service: \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1.0 type: dev.knative.eventing.samples.heartbeat source: https://knative.dev/eventing-contrib/cmd/heartbeats/#event-test/mypod id: 2b72d7bf-c38f-4a98-a433-608fbcdd2596 time: 2019-10-18T15:23:20.809775386Z contenttype: application/json Extensions, beats: true heart: yes the: 42 Data, { \"id\": 2, \"label\": \"\" }","title":"Verify"},{"location":"eventing/sources/containersource/#cleanup","text":"Delete the containersource-example namespace and all of its resources from your cluster by entering the following command: kubectl delete namespace containersource-example","title":"Cleanup"},{"location":"eventing/sources/containersource/#reference-documentation","text":"See the ContainerSource specification .","title":"Reference Documentation"},{"location":"eventing/sources/containersource/#contact","text":"For any inquiries about this source, please reach out on to the Knative users group .","title":"Contact"},{"location":"eventing/sources/pingsource/","text":"PingSource \u00b6 A PingSource produces events with a fixed payload on a specified cron schedule. Installation \u00b6 The PingSource source type is enabled by default when you install Knative Eventing. Example \u00b6 This example shows how to send an event every minute to a Event Display Service. Creating a namespace \u00b6 Create a new namespace called pingsource-example by entering the following command: kubectl create namespace pingsource-example Creating the Event Display Service \u00b6 In this step, you create one event consumer, event-display to verify that PingSource is properly working. To deploy the event-display consumer to your cluster, run the following command: kubectl -n pingsource-example apply -f - << EOF apiVersion: apps/v1 kind: Deployment metadata: name: event-display spec: replicas: 1 selector: matchLabels: &labels app: event-display template: metadata: labels: *labels spec: containers: - name: event-display image: gcr.io/knative-releases/knative.dev/eventing-contrib/cmd/event_display --- kind: Service apiVersion: v1 metadata: name: event-display spec: selector: app: event-display ports: - protocol: TCP port: 80 targetPort: 8080 EOF Creating the PingSource \u00b6 You can now create the PingSource sending an event containing {\"message\": \"Hello world!\"} every minute. YAML kubectl create -n pingsource-example -f - <<EOF apiVersion: sources.knative.dev/v1beta2 kind: PingSource metadata: name: test-ping-source spec: schedule: \"*/1 * * * *\" contentType: \"application/json\" data: '{\"message\": \"Hello world!\"}' sink: ref: apiVersion: v1 kind: Service name: event-display EOF Kn kn source ping create test-ping-source \\ --namespace pingsource-example \\ --schedule \"*/1 * * * *\" \\ --data '{\"message\": \"Hello world!\"}' \\ --sink http://event-display.pingsource-example.svc.cluster.local Warning Notice that the namespace is specified in two places in the command in --namespace and the --sink hostname (Optional) Create a PingSource with binary data \u00b6 Sometimes you may want to send binary data, which cannot be directly serialized in yaml, to downstream. This can be achieved by using dataBase64 as the payload. As the name suggests, dataBase64 should carry data that is base64 encoded. Please note that data and dataBase64 cannot co-exist. kubectl create -n pingsource-example -f - <<EOF apiVersion: sources.knative.dev/v1beta2 kind: PingSource metadata: name: test-ping-source-binary spec: schedule: \"*/1 * * * *\" contentType: \"text/plain\" dataBase64: \"ZGF0YQ==\" sink: ref: apiVersion: v1 kind: Service name: event-display EOF Verify \u00b6 View the logs for the event-display event consumer by entering the following command: kubectl -n pingsource-example logs -l app = event-display --tail = 100 This returns the Attributes and Data of the events that the PingSource sent to the event-display Service: \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1 .0 type: dev.knative.sources.ping source: /apis/v1/namespaces/pingsource-example/pingsources/test-ping-source id: 49f04fe2-7708-453d-ae0a-5fbaca9586a8 time: 2021 -03-25T19:41:00.444508332Z datacontenttype: application/json Data, { \"message\" : \"Hello world!\" } If you created a PingSource with binary data, you should also see the following: \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1 .0 type: dev.knative.sources.ping source: /apis/v1/namespaces/pingsource-example/pingsources/test-ping-source-binary id: ddd7bad2-9b6a-42a7-8f9b-b64494a6ce43 time: 2021 -03-25T19:38:00.455013472Z datacontenttype: text/plain Data, data Cleanup \u00b6 Delete the pingsource-example namespace and all of its resources from your cluster by entering the following command: kubectl delete namespace pingsource-example Reference Documentation \u00b6 See the PingSource specification . Contact \u00b6 For any inquiries about this source, please reach out on to the Knative users group .","title":"PingSource"},{"location":"eventing/sources/pingsource/#pingsource","text":"A PingSource produces events with a fixed payload on a specified cron schedule.","title":"PingSource"},{"location":"eventing/sources/pingsource/#installation","text":"The PingSource source type is enabled by default when you install Knative Eventing.","title":"Installation"},{"location":"eventing/sources/pingsource/#example","text":"This example shows how to send an event every minute to a Event Display Service.","title":"Example"},{"location":"eventing/sources/pingsource/#creating-a-namespace","text":"Create a new namespace called pingsource-example by entering the following command: kubectl create namespace pingsource-example","title":"Creating a namespace"},{"location":"eventing/sources/pingsource/#creating-the-event-display-service","text":"In this step, you create one event consumer, event-display to verify that PingSource is properly working. To deploy the event-display consumer to your cluster, run the following command: kubectl -n pingsource-example apply -f - << EOF apiVersion: apps/v1 kind: Deployment metadata: name: event-display spec: replicas: 1 selector: matchLabels: &labels app: event-display template: metadata: labels: *labels spec: containers: - name: event-display image: gcr.io/knative-releases/knative.dev/eventing-contrib/cmd/event_display --- kind: Service apiVersion: v1 metadata: name: event-display spec: selector: app: event-display ports: - protocol: TCP port: 80 targetPort: 8080 EOF","title":"Creating the Event Display Service"},{"location":"eventing/sources/pingsource/#creating-the-pingsource","text":"You can now create the PingSource sending an event containing {\"message\": \"Hello world!\"} every minute. YAML kubectl create -n pingsource-example -f - <<EOF apiVersion: sources.knative.dev/v1beta2 kind: PingSource metadata: name: test-ping-source spec: schedule: \"*/1 * * * *\" contentType: \"application/json\" data: '{\"message\": \"Hello world!\"}' sink: ref: apiVersion: v1 kind: Service name: event-display EOF Kn kn source ping create test-ping-source \\ --namespace pingsource-example \\ --schedule \"*/1 * * * *\" \\ --data '{\"message\": \"Hello world!\"}' \\ --sink http://event-display.pingsource-example.svc.cluster.local Warning Notice that the namespace is specified in two places in the command in --namespace and the --sink hostname","title":"Creating the PingSource"},{"location":"eventing/sources/pingsource/#optional-create-a-pingsource-with-binary-data","text":"Sometimes you may want to send binary data, which cannot be directly serialized in yaml, to downstream. This can be achieved by using dataBase64 as the payload. As the name suggests, dataBase64 should carry data that is base64 encoded. Please note that data and dataBase64 cannot co-exist. kubectl create -n pingsource-example -f - <<EOF apiVersion: sources.knative.dev/v1beta2 kind: PingSource metadata: name: test-ping-source-binary spec: schedule: \"*/1 * * * *\" contentType: \"text/plain\" dataBase64: \"ZGF0YQ==\" sink: ref: apiVersion: v1 kind: Service name: event-display EOF","title":"(Optional) Create a PingSource with binary data"},{"location":"eventing/sources/pingsource/#verify","text":"View the logs for the event-display event consumer by entering the following command: kubectl -n pingsource-example logs -l app = event-display --tail = 100 This returns the Attributes and Data of the events that the PingSource sent to the event-display Service: \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1 .0 type: dev.knative.sources.ping source: /apis/v1/namespaces/pingsource-example/pingsources/test-ping-source id: 49f04fe2-7708-453d-ae0a-5fbaca9586a8 time: 2021 -03-25T19:41:00.444508332Z datacontenttype: application/json Data, { \"message\" : \"Hello world!\" } If you created a PingSource with binary data, you should also see the following: \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1 .0 type: dev.knative.sources.ping source: /apis/v1/namespaces/pingsource-example/pingsources/test-ping-source-binary id: ddd7bad2-9b6a-42a7-8f9b-b64494a6ce43 time: 2021 -03-25T19:38:00.455013472Z datacontenttype: text/plain Data, data","title":"Verify"},{"location":"eventing/sources/pingsource/#cleanup","text":"Delete the pingsource-example namespace and all of its resources from your cluster by entering the following command: kubectl delete namespace pingsource-example","title":"Cleanup"},{"location":"eventing/sources/pingsource/#reference-documentation","text":"See the PingSource specification .","title":"Reference Documentation"},{"location":"eventing/sources/pingsource/#contact","text":"For any inquiries about this source, please reach out on to the Knative users group .","title":"Contact"},{"location":"eventing/sources/sinkbinding/","text":"Sink binding \u00b6 The SinkBinding custom object supports decoupling event production from delivery addressing. You can use sink binding to connect Kubernetes resources that embed a PodSpec and want to produce events, such as an event source, to an addressable Kubernetes object that can receive events, also known as an event sink . Sink binding can be used to create new event sources using any of the familiar compute objects that Kubernetes makes available. For example, Deployment , Job , DaemonSet , or StatefulSet objects, or Knative abstractions, such as Service or Configuration objects, can be used. Sink binding injects environment variables into the PodTemplateSpec of the event sink, so that the application code does not need to interact directly with the Kubernetes API to locate the event destination. Sink binding operates in one of two modes; Inclusion or Exclusion . You can set the mode by modifying the SINK_BINDING_SELECTION_MODE of the eventing-webhook deployment accordingly. The mode determines the default scope of the webhook. By default, the webhook is set to exclusion mode, which means that any namespace that does not have the label bindings.knative.dev/exclude: true will be subject to mutation evalutation. If SINK_BINDING_SELECTION_MODE is set to inclusion , only the resources in a namespace labelled with bindings.knative.dev/include: true will be considered. In inclusion mode, any SinkBinding resource created will automatically label the subject namespace with bindings.knative.dev/include: true for inclusion in the potential environment variable inclusions. Getting started \u00b6 The following procedures show how you can create a sink binding and connect it to a service and event source in your cluster. Creating a namespace \u00b6 Create a namespace called sinkbinding-example : kubectl create namespace sinkbinding-example Creating a Knative service \u00b6 Create a Knative service if you do not have an existing event sink that you want to connect to the sink binding. Prerequisites \u00b6 You must have Knative Serving installed on your cluster. Optional: If you want to use kn commands with sink binding, you must install the kn CLI. Procedure \u00b6 Create a Knative service: Kn kn service create hello --image gcr.io/knative-releases/knative.dev/eventing-contrib/cmd/event_display --env RESPONSE = \"Hello Serverless!\" YAML Copy the sample YAML into a service.yaml file: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : event-display spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing-contrib/cmd/event_display Apply the file: kubectl apply --filename service.yaml Creating a cron job \u00b6 Create a cron job if you do not have an existing event source that you want to connect to the sink binding. Create a CronJob object: Copy the sample YAML into a cronjob.yaml file: apiVersion : batch/v1beta1 kind : CronJob metadata : name : heartbeat-cron spec : # Run every minute schedule : \"*/1 * * * *\" jobTemplate : metadata : labels : app : heartbeat-cron spec : template : spec : restartPolicy : Never containers : - name : single-heartbeat image : gcr.io/knative-releases/knative.dev/eventing-contrib/cmd/heartbeats args : - --period=1 env : - name : ONE_SHOT value : \"true\" - name : POD_NAME valueFrom : fieldRef : fieldPath : metadata.name - name : POD_NAMESPACE valueFrom : fieldRef : fieldPath : metadata.namespace Apply the file: kubectl apply --filename heartbeats-source.yaml Cloning a sample heartbeat cron job \u00b6 Knative event-contrib contains a sample heartbeats event source. Prerequisites \u00b6 Ensure that ko publish is set up correctly: KO_DOCKER_REPO must be set. For example, gcr.io/[gcloud-project] or docker.io/<username> . You must have authenticated with your KO_DOCKER_REPO . Procedure \u00b6 Clone the event-contib repository: $ git clone -b \"v0.21.x\" https://github.com/knative/eventing-contrib.git Build a heartbeats image, and publish the image to your image repository: $ ko publish knative.dev/eventing-contrib/cmd/heartbeats Creating a SinkBinding object \u00b6 Create a SinkBinding object that directs events from your cron job to the event sink. Prerequisites \u00b6 You must have Knative Eventing installed on your cluster. Optional: If you want to use kn commands with sink binding, you must install the kn CLI. Procedure \u00b6 Create a sink binding: Kn kn source binding create bind-heartbeat \\ --namespace sinkbinding-example \\ --subject \"Job:batch/v1:app=heartbeat-cron\" \\ --sink http://event-display.svc.cluster.local \\ --ce-override \"sink=bound\" YAML Copy the sample YAML into a cronjob.yaml file: apiVersion : sources.knative.dev/v1alpha1 kind : SinkBinding metadata : name : bind-heartbeat spec : subject : apiVersion : batch/v1 kind : Job selector : matchLabels : app : heartbeat-cron sink : ref : apiVersion : serving.knative.dev/v1 kind : Service name : event-display Apply the file: kubectl apply --filename heartbeats-source.yaml Verification steps \u00b6 Verify that a message was sent to the Knative eventing system by looking at the event-display service logs: kubectl logs -l serving.knative.dev/service = event-display -c user-container --since = 10m Observe the lines showing the request headers and body of the event message, sent by the heartbeats source to the display function: \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1 .0 type: dev.knative.eventing.samples.heartbeat source: https://knative.dev/eventing-contrib/cmd/heartbeats/#default/heartbeat-cron-1582120020-75qrz id: 5f4122be-ac6f-4349-a94f-4bfc6eb3f687 time: 2020 -02-19T13:47:10.41428688Z datacontenttype: application/json Extensions, beats: true heart: yes the: 42 Data, { \"id\" : 1 , \"label\" : \"\" } Cleanup \u00b6 Delete the sinkbinding-example namespace and all of its resources from your cluster: kubectl delete namespace sinkbinding-example","title":"Sink Binding"},{"location":"eventing/sources/sinkbinding/#sink-binding","text":"The SinkBinding custom object supports decoupling event production from delivery addressing. You can use sink binding to connect Kubernetes resources that embed a PodSpec and want to produce events, such as an event source, to an addressable Kubernetes object that can receive events, also known as an event sink . Sink binding can be used to create new event sources using any of the familiar compute objects that Kubernetes makes available. For example, Deployment , Job , DaemonSet , or StatefulSet objects, or Knative abstractions, such as Service or Configuration objects, can be used. Sink binding injects environment variables into the PodTemplateSpec of the event sink, so that the application code does not need to interact directly with the Kubernetes API to locate the event destination. Sink binding operates in one of two modes; Inclusion or Exclusion . You can set the mode by modifying the SINK_BINDING_SELECTION_MODE of the eventing-webhook deployment accordingly. The mode determines the default scope of the webhook. By default, the webhook is set to exclusion mode, which means that any namespace that does not have the label bindings.knative.dev/exclude: true will be subject to mutation evalutation. If SINK_BINDING_SELECTION_MODE is set to inclusion , only the resources in a namespace labelled with bindings.knative.dev/include: true will be considered. In inclusion mode, any SinkBinding resource created will automatically label the subject namespace with bindings.knative.dev/include: true for inclusion in the potential environment variable inclusions.","title":"Sink binding"},{"location":"eventing/sources/sinkbinding/#getting-started","text":"The following procedures show how you can create a sink binding and connect it to a service and event source in your cluster.","title":"Getting started"},{"location":"eventing/sources/sinkbinding/#creating-a-namespace","text":"Create a namespace called sinkbinding-example : kubectl create namespace sinkbinding-example","title":"Creating a namespace"},{"location":"eventing/sources/sinkbinding/#creating-a-knative-service","text":"Create a Knative service if you do not have an existing event sink that you want to connect to the sink binding.","title":"Creating a Knative service"},{"location":"eventing/sources/sinkbinding/#prerequisites","text":"You must have Knative Serving installed on your cluster. Optional: If you want to use kn commands with sink binding, you must install the kn CLI.","title":"Prerequisites"},{"location":"eventing/sources/sinkbinding/#procedure","text":"Create a Knative service: Kn kn service create hello --image gcr.io/knative-releases/knative.dev/eventing-contrib/cmd/event_display --env RESPONSE = \"Hello Serverless!\" YAML Copy the sample YAML into a service.yaml file: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : event-display spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing-contrib/cmd/event_display Apply the file: kubectl apply --filename service.yaml","title":"Procedure"},{"location":"eventing/sources/sinkbinding/#creating-a-cron-job","text":"Create a cron job if you do not have an existing event source that you want to connect to the sink binding. Create a CronJob object: Copy the sample YAML into a cronjob.yaml file: apiVersion : batch/v1beta1 kind : CronJob metadata : name : heartbeat-cron spec : # Run every minute schedule : \"*/1 * * * *\" jobTemplate : metadata : labels : app : heartbeat-cron spec : template : spec : restartPolicy : Never containers : - name : single-heartbeat image : gcr.io/knative-releases/knative.dev/eventing-contrib/cmd/heartbeats args : - --period=1 env : - name : ONE_SHOT value : \"true\" - name : POD_NAME valueFrom : fieldRef : fieldPath : metadata.name - name : POD_NAMESPACE valueFrom : fieldRef : fieldPath : metadata.namespace Apply the file: kubectl apply --filename heartbeats-source.yaml","title":"Creating a cron job"},{"location":"eventing/sources/sinkbinding/#cloning-a-sample-heartbeat-cron-job","text":"Knative event-contrib contains a sample heartbeats event source.","title":"Cloning a sample heartbeat cron job"},{"location":"eventing/sources/sinkbinding/#prerequisites_1","text":"Ensure that ko publish is set up correctly: KO_DOCKER_REPO must be set. For example, gcr.io/[gcloud-project] or docker.io/<username> . You must have authenticated with your KO_DOCKER_REPO .","title":"Prerequisites"},{"location":"eventing/sources/sinkbinding/#procedure_1","text":"Clone the event-contib repository: $ git clone -b \"v0.21.x\" https://github.com/knative/eventing-contrib.git Build a heartbeats image, and publish the image to your image repository: $ ko publish knative.dev/eventing-contrib/cmd/heartbeats","title":"Procedure"},{"location":"eventing/sources/sinkbinding/#creating-a-sinkbinding-object","text":"Create a SinkBinding object that directs events from your cron job to the event sink.","title":"Creating a SinkBinding object"},{"location":"eventing/sources/sinkbinding/#prerequisites_2","text":"You must have Knative Eventing installed on your cluster. Optional: If you want to use kn commands with sink binding, you must install the kn CLI.","title":"Prerequisites"},{"location":"eventing/sources/sinkbinding/#procedure_2","text":"Create a sink binding: Kn kn source binding create bind-heartbeat \\ --namespace sinkbinding-example \\ --subject \"Job:batch/v1:app=heartbeat-cron\" \\ --sink http://event-display.svc.cluster.local \\ --ce-override \"sink=bound\" YAML Copy the sample YAML into a cronjob.yaml file: apiVersion : sources.knative.dev/v1alpha1 kind : SinkBinding metadata : name : bind-heartbeat spec : subject : apiVersion : batch/v1 kind : Job selector : matchLabels : app : heartbeat-cron sink : ref : apiVersion : serving.knative.dev/v1 kind : Service name : event-display Apply the file: kubectl apply --filename heartbeats-source.yaml","title":"Procedure"},{"location":"eventing/sources/sinkbinding/#verification-steps","text":"Verify that a message was sent to the Knative eventing system by looking at the event-display service logs: kubectl logs -l serving.knative.dev/service = event-display -c user-container --since = 10m Observe the lines showing the request headers and body of the event message, sent by the heartbeats source to the display function: \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1 .0 type: dev.knative.eventing.samples.heartbeat source: https://knative.dev/eventing-contrib/cmd/heartbeats/#default/heartbeat-cron-1582120020-75qrz id: 5f4122be-ac6f-4349-a94f-4bfc6eb3f687 time: 2020 -02-19T13:47:10.41428688Z datacontenttype: application/json Extensions, beats: true heart: yes the: 42 Data, { \"id\" : 1 , \"label\" : \"\" }","title":"Verification steps"},{"location":"eventing/sources/sinkbinding/#cleanup","text":"Delete the sinkbinding-example namespace and all of its resources from your cluster: kubectl delete namespace sinkbinding-example","title":"Cleanup"},{"location":"eventing/sources/creating-event-sources/_index/","text":"Creating an event source \u00b6 You can create your own event source for use with Knative Eventing components by using the following methods: Build an event source in Javascript, and implement it using a ContainerSource or SinkBinding. By creating your own event source controller, receiver adapter, and custom resource definition (CRD).","title":"Creating an event source"},{"location":"eventing/sources/creating-event-sources/_index/#creating-an-event-source","text":"You can create your own event source for use with Knative Eventing components by using the following methods: Build an event source in Javascript, and implement it using a ContainerSource or SinkBinding. By creating your own event source controller, receiver adapter, and custom resource definition (CRD).","title":"Creating an event source"},{"location":"eventing/sources/creating-event-sources/writing-event-source/01-theory/","text":"Topics \u00b6 What are the personas and critical paths? Contributor: implement a new source with minimal k8s overhead (don't have to learn controller/k8s internals) Operator: easily install Sources and verify that they are \"safe\" Developer: easily discover what Sources they can pull from on this cluster Developer: easily configure a Source based on existing knowledge of other Sources. Separation of concerns \u00b6 Contributor: \u00b6 Receive Adapter (RA) - process that receives incoming events. Implement CloudEvent binding interfaces, cloudevent's go sdk provides libraries for standard access to configure interfaces as needed. Passing configuration from the Source CRD YAML, that the controller needs to configure the Receive Adapter Source library (provided by Knative): \u00b6 Controller runtime (this is what we share via injection) incorporates protocol specific config into \"generic controller\" CRD. Identifying specific event characteristics (i.e. value of interest, relevant metadata, etc) to pass along to the serverless system Propagating events internally to the system (i.e. cloudevents) Theory \u00b6 Quick Introduction to Knative Eventing Sources A Knative Source is Kubernetes Custom Resource that generates or imports an event and pushes that event to another endpoint on the cluster via a CloudEvents . The specification for Knative Eventing Sources contains a number of requirements that together define a well-behaved Knative Source. To achieve this, there are several separations of concerns that we have to keep in mind: 1. A controller to run our Event Source and reconcile the underlying Receive Adapter deployments 2. A \"receive adapter\" which generates or imports the actual events 3. A series of identifying characteristics for our event 4. Transporting a valid event to the serverless system for further processing There are also two different classes of developer to consider: 1. A \"contributor\" knows about the foreign protocol but is not a Knative expert. 2. Knative Eventing expert knows how Knative Eventing components are implemented, configured and deployed, but is not an expert in all the foreign protocols that sources may implement. These two roles will often not be the same person. We want to confine the job of the \"contributor\" to implementing the Receive Adapter , and specifying what configuration their adapter needs to connect, subscribe, or do whatever it does. The Knative Eventing developer exposes the protocol configuration as part of the Source CRD , and the controller passes any required configuration (which may include resolved data like URLs) to the Receive Adapter . API Resources required: KubeClientSet.Appsv1.Deployment (Inherited via the Eventing base reconciler) Used to deploy the Receive Adapter for \"importing\" events EventingClientSet.EventingV1Alpha1 (Inherited via the Eventing base reconciler) Used to interact with Events within the Knative system SourceClientSet.SourcesV1Alpha1 Used for source \u2014 in this case, samplesource \u2014 specific config and translated to the underlying deployment (via the inherited KubeClientSet) To ease writing a new event source, the eventing subsystem has offloaded several core functionalities (via injection) to the eventing-sources-controller . Fig 1. - Via shared Knative Dependency Injection Specifically, the clientset , cache , informers , and listers can all be generated and shared. Thus, they can be generated, imported, and assigned to the underlying reconciler when creating a new controller source implementation: import ( // ... sampleSourceClient \"knative.dev/sample-source/pkg/client/injection/client\" samplesourceinformer \"knative.dev/sample-source/pkg/client/injection/informers/samples/v1alpha1/samplesource\" ) // ... func NewController(ctx context.Context, cmw configmap.Watcher) *controller.Impl { sampleSourceInformer := samplesourceinformer.Get(ctx) r := &Reconciler{ // ... samplesourceClientSet: sampleSourceClient.Get(ctx), samplesourceLister: sampleSourceInformer.Lister(), // ... } Sample source's update-codegen.sh have the configuration to have the required things above generated and injected: # Generation ${ CODEGEN_PKG } /generate-groups.sh \"deepcopy,client,informer,lister\" \\ knative.dev/sample-source/pkg/client knative.dev/sample-source/pkg/apis \\ \"samples:v1alpha1\" \\ --go-header-file ${ REPO_ROOT } /hack/boilerplate/boilerplate.go.txt # Injection ${ KNATIVE_CODEGEN_PKG } /hack/generate-knative.sh \"injection\" \\ knative.dev/sample-source/pkg/client knative.dev/sample-source/pkg/apis \\ \"samples:v1alpha1\" \\ --go-header-file ${ REPO_ROOT } /hack/boilerplate/boilerplate.go.txt File Layout & Hierarchy: cmd/controller/main.go - Pass source\u2019s NewController implementation to the shared main cmd/receive_adapter/main.go - Translate resource variables to underlying adapter struct (to eventually be passed into the serverless system) pkg/reconciler/sample/controller.go - NewController implementation to pass to sharedmain pkg/reconciler/sample/samplesource.go - reconciliation functions for the receive adapter pkg/apis/samples/VERSION/samplesource_types.go - schema for the underlying api types (variables to be defined in the resource yaml) pkg/apis/samples/VERSION/samplesource_lifecycle.go - status updates for the source\u2019s reconciliation details Source ready Sink provided Deployed Eventtype Provided K8s Resources Correct pkg/adapter/adapter.go - receive_adapter functions supporting translation of events to CloudEvents","title":"Design and Theory Behind an Event Source"},{"location":"eventing/sources/creating-event-sources/writing-event-source/01-theory/#topics","text":"What are the personas and critical paths? Contributor: implement a new source with minimal k8s overhead (don't have to learn controller/k8s internals) Operator: easily install Sources and verify that they are \"safe\" Developer: easily discover what Sources they can pull from on this cluster Developer: easily configure a Source based on existing knowledge of other Sources.","title":"Topics"},{"location":"eventing/sources/creating-event-sources/writing-event-source/01-theory/#separation-of-concerns","text":"","title":"Separation of concerns"},{"location":"eventing/sources/creating-event-sources/writing-event-source/01-theory/#contributor","text":"Receive Adapter (RA) - process that receives incoming events. Implement CloudEvent binding interfaces, cloudevent's go sdk provides libraries for standard access to configure interfaces as needed. Passing configuration from the Source CRD YAML, that the controller needs to configure the Receive Adapter","title":"Contributor:"},{"location":"eventing/sources/creating-event-sources/writing-event-source/01-theory/#source-library-provided-by-knative","text":"Controller runtime (this is what we share via injection) incorporates protocol specific config into \"generic controller\" CRD. Identifying specific event characteristics (i.e. value of interest, relevant metadata, etc) to pass along to the serverless system Propagating events internally to the system (i.e. cloudevents)","title":"Source library (provided by Knative):"},{"location":"eventing/sources/creating-event-sources/writing-event-source/01-theory/#theory","text":"Quick Introduction to Knative Eventing Sources A Knative Source is Kubernetes Custom Resource that generates or imports an event and pushes that event to another endpoint on the cluster via a CloudEvents . The specification for Knative Eventing Sources contains a number of requirements that together define a well-behaved Knative Source. To achieve this, there are several separations of concerns that we have to keep in mind: 1. A controller to run our Event Source and reconcile the underlying Receive Adapter deployments 2. A \"receive adapter\" which generates or imports the actual events 3. A series of identifying characteristics for our event 4. Transporting a valid event to the serverless system for further processing There are also two different classes of developer to consider: 1. A \"contributor\" knows about the foreign protocol but is not a Knative expert. 2. Knative Eventing expert knows how Knative Eventing components are implemented, configured and deployed, but is not an expert in all the foreign protocols that sources may implement. These two roles will often not be the same person. We want to confine the job of the \"contributor\" to implementing the Receive Adapter , and specifying what configuration their adapter needs to connect, subscribe, or do whatever it does. The Knative Eventing developer exposes the protocol configuration as part of the Source CRD , and the controller passes any required configuration (which may include resolved data like URLs) to the Receive Adapter . API Resources required: KubeClientSet.Appsv1.Deployment (Inherited via the Eventing base reconciler) Used to deploy the Receive Adapter for \"importing\" events EventingClientSet.EventingV1Alpha1 (Inherited via the Eventing base reconciler) Used to interact with Events within the Knative system SourceClientSet.SourcesV1Alpha1 Used for source \u2014 in this case, samplesource \u2014 specific config and translated to the underlying deployment (via the inherited KubeClientSet) To ease writing a new event source, the eventing subsystem has offloaded several core functionalities (via injection) to the eventing-sources-controller . Fig 1. - Via shared Knative Dependency Injection Specifically, the clientset , cache , informers , and listers can all be generated and shared. Thus, they can be generated, imported, and assigned to the underlying reconciler when creating a new controller source implementation: import ( // ... sampleSourceClient \"knative.dev/sample-source/pkg/client/injection/client\" samplesourceinformer \"knative.dev/sample-source/pkg/client/injection/informers/samples/v1alpha1/samplesource\" ) // ... func NewController(ctx context.Context, cmw configmap.Watcher) *controller.Impl { sampleSourceInformer := samplesourceinformer.Get(ctx) r := &Reconciler{ // ... samplesourceClientSet: sampleSourceClient.Get(ctx), samplesourceLister: sampleSourceInformer.Lister(), // ... } Sample source's update-codegen.sh have the configuration to have the required things above generated and injected: # Generation ${ CODEGEN_PKG } /generate-groups.sh \"deepcopy,client,informer,lister\" \\ knative.dev/sample-source/pkg/client knative.dev/sample-source/pkg/apis \\ \"samples:v1alpha1\" \\ --go-header-file ${ REPO_ROOT } /hack/boilerplate/boilerplate.go.txt # Injection ${ KNATIVE_CODEGEN_PKG } /hack/generate-knative.sh \"injection\" \\ knative.dev/sample-source/pkg/client knative.dev/sample-source/pkg/apis \\ \"samples:v1alpha1\" \\ --go-header-file ${ REPO_ROOT } /hack/boilerplate/boilerplate.go.txt File Layout & Hierarchy: cmd/controller/main.go - Pass source\u2019s NewController implementation to the shared main cmd/receive_adapter/main.go - Translate resource variables to underlying adapter struct (to eventually be passed into the serverless system) pkg/reconciler/sample/controller.go - NewController implementation to pass to sharedmain pkg/reconciler/sample/samplesource.go - reconciliation functions for the receive adapter pkg/apis/samples/VERSION/samplesource_types.go - schema for the underlying api types (variables to be defined in the resource yaml) pkg/apis/samples/VERSION/samplesource_lifecycle.go - status updates for the source\u2019s reconciliation details Source ready Sink provided Deployed Eventtype Provided K8s Resources Correct pkg/adapter/adapter.go - receive_adapter functions supporting translation of events to CloudEvents","title":"Theory"},{"location":"eventing/sources/creating-event-sources/writing-event-source/02-lifecycle-and-types/","text":"Sample Source Lifecycle and Types \u00b6 API Definition \u00b6 Define the types required in the resource\u2019s schema in pkg/apis/samples/v1alpha1/samplesource_types.go This includes the fields that will be required in the resource yaml as well as what will be referenced in the controller using the source\u2019s clientset and API // +genclient // +genreconciler // +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object // +k8s:openapi-gen=true type SampleSource struct { metav1 . TypeMeta `json:\",inline\"` // +optional metav1 . ObjectMeta `json:\"metadata,omitempty\"` // Spec holds the desired state of the SampleSource (from the client). Spec SampleSourceSpec `json:\"spec\"` // Status communicates the observed state of the SampleSource (from the controller). // +optional Status SampleSourceStatus `json:\"status,omitempty\"` } // SampleSourceSpec holds the desired state of the SampleSource (from the client). type SampleSourceSpec struct { // inherits duck/v1 SourceSpec, which currently provides: // * Sink - a reference to an object that will resolve to a domain name or // a URI directly to use as the sink. // * CloudEventOverrides - defines overrides to control the output format // and modifications of the event sent to the sink. duckv1 . SourceSpec `json:\",inline\"` // ServiceAccountName holds the name of the Kubernetes service account // as which the underlying K8s resources should be run. If unspecified // this will default to the \"default\" service account for the namespace // in which the SampleSource exists. // +optional ServiceAccountName string `json:\"serviceAccountName,omitempty\"` // Interval is the time interval between events. // // The string format is a sequence of decimal numbers, each with optional // fraction and a unit suffix, such as \"300ms\", \"-1.5h\" or \"2h45m\". Valid time // units are \"ns\", \"us\" (or \"\u00b5s\"), \"ms\", \"s\", \"m\", \"h\". If unspecified // this will default to \"10s\". Interval string `json:\"interval\"` } // SampleSourceStatus communicates the observed state of the SampleSource (from the controller). type SampleSourceStatus struct { duckv1 . Status `json:\",inline\"` // SinkURI is the current active sink URI that has been configured // for the SampleSource. // +optional SinkURI * apis . URL `json:\"sinkUri,omitempty\"` } Define the lifecycle that will be reflected in the status and SinkURI fields const ( // SampleConditionReady has status True when the SampleSource is ready to send events. SampleConditionReady = apis . ConditionReady // ... ) Define the functions that will be called from the Reconciler functions to set the lifecycle conditions. This is typically done in pkg/apis/samples/VERSION/samplesource_lifecycle.go // InitializeConditions sets relevant unset conditions to Unknown state. func ( s * SampleSourceStatus ) InitializeConditions () { SampleCondSet . Manage ( s ). InitializeConditions () } ... // MarkSink sets the condition that the source has a sink configured. func ( s * SampleSourceStatus ) MarkSink ( uri * apis . URL ) { s . SinkURI = uri if len ( uri . String ()) > 0 { SampleCondSet . Manage ( s ). MarkTrue ( SampleConditionSinkProvided ) } else { SampleCondSet . Manage ( s ). MarkUnknown ( SampleConditionSinkProvided , \"SinkEmpty\" , \"Sink has resolved to empty.%s\" , \"\" ) } } // MarkNoSink sets the condition that the source does not have a sink configured. func ( s * SampleSourceStatus ) MarkNoSink ( reason , messageFormat string , messageA ... interface {}) { SampleCondSet . Manage ( s ). MarkFalse ( SampleConditionSinkProvided , reason , messageFormat , messageA ... ) }","title":"Sample Source Lifecycle and Types"},{"location":"eventing/sources/creating-event-sources/writing-event-source/02-lifecycle-and-types/#sample-source-lifecycle-and-types","text":"","title":"Sample Source Lifecycle and Types"},{"location":"eventing/sources/creating-event-sources/writing-event-source/02-lifecycle-and-types/#api-definition","text":"Define the types required in the resource\u2019s schema in pkg/apis/samples/v1alpha1/samplesource_types.go This includes the fields that will be required in the resource yaml as well as what will be referenced in the controller using the source\u2019s clientset and API // +genclient // +genreconciler // +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object // +k8s:openapi-gen=true type SampleSource struct { metav1 . TypeMeta `json:\",inline\"` // +optional metav1 . ObjectMeta `json:\"metadata,omitempty\"` // Spec holds the desired state of the SampleSource (from the client). Spec SampleSourceSpec `json:\"spec\"` // Status communicates the observed state of the SampleSource (from the controller). // +optional Status SampleSourceStatus `json:\"status,omitempty\"` } // SampleSourceSpec holds the desired state of the SampleSource (from the client). type SampleSourceSpec struct { // inherits duck/v1 SourceSpec, which currently provides: // * Sink - a reference to an object that will resolve to a domain name or // a URI directly to use as the sink. // * CloudEventOverrides - defines overrides to control the output format // and modifications of the event sent to the sink. duckv1 . SourceSpec `json:\",inline\"` // ServiceAccountName holds the name of the Kubernetes service account // as which the underlying K8s resources should be run. If unspecified // this will default to the \"default\" service account for the namespace // in which the SampleSource exists. // +optional ServiceAccountName string `json:\"serviceAccountName,omitempty\"` // Interval is the time interval between events. // // The string format is a sequence of decimal numbers, each with optional // fraction and a unit suffix, such as \"300ms\", \"-1.5h\" or \"2h45m\". Valid time // units are \"ns\", \"us\" (or \"\u00b5s\"), \"ms\", \"s\", \"m\", \"h\". If unspecified // this will default to \"10s\". Interval string `json:\"interval\"` } // SampleSourceStatus communicates the observed state of the SampleSource (from the controller). type SampleSourceStatus struct { duckv1 . Status `json:\",inline\"` // SinkURI is the current active sink URI that has been configured // for the SampleSource. // +optional SinkURI * apis . URL `json:\"sinkUri,omitempty\"` } Define the lifecycle that will be reflected in the status and SinkURI fields const ( // SampleConditionReady has status True when the SampleSource is ready to send events. SampleConditionReady = apis . ConditionReady // ... ) Define the functions that will be called from the Reconciler functions to set the lifecycle conditions. This is typically done in pkg/apis/samples/VERSION/samplesource_lifecycle.go // InitializeConditions sets relevant unset conditions to Unknown state. func ( s * SampleSourceStatus ) InitializeConditions () { SampleCondSet . Manage ( s ). InitializeConditions () } ... // MarkSink sets the condition that the source has a sink configured. func ( s * SampleSourceStatus ) MarkSink ( uri * apis . URL ) { s . SinkURI = uri if len ( uri . String ()) > 0 { SampleCondSet . Manage ( s ). MarkTrue ( SampleConditionSinkProvided ) } else { SampleCondSet . Manage ( s ). MarkUnknown ( SampleConditionSinkProvided , \"SinkEmpty\" , \"Sink has resolved to empty.%s\" , \"\" ) } } // MarkNoSink sets the condition that the source does not have a sink configured. func ( s * SampleSourceStatus ) MarkNoSink ( reason , messageFormat string , messageA ... interface {}) { SampleCondSet . Manage ( s ). MarkFalse ( SampleConditionSinkProvided , reason , messageFormat , messageA ... ) }","title":"API Definition"},{"location":"eventing/sources/creating-event-sources/writing-event-source/03-controller/","text":"Controller Implementation \u00b6 cmd/controller \u00b6 Pass the new controller implementation to the shared main import ( // The set of controllers this controller process runs. \"knative.dev/sample-source/pkg/reconciler/sample\" // This defines the shared main for injected controllers. \"knative.dev/pkg/injection/sharedmain\" ) func main () { sharedmain . Main ( \"sample-source-controller\" , sample . NewController ) } Define the NewController implementation, it will be passed a configmap.Watcher , as well as a context which the injected listers will use for the reconciler struct arguments func NewController ( ctx context . Context , cmw configmap . Watcher , ) * controller . Impl { // ... deploymentInformer := deploymentinformer . Get ( ctx ) sinkBindingInformer := sinkbindinginformer . Get ( ctx ) sampleSourceInformer := samplesourceinformer . Get ( ctx ) r := & Reconciler { dr : & reconciler . DeploymentReconciler { KubeClientSet : kubeclient . Get ( ctx )}, sbr : & reconciler . SinkBindingReconciler { EventingClientSet : eventingclient . Get ( ctx )}, // Config accessor takes care of tracing/config/logging config propagation to the receive adapter configAccessor : reconcilersource . WatchConfigurations ( ctx , \"sample-source\" , cmw ), } The base reconciler is imported from the knative.dev/pkg dependency: import ( // ... reconcilersource \"knative.dev/eventing/pkg/reconciler/source\" // ... ) Ensure the correct informers have EventHandlers filtered to them sampleSourceInformer . Informer (). AddEventHandler ( controller . HandleAll ( impl . Enqueue )) Controller for the SampleSource uses Deployment and SinkBinding resources to deploy and also bind the event source and the receive adapter. Also ensure the informers are set up correctly for these secondary resources deploymentInformer . Informer (). AddEventHandler ( cache . FilteringResourceEventHandler { FilterFunc : controller . FilterGroupKind ( v1alpha1 . Kind ( \"SampleSource\" )), Handler : controller . HandleAll ( impl . EnqueueControllerOf ), }) sinkBindingInformer . Informer (). AddEventHandler ( cache . FilteringResourceEventHandler { FilterFunc : controller . FilterGroupKind ( v1alpha1 . Kind ( \"SampleSource\" )), Handler : controller . HandleAll ( impl . EnqueueControllerOf ), })","title":"Controller Implementation and Design"},{"location":"eventing/sources/creating-event-sources/writing-event-source/03-controller/#controller-implementation","text":"","title":"Controller Implementation"},{"location":"eventing/sources/creating-event-sources/writing-event-source/03-controller/#cmdcontroller","text":"Pass the new controller implementation to the shared main import ( // The set of controllers this controller process runs. \"knative.dev/sample-source/pkg/reconciler/sample\" // This defines the shared main for injected controllers. \"knative.dev/pkg/injection/sharedmain\" ) func main () { sharedmain . Main ( \"sample-source-controller\" , sample . NewController ) } Define the NewController implementation, it will be passed a configmap.Watcher , as well as a context which the injected listers will use for the reconciler struct arguments func NewController ( ctx context . Context , cmw configmap . Watcher , ) * controller . Impl { // ... deploymentInformer := deploymentinformer . Get ( ctx ) sinkBindingInformer := sinkbindinginformer . Get ( ctx ) sampleSourceInformer := samplesourceinformer . Get ( ctx ) r := & Reconciler { dr : & reconciler . DeploymentReconciler { KubeClientSet : kubeclient . Get ( ctx )}, sbr : & reconciler . SinkBindingReconciler { EventingClientSet : eventingclient . Get ( ctx )}, // Config accessor takes care of tracing/config/logging config propagation to the receive adapter configAccessor : reconcilersource . WatchConfigurations ( ctx , \"sample-source\" , cmw ), } The base reconciler is imported from the knative.dev/pkg dependency: import ( // ... reconcilersource \"knative.dev/eventing/pkg/reconciler/source\" // ... ) Ensure the correct informers have EventHandlers filtered to them sampleSourceInformer . Informer (). AddEventHandler ( controller . HandleAll ( impl . Enqueue )) Controller for the SampleSource uses Deployment and SinkBinding resources to deploy and also bind the event source and the receive adapter. Also ensure the informers are set up correctly for these secondary resources deploymentInformer . Informer (). AddEventHandler ( cache . FilteringResourceEventHandler { FilterFunc : controller . FilterGroupKind ( v1alpha1 . Kind ( \"SampleSource\" )), Handler : controller . HandleAll ( impl . EnqueueControllerOf ), }) sinkBindingInformer . Informer (). AddEventHandler ( cache . FilteringResourceEventHandler { FilterFunc : controller . FilterGroupKind ( v1alpha1 . Kind ( \"SampleSource\" )), Handler : controller . HandleAll ( impl . EnqueueControllerOf ), })","title":"cmd/controller"},{"location":"eventing/sources/creating-event-sources/writing-event-source/04-reconciler/","text":"Reconciler Implementation and Design \u00b6 Reconciler Functionality \u00b6 General steps the reconciliation process needs to cover: 1. Update the ObservedGeneration and initialize the Status conditions (as defined in samplesource_lifecycle.go and samplesource_types.go ) src . Status . InitializeConditions () src . Status . ObservedGeneration = src . Generation 2. Create/reconcile the Receive Adapter (detailed below) 3. If successful, update the Status and MarkDeployed src . Status . PropagateDeploymentAvailability ( ra ) 4. Create/reconcile the SinkBinding for the Receive Adapter targeting the Sink (detailed below) 5. MarkSink with the result src . Status . MarkSink ( sb . Status . SinkURI ) 6. Return a new reconciler event stating that the process is done return pkgreconciler . NewEvent ( corev1 . EventTypeNormal , \"SampleSourceReconciled\" , \"SampleSource reconciled: \\\"%s/%s\\\"\" , namespace , name ) Reconcile/Create The Receive Adapter \u00b6 As part of the source reconciliation, we have to create and deploy (and update if necessary) the underlying receive adapter. Verify the specified kubernetes resources are valid, and update the Status accordingly Assemble the ReceiveAdapterArgs raArgs := resources . ReceiveAdapterArgs { EventSource : src . Namespace + \"/\" + src . Name , Image : r . ReceiveAdapterImage , Source : src , Labels : resources . Labels ( src . Name ), AdditionalEnvs : r . configAccessor . ToEnvVars (), // Grab config envs for tracing/logging/metrics } NB The exact arguments may change based on functional requirements Create the underlying deployment from the arguments provided, matching pod templates, labels, owner references, etc as needed to fill out the deployment Example: pkg/reconciler/sample/resources/receive_adapter.go Fetch the existing receive adapter deployment namespace := owner . GetObjectMeta (). GetNamespace () ra , err := r . KubeClientSet . AppsV1 (). Deployments ( namespace ). Get ( expected . Name , metav1 . GetOptions {}) Otherwise, create the deployment ra , err = r . KubeClientSet . AppsV1 (). Deployments ( namespace ). Create ( expected ) Check if the expected vs existing spec is different, and update the deployment if required } else if r . podSpecImageSync ( expected . Spec . Template . Spec , ra . Spec . Template . Spec ) { ra . Spec . Template . Spec = expected . Spec . Template . Spec if ra , err = r . KubeClientSet . AppsV1 (). Deployments ( namespace ). Update ( ra ); err != nil { return ra , err } If updated, record the event return pkgreconciler . NewEvent ( corev1 . EventTypeNormal , \"DeploymentUpdated\" , \"updated deployment: \\\"%s/%s\\\"\" , namespace , name ) Reconcile/Create The SinkBinding \u00b6 Instead of directly giving the details of the sink to the receive adapter, use a SinkBinding to bind the receive adapter with the sink. Steps here are almost the same with the Deployment reconciliation above, but it is for another resource, SinkBinding . Create a Reference for the receive adapter deployment. This deployment will be SinkBinding 's source: tracker . Reference { APIVersion : appsv1 . SchemeGroupVersion . String (), Kind : \"Deployment\" , Namespace : ra . Namespace , Name : ra . Name , } Fetch the existing SinkBinding namespace := owner . GetObjectMeta (). GetNamespace () sb , err := r . EventingClientSet . SourcesV1alpha2 (). SinkBindings ( namespace ). Get ( expected . Name , metav1 . GetOptions {}) If it doesn't exist, create it sb , err = r . EventingClientSet . SourcesV1alpha2 (). SinkBindings ( namespace ). Create ( expected ) Check if the expected vs existing spec is different, and update the SinkBinding if required else if r . specChanged ( sb . Spec , expected . Spec ) { sb . Spec = expected . Spec if sb , err = r . EventingClientSet . SourcesV1alpha2 (). SinkBindings ( namespace ). Update ( sb ); err != nil { return sb , err } If updated, record the event return pkgreconciler . NewEvent ( corev1 . EventTypeNormal , \"SinkBindingUpdated\" , \"updated SinkBinding: \\\"%s/%s\\\"\" , namespace , name )","title":"Reconciler Implementation and Design"},{"location":"eventing/sources/creating-event-sources/writing-event-source/04-reconciler/#reconciler-implementation-and-design","text":"","title":"Reconciler Implementation and Design"},{"location":"eventing/sources/creating-event-sources/writing-event-source/04-reconciler/#reconciler-functionality","text":"General steps the reconciliation process needs to cover: 1. Update the ObservedGeneration and initialize the Status conditions (as defined in samplesource_lifecycle.go and samplesource_types.go ) src . Status . InitializeConditions () src . Status . ObservedGeneration = src . Generation 2. Create/reconcile the Receive Adapter (detailed below) 3. If successful, update the Status and MarkDeployed src . Status . PropagateDeploymentAvailability ( ra ) 4. Create/reconcile the SinkBinding for the Receive Adapter targeting the Sink (detailed below) 5. MarkSink with the result src . Status . MarkSink ( sb . Status . SinkURI ) 6. Return a new reconciler event stating that the process is done return pkgreconciler . NewEvent ( corev1 . EventTypeNormal , \"SampleSourceReconciled\" , \"SampleSource reconciled: \\\"%s/%s\\\"\" , namespace , name )","title":"Reconciler Functionality"},{"location":"eventing/sources/creating-event-sources/writing-event-source/04-reconciler/#reconcilecreate-the-receive-adapter","text":"As part of the source reconciliation, we have to create and deploy (and update if necessary) the underlying receive adapter. Verify the specified kubernetes resources are valid, and update the Status accordingly Assemble the ReceiveAdapterArgs raArgs := resources . ReceiveAdapterArgs { EventSource : src . Namespace + \"/\" + src . Name , Image : r . ReceiveAdapterImage , Source : src , Labels : resources . Labels ( src . Name ), AdditionalEnvs : r . configAccessor . ToEnvVars (), // Grab config envs for tracing/logging/metrics } NB The exact arguments may change based on functional requirements Create the underlying deployment from the arguments provided, matching pod templates, labels, owner references, etc as needed to fill out the deployment Example: pkg/reconciler/sample/resources/receive_adapter.go Fetch the existing receive adapter deployment namespace := owner . GetObjectMeta (). GetNamespace () ra , err := r . KubeClientSet . AppsV1 (). Deployments ( namespace ). Get ( expected . Name , metav1 . GetOptions {}) Otherwise, create the deployment ra , err = r . KubeClientSet . AppsV1 (). Deployments ( namespace ). Create ( expected ) Check if the expected vs existing spec is different, and update the deployment if required } else if r . podSpecImageSync ( expected . Spec . Template . Spec , ra . Spec . Template . Spec ) { ra . Spec . Template . Spec = expected . Spec . Template . Spec if ra , err = r . KubeClientSet . AppsV1 (). Deployments ( namespace ). Update ( ra ); err != nil { return ra , err } If updated, record the event return pkgreconciler . NewEvent ( corev1 . EventTypeNormal , \"DeploymentUpdated\" , \"updated deployment: \\\"%s/%s\\\"\" , namespace , name )","title":"Reconcile/Create The Receive Adapter"},{"location":"eventing/sources/creating-event-sources/writing-event-source/04-reconciler/#reconcilecreate-the-sinkbinding","text":"Instead of directly giving the details of the sink to the receive adapter, use a SinkBinding to bind the receive adapter with the sink. Steps here are almost the same with the Deployment reconciliation above, but it is for another resource, SinkBinding . Create a Reference for the receive adapter deployment. This deployment will be SinkBinding 's source: tracker . Reference { APIVersion : appsv1 . SchemeGroupVersion . String (), Kind : \"Deployment\" , Namespace : ra . Namespace , Name : ra . Name , } Fetch the existing SinkBinding namespace := owner . GetObjectMeta (). GetNamespace () sb , err := r . EventingClientSet . SourcesV1alpha2 (). SinkBindings ( namespace ). Get ( expected . Name , metav1 . GetOptions {}) If it doesn't exist, create it sb , err = r . EventingClientSet . SourcesV1alpha2 (). SinkBindings ( namespace ). Create ( expected ) Check if the expected vs existing spec is different, and update the SinkBinding if required else if r . specChanged ( sb . Spec , expected . Spec ) { sb . Spec = expected . Spec if sb , err = r . EventingClientSet . SourcesV1alpha2 (). SinkBindings ( namespace ). Update ( sb ); err != nil { return sb , err } If updated, record the event return pkgreconciler . NewEvent ( corev1 . EventTypeNormal , \"SinkBindingUpdated\" , \"updated SinkBinding: \\\"%s/%s\\\"\" , namespace , name )","title":"Reconcile/Create The SinkBinding"},{"location":"eventing/sources/creating-event-sources/writing-event-source/05-receive-adapter/","text":"Receive Adapter Implementation and Design \u00b6 Receive Adapter cmd \u00b6 Similar to the controller, we'll need an injection based main.go similar to the controller under cmd/receiver_adapter/main.go // This Adapter generates events at a regular interval. package main import ( \"knative.dev/eventing/pkg/adapter\" myadapter \"knative.dev/sample-source/pkg/adapter\" ) func main () { adapter . Main ( \"sample-source\" , myadapter . NewEnv , myadapter . NewAdapter ) } Defining NewAdapter implementation and Start function \u00b6 The adapter's pkg implementation consists of two main functions; A NewAdapter(ctx context.Context, aEnv adapter.EnvConfigAccessor, ceClient cloudevents.Client) adapter.Adapter {} call, which creates the new adapter with passed variables via the EnvConfigAccessor . The created adapter will be passed the cloudevents client (which is where the events are forwarded to). This is sometimes referred to as a sink, or ceClient in the Knative ecosystem. The return value is a reference to the adapter as defined by the adapter's local struct. In our sample-source 's case; // Adapter generates events at a regular interval. type Adapter struct { logger * zap . Logger interval time . Duration nextID int client cloudevents . Client } Start function implemented as an interface to the adapter struct. func ( a * Adapter ) Start ( stopCh <- chan struct {}) error { stopCh is the signal to stop the Adapter. Otherwise the role of the function is to process the next event. In the case of the sample-source , it creates an event to forward to the specified cloudevent sink/client every X interval, as specified by the loaded EnvConfigAccessor (loaded via the resource yaml). func ( a * Adapter ) Start ( stopCh <- chan struct {}) error { a . logger . Infow ( \"Starting heartbeat\" , zap . String ( \"interval\" , a . interval . String ())) for { select { case <- time . After ( a . interval ): event := a . newEvent () a . logger . Infow ( \"Sending new event\" , zap . String ( \"event\" , event . String ())) if result := a . client . Send ( context . Background (), event ); ! cloudevents . IsACK ( result ) { a . logger . Infow ( \"failed to send event\" , zap . String ( \"event\" , event . String ()), zap . Error ( result )) // We got an error but it could be transient, try again next interval. continue } case <- stopCh : a . logger . Info ( \"Shutting down...\" ) return nil } } }","title":"Receive Adapter Implementation and Design"},{"location":"eventing/sources/creating-event-sources/writing-event-source/05-receive-adapter/#receive-adapter-implementation-and-design","text":"","title":"Receive Adapter Implementation and Design"},{"location":"eventing/sources/creating-event-sources/writing-event-source/05-receive-adapter/#receive-adapter-cmd","text":"Similar to the controller, we'll need an injection based main.go similar to the controller under cmd/receiver_adapter/main.go // This Adapter generates events at a regular interval. package main import ( \"knative.dev/eventing/pkg/adapter\" myadapter \"knative.dev/sample-source/pkg/adapter\" ) func main () { adapter . Main ( \"sample-source\" , myadapter . NewEnv , myadapter . NewAdapter ) }","title":"Receive Adapter cmd"},{"location":"eventing/sources/creating-event-sources/writing-event-source/05-receive-adapter/#defining-newadapter-implementation-and-start-function","text":"The adapter's pkg implementation consists of two main functions; A NewAdapter(ctx context.Context, aEnv adapter.EnvConfigAccessor, ceClient cloudevents.Client) adapter.Adapter {} call, which creates the new adapter with passed variables via the EnvConfigAccessor . The created adapter will be passed the cloudevents client (which is where the events are forwarded to). This is sometimes referred to as a sink, or ceClient in the Knative ecosystem. The return value is a reference to the adapter as defined by the adapter's local struct. In our sample-source 's case; // Adapter generates events at a regular interval. type Adapter struct { logger * zap . Logger interval time . Duration nextID int client cloudevents . Client } Start function implemented as an interface to the adapter struct. func ( a * Adapter ) Start ( stopCh <- chan struct {}) error { stopCh is the signal to stop the Adapter. Otherwise the role of the function is to process the next event. In the case of the sample-source , it creates an event to forward to the specified cloudevent sink/client every X interval, as specified by the loaded EnvConfigAccessor (loaded via the resource yaml). func ( a * Adapter ) Start ( stopCh <- chan struct {}) error { a . logger . Infow ( \"Starting heartbeat\" , zap . String ( \"interval\" , a . interval . String ())) for { select { case <- time . After ( a . interval ): event := a . newEvent () a . logger . Infow ( \"Sending new event\" , zap . String ( \"event\" , event . String ())) if result := a . client . Send ( context . Background (), event ); ! cloudevents . IsACK ( result ) { a . logger . Infow ( \"failed to send event\" , zap . String ( \"event\" , event . String ()), zap . Error ( result )) // We got an error but it could be transient, try again next interval. continue } case <- stopCh : a . logger . Info ( \"Shutting down...\" ) return nil } } }","title":"Defining NewAdapter implementation and Start function"},{"location":"eventing/sources/creating-event-sources/writing-event-source/06-yaml/","text":"Publishing to your Kubernetes cluster \u00b6 Run the sample source locally \u00b6 Start a minikube cluster. If you already have a Kubernetes cluster running, you can skip this step. The cluster must be 1.15+ minikube start Setup ko to use the minikube docker instance and local registry eval $( minikube docker-env ) export KO_DOCKER_REPO = ko.local Apply the CRD and configuration yaml ko apply -f config Once the sample-source-controller-manager is running in the knative-samples namespace, you can apply the example.yaml to connect our sample-source every 10s directly to a ksvc . apiVersion : serving.knative.dev/v1 kind : Service metadata : name : event-display namespace : knative-samples spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing-contrib/cmd/event_display --- apiVersion : samples.knative.dev/v1alpha1 kind : SampleSource metadata : name : sample-source namespace : knative-samples spec : interval : \"10s\" sink : ref : apiVersion : serving.knative.dev/v1 kind : Service name : event-display ko apply -f example.yaml Once reconciled, you can confirm the ksvc is outputting valid cloudevents every 10s to align with our specified interval. % kubectl -n knative-samples logs -l serving.knative.dev/service = event-display -c user-container -f \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1.0 type: dev.knative.sample source: http://sample.knative.dev/heartbeat-source id: d4619592-363e-4a41-82d1-b1586c390e24 time: 2019-12-17T01:31:10.795588888Z datacontenttype: application/json Data, { \"Sequence\": 0, \"Heartbeat\": \"10s\" } \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1.0 type: dev.knative.sample source: http://sample.knative.dev/heartbeat-source id: db2edad0-06bc-4234-b9e1-7ea3955841d6 time: 2019-12-17T01:31:20.825969504Z datacontenttype: application/json Data, { \"Sequence\": 1, \"Heartbeat\": \"10s\" }","title":"Publishing to your Kubernetes cluster"},{"location":"eventing/sources/creating-event-sources/writing-event-source/06-yaml/#publishing-to-your-kubernetes-cluster","text":"","title":"Publishing to your Kubernetes cluster"},{"location":"eventing/sources/creating-event-sources/writing-event-source/06-yaml/#run-the-sample-source-locally","text":"Start a minikube cluster. If you already have a Kubernetes cluster running, you can skip this step. The cluster must be 1.15+ minikube start Setup ko to use the minikube docker instance and local registry eval $( minikube docker-env ) export KO_DOCKER_REPO = ko.local Apply the CRD and configuration yaml ko apply -f config Once the sample-source-controller-manager is running in the knative-samples namespace, you can apply the example.yaml to connect our sample-source every 10s directly to a ksvc . apiVersion : serving.knative.dev/v1 kind : Service metadata : name : event-display namespace : knative-samples spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing-contrib/cmd/event_display --- apiVersion : samples.knative.dev/v1alpha1 kind : SampleSource metadata : name : sample-source namespace : knative-samples spec : interval : \"10s\" sink : ref : apiVersion : serving.knative.dev/v1 kind : Service name : event-display ko apply -f example.yaml Once reconciled, you can confirm the ksvc is outputting valid cloudevents every 10s to align with our specified interval. % kubectl -n knative-samples logs -l serving.knative.dev/service = event-display -c user-container -f \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1.0 type: dev.knative.sample source: http://sample.knative.dev/heartbeat-source id: d4619592-363e-4a41-82d1-b1586c390e24 time: 2019-12-17T01:31:10.795588888Z datacontenttype: application/json Data, { \"Sequence\": 0, \"Heartbeat\": \"10s\" } \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1.0 type: dev.knative.sample source: http://sample.knative.dev/heartbeat-source id: db2edad0-06bc-4234-b9e1-7ea3955841d6 time: 2019-12-17T01:31:20.825969504Z datacontenttype: application/json Data, { \"Sequence\": 1, \"Heartbeat\": \"10s\" }","title":"Run the sample source locally"},{"location":"eventing/sources/creating-event-sources/writing-event-source/07-knative-sandbox/","text":"Moving the event source to knative-sandbox \u00b6 If you would like to move your source over to the knative-sandbox organization follow the instructions to create a sandbox repository .","title":"Moving the event source to knative-sandbox"},{"location":"eventing/sources/creating-event-sources/writing-event-source/07-knative-sandbox/#moving-the-event-source-to-knative-sandbox","text":"If you would like to move your source over to the knative-sandbox organization follow the instructions to create a sandbox repository .","title":"Moving the event source to knative-sandbox"},{"location":"eventing/sources/creating-event-sources/writing-event-source/_index/","text":"Creating an event source by using the sample event source \u00b6 This guide explains how to create your own event source for Knative Eventing by using a sample repository , and explains the key concepts behind each required component. Documentation for the default Knative event sources can be used as an additional reference. After completing the provided tutorial, you will have created a basic event source controller and a receive adapter. Events can be viewed by using the event_display Knative service. Prerequisites \u00b6 You are familiar with Kubernetes and Go development. You have installed Git. You have installed Go. Clone the sample source . Optional: Install the ko CLI tool. Install the kubectl CLI tool. Set up minikube . Steps \u00b6 Separation of Concerns API Definition Controller Reconciler Receive Adapter Example YAML Moving the event source to the knative-sandbox organization","title":"Creating an event source by using the sample event source"},{"location":"eventing/sources/creating-event-sources/writing-event-source/_index/#creating-an-event-source-by-using-the-sample-event-source","text":"This guide explains how to create your own event source for Knative Eventing by using a sample repository , and explains the key concepts behind each required component. Documentation for the default Knative event sources can be used as an additional reference. After completing the provided tutorial, you will have created a basic event source controller and a receive adapter. Events can be viewed by using the event_display Knative service.","title":"Creating an event source by using the sample event source"},{"location":"eventing/sources/creating-event-sources/writing-event-source/_index/#prerequisites","text":"You are familiar with Kubernetes and Go development. You have installed Git. You have installed Go. Clone the sample source . Optional: Install the ko CLI tool. Install the kubectl CLI tool. Set up minikube .","title":"Prerequisites"},{"location":"eventing/sources/creating-event-sources/writing-event-source/_index/#steps","text":"Separation of Concerns API Definition Controller Reconciler Receive Adapter Example YAML Moving the event source to the knative-sandbox organization","title":"Steps"},{"location":"eventing/sources/creating-event-sources/writing-event-source-easy-way/_index/","text":"Writing an event source using Javascript \u00b6 This tutorial provides instructions to build an event source in Javascript and implement it with a ContainerSource or SinkBinding. Using a ContainerSource is a simple way to turn any dispatcher container into a Knative event source. Using SinkBinding provides a framework for injecting environment variables into any Kubernetes resource that has a spec.template and is PodSpecable . ContainerSource and SinkBinding both work by injecting environment variables to an application. Injected environment variables at minimum contain the URL of a sink that will receive events. Bootstrapping \u00b6 Create the project and add the dependencies: npm init npm install cloudevents-sdk@2.0.1 --save NOTE: Due to this bug , you must use version 2.0.1 of the Javascript SDK or newer. Using ContainerSource \u00b6 A ContainerSource creates a container for your event source image and manages this container. The sink URL to post the events will be made available to the application through the K_SINK environment variable by the ContainerSource. Example \u00b6 The following example event source emits an event to the sink every 1000 milliseconds: // File - index.js const { CloudEvent , HTTPEmitter } = require ( \"cloudevents-sdk\" ); let sinkUrl = process . env [ 'K_SINK' ]; console . log ( \"Sink URL is \" + sinkUrl ); let emitter = new HTTPEmitter ({ url : sinkUrl }); let eventIndex = 0 ; setInterval ( function () { console . log ( \"Emitting event #\" + ++ eventIndex ); let myevent = new CloudEvent ({ source : \"urn:event:from:my-api/resource/123\" , type : \"your.event.source.type\" , id : \"your-event-id\" , dataContentType : \"application/json\" , data : { \"hello\" : \"World \" + eventIndex }, }); // Emit the event emitter . send ( myevent ) . then ( response => { // Treat the response console . log ( \"Event posted successfully\" ); console . log ( response . data ); }) . catch ( err => { // Deal with errors console . log ( \"Error during event post\" ); console . error ( err ); }); }, 1000 ); # File - Dockerfile FROM node:10 WORKDIR /usr/src/app COPY package*.json ./ RUN npm install COPY . . EXPOSE 8080 CMD [ \"node\" , \"index.js\" ] The example code uses Binary mode for CloudEvents. To employ structured code, change let binding = new v1.BinaryHTTPEmitter(config); to let binding = new v1.StructuredHTTPEmitter(config); . Binary mode is used in most cases because: - It is faster in terms of serialization and deserialization. - It works better with CloudEvent-aware proxies, such as Knative Channels, and can simply check the header instead of parsing the payload. Procedure \u00b6 Build and push the image: docker build . -t path/to/image/registry/node-knative-heartbeat-source:v1 docker push path/to/image/registry/node-knative-heartbeat-source:v1 Create the event display service which logs any CloudEvents posted to it: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : event-display spec : template : spec : containers : - image : docker.io/aliok/event_display-864884f202126ec3150c5fcef437d90c@sha256:93cb4dcda8fee80a1f68662ae6bf20301471b046ede628f3c3f94f39752fbe08 Create the ContainerSource object: apiVersion : sources.knative.dev/v1 kind : ContainerSource metadata : name : test-heartbeats spec : template : spec : containers : - image : path/to/image/registry/node-knative-heartbeat-source:v1 name : heartbeats sink : ref : apiVersion : serving.knative.dev/v1 kind : Service name : event-display Check the logs of the event display service. You will see a new message is pushed every second: $ kubectl logs -l serving.knative.dev/service = event-display -c user-container \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1 .0 type: your.event.source.type source: urn:event:from:your-api/resource/123 id: your-event-id datacontenttype: application/json Data, { \"hello\" : \"World 1\" } Optional: If you are interested in seeing what is injected into the event source as a K_SINK , you can check the logs: $ kubectl logs test-heartbeats-deployment-7575c888c7-85w5t Sink URL is http://event-display.default.svc.cluster.local Emitting event #1 Emitting event #2 Event posted successfully Event posted successfully Using SinkBinding \u00b6 SinkBinding does not create any containers. It injects the sink information to an already existing Kubernetes resources. This is a flexible approach as you can use any Kubernetes PodSpecable object as an event source, such as Deployment, Job, or Knative services. Procedure \u00b6 Create an event display service: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : event-display spec : template : spec : containers : - image : docker.io/aliok/event_display-864884f202126ec3150c5fcef437d90c@sha256:93cb4dcda8fee80a1f68662ae6bf20301471b046ede628f3c3f94f39752fbe08 Create a Kubernetes deployment that runs the event source: apiVersion : apps/v1 kind : Deployment metadata : name : node-heartbeats-deployment labels : app : node-heartbeats spec : replicas : 2 selector : matchLabels : app : node-heartbeats template : metadata : labels : app : node-heartbeats spec : containers : - name : node-heartbeats image : path/to/image/registry/node-knative-heartbeat-source:v1 ports : - containerPort : 8080 Because the SinkBinding has not yet been created, you will see an error message, because the K_SINK environment variable is not yet injected: $ kubectl logs node-heartbeats-deployment-9ffbb644b-llkzk Sink URL is undefined Emitting event #1 Error during event post TypeError [ ERR_INVALID_ARG_TYPE ] : The \"url\" argument must be of type string. Received type undefined Create the SinkBinding object: apiVersion : sources.knative.dev/v1 kind : SinkBinding metadata : name : bind-node-heartbeat spec : subject : apiVersion : apps/v1 kind : Deployment selector : matchLabels : app : node-heartbeats sink : ref : apiVersion : serving.knative.dev/v1 kind : Service name : event-display You will see the pods are recreated and this time the K_SINK environment variable is injected. Also note that since the replicas is set to 2, there will be 2 pods that are posting events to the sink. $ kubectl logs event-display-dpplv-deployment-67c9949cf9-bvjvk -c user-container \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1 .0 type: your.event.source.type source: urn:event:from:your-api/resource/123 id: your-event-id datacontenttype: application/json Data, { \"hello\" : \"World 1\" } \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1 .0 type: your.event.source.type source: urn:event:from:your-api/resource/123 id: your-event-id datacontenttype: application/json Data, { \"hello\" : \"World 1\" }","title":"Writing an event source using Javascript"},{"location":"eventing/sources/creating-event-sources/writing-event-source-easy-way/_index/#writing-an-event-source-using-javascript","text":"This tutorial provides instructions to build an event source in Javascript and implement it with a ContainerSource or SinkBinding. Using a ContainerSource is a simple way to turn any dispatcher container into a Knative event source. Using SinkBinding provides a framework for injecting environment variables into any Kubernetes resource that has a spec.template and is PodSpecable . ContainerSource and SinkBinding both work by injecting environment variables to an application. Injected environment variables at minimum contain the URL of a sink that will receive events.","title":"Writing an event source using Javascript"},{"location":"eventing/sources/creating-event-sources/writing-event-source-easy-way/_index/#bootstrapping","text":"Create the project and add the dependencies: npm init npm install cloudevents-sdk@2.0.1 --save NOTE: Due to this bug , you must use version 2.0.1 of the Javascript SDK or newer.","title":"Bootstrapping"},{"location":"eventing/sources/creating-event-sources/writing-event-source-easy-way/_index/#using-containersource","text":"A ContainerSource creates a container for your event source image and manages this container. The sink URL to post the events will be made available to the application through the K_SINK environment variable by the ContainerSource.","title":"Using ContainerSource"},{"location":"eventing/sources/creating-event-sources/writing-event-source-easy-way/_index/#example","text":"The following example event source emits an event to the sink every 1000 milliseconds: // File - index.js const { CloudEvent , HTTPEmitter } = require ( \"cloudevents-sdk\" ); let sinkUrl = process . env [ 'K_SINK' ]; console . log ( \"Sink URL is \" + sinkUrl ); let emitter = new HTTPEmitter ({ url : sinkUrl }); let eventIndex = 0 ; setInterval ( function () { console . log ( \"Emitting event #\" + ++ eventIndex ); let myevent = new CloudEvent ({ source : \"urn:event:from:my-api/resource/123\" , type : \"your.event.source.type\" , id : \"your-event-id\" , dataContentType : \"application/json\" , data : { \"hello\" : \"World \" + eventIndex }, }); // Emit the event emitter . send ( myevent ) . then ( response => { // Treat the response console . log ( \"Event posted successfully\" ); console . log ( response . data ); }) . catch ( err => { // Deal with errors console . log ( \"Error during event post\" ); console . error ( err ); }); }, 1000 ); # File - Dockerfile FROM node:10 WORKDIR /usr/src/app COPY package*.json ./ RUN npm install COPY . . EXPOSE 8080 CMD [ \"node\" , \"index.js\" ] The example code uses Binary mode for CloudEvents. To employ structured code, change let binding = new v1.BinaryHTTPEmitter(config); to let binding = new v1.StructuredHTTPEmitter(config); . Binary mode is used in most cases because: - It is faster in terms of serialization and deserialization. - It works better with CloudEvent-aware proxies, such as Knative Channels, and can simply check the header instead of parsing the payload.","title":"Example"},{"location":"eventing/sources/creating-event-sources/writing-event-source-easy-way/_index/#procedure","text":"Build and push the image: docker build . -t path/to/image/registry/node-knative-heartbeat-source:v1 docker push path/to/image/registry/node-knative-heartbeat-source:v1 Create the event display service which logs any CloudEvents posted to it: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : event-display spec : template : spec : containers : - image : docker.io/aliok/event_display-864884f202126ec3150c5fcef437d90c@sha256:93cb4dcda8fee80a1f68662ae6bf20301471b046ede628f3c3f94f39752fbe08 Create the ContainerSource object: apiVersion : sources.knative.dev/v1 kind : ContainerSource metadata : name : test-heartbeats spec : template : spec : containers : - image : path/to/image/registry/node-knative-heartbeat-source:v1 name : heartbeats sink : ref : apiVersion : serving.knative.dev/v1 kind : Service name : event-display Check the logs of the event display service. You will see a new message is pushed every second: $ kubectl logs -l serving.knative.dev/service = event-display -c user-container \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1 .0 type: your.event.source.type source: urn:event:from:your-api/resource/123 id: your-event-id datacontenttype: application/json Data, { \"hello\" : \"World 1\" } Optional: If you are interested in seeing what is injected into the event source as a K_SINK , you can check the logs: $ kubectl logs test-heartbeats-deployment-7575c888c7-85w5t Sink URL is http://event-display.default.svc.cluster.local Emitting event #1 Emitting event #2 Event posted successfully Event posted successfully","title":"Procedure"},{"location":"eventing/sources/creating-event-sources/writing-event-source-easy-way/_index/#using-sinkbinding","text":"SinkBinding does not create any containers. It injects the sink information to an already existing Kubernetes resources. This is a flexible approach as you can use any Kubernetes PodSpecable object as an event source, such as Deployment, Job, or Knative services.","title":"Using SinkBinding"},{"location":"eventing/sources/creating-event-sources/writing-event-source-easy-way/_index/#procedure_1","text":"Create an event display service: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : event-display spec : template : spec : containers : - image : docker.io/aliok/event_display-864884f202126ec3150c5fcef437d90c@sha256:93cb4dcda8fee80a1f68662ae6bf20301471b046ede628f3c3f94f39752fbe08 Create a Kubernetes deployment that runs the event source: apiVersion : apps/v1 kind : Deployment metadata : name : node-heartbeats-deployment labels : app : node-heartbeats spec : replicas : 2 selector : matchLabels : app : node-heartbeats template : metadata : labels : app : node-heartbeats spec : containers : - name : node-heartbeats image : path/to/image/registry/node-knative-heartbeat-source:v1 ports : - containerPort : 8080 Because the SinkBinding has not yet been created, you will see an error message, because the K_SINK environment variable is not yet injected: $ kubectl logs node-heartbeats-deployment-9ffbb644b-llkzk Sink URL is undefined Emitting event #1 Error during event post TypeError [ ERR_INVALID_ARG_TYPE ] : The \"url\" argument must be of type string. Received type undefined Create the SinkBinding object: apiVersion : sources.knative.dev/v1 kind : SinkBinding metadata : name : bind-node-heartbeat spec : subject : apiVersion : apps/v1 kind : Deployment selector : matchLabels : app : node-heartbeats sink : ref : apiVersion : serving.knative.dev/v1 kind : Service name : event-display You will see the pods are recreated and this time the K_SINK environment variable is injected. Also note that since the replicas is set to 2, there will be 2 pods that are posting events to the sink. $ kubectl logs event-display-dpplv-deployment-67c9949cf9-bvjvk -c user-container \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1 .0 type: your.event.source.type source: urn:event:from:your-api/resource/123 id: your-event-id datacontenttype: application/json Data, { \"hello\" : \"World 1\" } \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1 .0 type: your.event.source.type source: urn:event:from:your-api/resource/123 id: your-event-id datacontenttype: application/json Data, { \"hello\" : \"World 1\" }","title":"Procedure"},{"location":"eventing/sugar/_index/","text":"Macro Rendering Error \u00b6 TemplateSyntaxError : unexpected '<' Traceback (most recent call last): File \"/usr/local/lib/python3.9/site-packages/mkdocs_macros/plugin.py\", line 441, in render md_template = self.env.from_string(markdown) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 941, in from_string return cls.from_code(self, self.compile(source), globals, None) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 638, in compile self.handle_exception(source=source_hint) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 832, in handle_exception reraise(*rewrite_traceback_stack(source=source)) File \"/usr/local/lib/python3.9/site-packages/jinja2/_compat.py\", line 28, in reraise raise value.with_traceback(tb) File \"<unknown>\", line 14, in template jinja2.exceptions.TemplateSyntaxError: unexpected '<'","title":"Knative Eventing Sugar Controller"},{"location":"eventing/sugar/_index/#macro-rendering-error","text":"TemplateSyntaxError : unexpected '<' Traceback (most recent call last): File \"/usr/local/lib/python3.9/site-packages/mkdocs_macros/plugin.py\", line 441, in render md_template = self.env.from_string(markdown) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 941, in from_string return cls.from_code(self, self.compile(source), globals, None) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 638, in compile self.handle_exception(source=source_hint) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 832, in handle_exception reraise(*rewrite_traceback_stack(source=source)) File \"/usr/local/lib/python3.9/site-packages/jinja2/_compat.py\", line 28, in reraise raise value.with_traceback(tb) File \"<unknown>\", line 14, in template jinja2.exceptions.TemplateSyntaxError: unexpected '<'","title":"Macro Rendering Error"},{"location":"eventing/triggers/","text":"Triggers \u00b6 A Trigger represents a desire to subscribe to events from a specific Broker. The subscriber value must be a Destination . Simple example which will receive all the events from the given ( default ) broker and deliver them to Knative Serving service my-service : kubectl create -f - <<EOF apiVersion: eventing.knative.dev/v1 kind: Trigger metadata: name: my-service-trigger spec: broker: default subscriber: ref: apiVersion: serving.knative.dev/v1 kind: Service name: my-service EOF Simple example which will receive all the events from the given ( default ) broker and deliver them to the custom path /my-custom-path for the Kubernetes service my-service : kubectl create -f - <<EOF apiVersion: eventing.knative.dev/v1 kind: Trigger metadata: name: my-service-trigger spec: broker: default subscriber: ref: apiVersion: v1 kind: Service name: my-service uri: /my-custom-path EOF Trigger Filtering \u00b6 Exact match filtering on any number of CloudEvents attributes as well as extensions are supported. If your filter sets multiple attributes, an event must have all of the attributes for the Trigger to filter it. Note that we only support exact matching on string values. Example: kubectl create -f - <<EOF apiVersion: eventing.knative.dev/v1 kind: Trigger metadata: name: my-service-trigger spec: broker: default filter: attributes: type: dev.knative.foo.bar myextension: my-extension-value subscriber: ref: apiVersion: serving.knative.dev/v1 kind: Service name: my-service EOF The example above filters events from the default Broker that are of type dev.knative.foo.bar AND have the extension myextension with the value my-extension-value .","title":"Triggers"},{"location":"eventing/triggers/#triggers","text":"A Trigger represents a desire to subscribe to events from a specific Broker. The subscriber value must be a Destination . Simple example which will receive all the events from the given ( default ) broker and deliver them to Knative Serving service my-service : kubectl create -f - <<EOF apiVersion: eventing.knative.dev/v1 kind: Trigger metadata: name: my-service-trigger spec: broker: default subscriber: ref: apiVersion: serving.knative.dev/v1 kind: Service name: my-service EOF Simple example which will receive all the events from the given ( default ) broker and deliver them to the custom path /my-custom-path for the Kubernetes service my-service : kubectl create -f - <<EOF apiVersion: eventing.knative.dev/v1 kind: Trigger metadata: name: my-service-trigger spec: broker: default subscriber: ref: apiVersion: v1 kind: Service name: my-service uri: /my-custom-path EOF","title":"Triggers"},{"location":"eventing/triggers/#trigger-filtering","text":"Exact match filtering on any number of CloudEvents attributes as well as extensions are supported. If your filter sets multiple attributes, an event must have all of the attributes for the Trigger to filter it. Note that we only support exact matching on string values. Example: kubectl create -f - <<EOF apiVersion: eventing.knative.dev/v1 kind: Trigger metadata: name: my-service-trigger spec: broker: default filter: attributes: type: dev.knative.foo.bar myextension: my-extension-value subscriber: ref: apiVersion: serving.knative.dev/v1 kind: Service name: my-service EOF The example above filters events from the default Broker that are of type dev.knative.foo.bar AND have the extension myextension with the value my-extension-value .","title":"Trigger Filtering"},{"location":"getting-started/first-autoscale/","text":"Remember those super powers we talked about? One of Knative Serving's powers is autoscaling. This means your Knative Service will only \"spin up\" to preform its job (in this case, saying \"Hello world!\") if it is needed, otherwise, it will spin down and wait for a new request to come in. Let's see this in action! We're going to peek under the hood at the Pods in Kubernetes where our Knative Service is running to watch our \"Hello world!\" Service scale up and down. Check the Knative Pods \u00b6 Let's run our \"Hello world!\" Service just one more time. This time, try the Knative Service URL in your browser or by using open instead of curl in your CLI. open <service-url> Where <service-url> is the URL where your Knative Service was deployed (same as the <service-url> in the last step). You can watch the pods and see how they scale down to zero after http traffic stops to the url kubectl get pod -l serving.knative.dev/service = hello -w The output should look like this: NAME READY STATUS hello-world 2 /2 Running hello-world 2 /2 Terminating hello-world 1 /2 Terminating hello-world 0 /2 Terminating Try to access the url again, and you will see a new pod running again. NAME READY STATUS hello-world 0 /2 Pending hello-world 0 /2 ContainerCreating hello-world 1 /2 Running hello-world 2 /2 Running Some people call this Serverless","title":"Autoscaling in Action"},{"location":"getting-started/first-autoscale/#check-the-knative-pods","text":"Let's run our \"Hello world!\" Service just one more time. This time, try the Knative Service URL in your browser or by using open instead of curl in your CLI. open <service-url> Where <service-url> is the URL where your Knative Service was deployed (same as the <service-url> in the last step). You can watch the pods and see how they scale down to zero after http traffic stops to the url kubectl get pod -l serving.knative.dev/service = hello -w The output should look like this: NAME READY STATUS hello-world 2 /2 Running hello-world 2 /2 Terminating hello-world 1 /2 Terminating hello-world 0 /2 Terminating Try to access the url again, and you will see a new pod running again. NAME READY STATUS hello-world 0 /2 Pending hello-world 0 /2 ContainerCreating hello-world 1 /2 Running hello-world 2 /2 Running Some people call this Serverless","title":"Check the Knative Pods"},{"location":"getting-started/first-broker/","text":"Background \u00b6 In the previous step, we talked a bit about CloudEvents and their role in Knative, specifically their ability to transport bits of information within our Knative deployment. But how does a CloudEvent get where it needs to go? If you \"emit\" (produce) a CloudEvent, how can you ensure that it arrives at its destination? That's where Brokers come in. Brokers are a fault-tolerant way to intake CloudEvents from Sources (which emit CloudEvents) and send them to the correct Sink (which consume CloudEvents). But a Broker can't do this work alone, Triggers help to filter the CloudEvents to the correct place (but more on that later). Examining the Broker \u00b6 As part of the KonK install, you should have an In-Memory Broker already installed (also known as an MT-Channel-based Broker ). Let's check to see some of the details of the Broker that was created using kn : kn broker list You should get: NAME URL AGE CONDITIONS READY REASON default <broker-url> 5m 5 OK / 5 True Where <broker-url> is the location of the Broker . (You'll need this later!) Warning In-Memory Brokers are for development use only and must not be used in a production deployment.","title":"Examining the Broker"},{"location":"getting-started/first-broker/#background","text":"In the previous step, we talked a bit about CloudEvents and their role in Knative, specifically their ability to transport bits of information within our Knative deployment. But how does a CloudEvent get where it needs to go? If you \"emit\" (produce) a CloudEvent, how can you ensure that it arrives at its destination? That's where Brokers come in. Brokers are a fault-tolerant way to intake CloudEvents from Sources (which emit CloudEvents) and send them to the correct Sink (which consume CloudEvents). But a Broker can't do this work alone, Triggers help to filter the CloudEvents to the correct place (but more on that later).","title":"Background"},{"location":"getting-started/first-broker/#examining-the-broker","text":"As part of the KonK install, you should have an In-Memory Broker already installed (also known as an MT-Channel-based Broker ). Let's check to see some of the details of the Broker that was created using kn : kn broker list You should get: NAME URL AGE CONDITIONS READY REASON default <broker-url> 5m 5 OK / 5 True Where <broker-url> is the location of the Broker . (You'll need this later!) Warning In-Memory Brokers are for development use only and must not be used in a production deployment.","title":"Examining the Broker"},{"location":"getting-started/first-cloud-event/","text":"Eventing Background and CloudEvents \u00b6 Background \u00b6 With Knative Serving, we have a powerful tool which can take our containerized code and deploy it with relative ease. Nowadays, modern infrastructure demands we deploy many services to do many different things and, often times, these services have dependency on one another. Your hypothetical \"Inventory Management\" Knative Service would like to know when your \"Point-Of-Sale\" Knative Service preforms an action (i.e. makes a sale) so it can preform its own relevant actions. Knative Eventing provides the basis for communicating these sorts of actions, whether those actions are coming from within your Kubernetes deployment or from some outside source. For the purposes of this tutorial, we will focus on four components for building this communication infrastructure ( Source , Trigger , Broker , and Sink ) , but there are many other tools in the Knative Eventing toolbox which can be used to create your own \"Functions as a Service\" (FaaS) on top of Kubernetes. What other components exist in Knative Eventing? If you want to find out more about the different components of Knative Eventing, like Channels, Sequences, Parallels, etc. check out the \"Eventing Components.\" Introducing CloudEvents \u00b6 As the name suggests, Knative Eventing communicates these actions in the form of \"Events\" and the format Knative uses for these Events is CloudEvents . For our purposes, the only thing you need to know about CloudEvents are: CloudEvents follow a specification (a specific format), with required and optional attributes. CloudEvents can be \"emitted\" (created) by almost anything. This includes Knative Services, as well as data sources that exist outside of your Kubernetes deployment. CloudEvents can carry some attributes (things like id , source , type , etc) as well as payloads (JSON, plaintext, reference to data that lives elsewhere, etc). Want to find out more about CloudEvents? Check out the CloudEvents website !","title":"Background and CloudEvents"},{"location":"getting-started/first-cloud-event/#eventing-background-and-cloudevents","text":"","title":"Eventing Background and CloudEvents"},{"location":"getting-started/first-cloud-event/#background","text":"With Knative Serving, we have a powerful tool which can take our containerized code and deploy it with relative ease. Nowadays, modern infrastructure demands we deploy many services to do many different things and, often times, these services have dependency on one another. Your hypothetical \"Inventory Management\" Knative Service would like to know when your \"Point-Of-Sale\" Knative Service preforms an action (i.e. makes a sale) so it can preform its own relevant actions. Knative Eventing provides the basis for communicating these sorts of actions, whether those actions are coming from within your Kubernetes deployment or from some outside source. For the purposes of this tutorial, we will focus on four components for building this communication infrastructure ( Source , Trigger , Broker , and Sink ) , but there are many other tools in the Knative Eventing toolbox which can be used to create your own \"Functions as a Service\" (FaaS) on top of Kubernetes. What other components exist in Knative Eventing? If you want to find out more about the different components of Knative Eventing, like Channels, Sequences, Parallels, etc. check out the \"Eventing Components.\"","title":"Background"},{"location":"getting-started/first-cloud-event/#introducing-cloudevents","text":"As the name suggests, Knative Eventing communicates these actions in the form of \"Events\" and the format Knative uses for these Events is CloudEvents . For our purposes, the only thing you need to know about CloudEvents are: CloudEvents follow a specification (a specific format), with required and optional attributes. CloudEvents can be \"emitted\" (created) by almost anything. This includes Knative Services, as well as data sources that exist outside of your Kubernetes deployment. CloudEvents can carry some attributes (things like id , source , type , etc) as well as payloads (JSON, plaintext, reference to data that lives elsewhere, etc). Want to find out more about CloudEvents? Check out the CloudEvents website !","title":"Introducing CloudEvents"},{"location":"getting-started/first-service/","text":"Deploying your first Knative Service \u00b6 Tip Hit n / . on your keyboard to move forward in the tutorial. Use p / , to go back at any time. In this tutorial, we are going to use KonK to deploy a \"Hello world\" Service! This service will accept an environment variable, TARGET , and print \" Hello $TARGET .\" For those of you familiar with other source-to-url tools, this may seem familiar. However, since our \"Hello world\" Service is being deployed as a Knative Service, it gets some super powers (scale-to-zero, traffic-splitting) out of the box . Deploying your first Knative Service: \"Hello world!\" \u00b6 kn kn service create hello \\ --image gcr.io/knative-samples/helloworld-go \\ --port 8080 \\ --env TARGET = world \\ --revision-name = world YAML apiVersion: serving.knative.dev/v1 kind: Service metadata: name: hello spec: template: spec: containers: - image: gcr.io/knative-samples/helloworld-go ports: - containerPort: 8080 env: - name: TARGET value: \"world\" Once you've created your YAML file (named something like \"hello.yaml\"): kubectl apply -f hello.yaml Expected Output \u00b6 After Knative has successfully created your service, you should see the following: Service hello created to latest revision 'hello-world' is available at URL: <service-url> Where <service-url> is the URL where your Knative Service can be reached. Note that the name \"world\" which you passed in as \"revision-name\" is being used to create the Revision 's name \"hello-world\" (we'll talk more about Revisions later). Testing your deployment \u00b6 curl <service-url> Where <service-url> is the URL returned to you by the previous command. The output should be: Hello world! Congratulations , you've just created your first Knative Service!","title":"Deploying your first Knative Service"},{"location":"getting-started/first-service/#deploying-your-first-knative-service","text":"Tip Hit n / . on your keyboard to move forward in the tutorial. Use p / , to go back at any time. In this tutorial, we are going to use KonK to deploy a \"Hello world\" Service! This service will accept an environment variable, TARGET , and print \" Hello $TARGET .\" For those of you familiar with other source-to-url tools, this may seem familiar. However, since our \"Hello world\" Service is being deployed as a Knative Service, it gets some super powers (scale-to-zero, traffic-splitting) out of the box .","title":"Deploying your first Knative Service"},{"location":"getting-started/first-service/#deploying-your-first-knative-service-hello-world","text":"kn kn service create hello \\ --image gcr.io/knative-samples/helloworld-go \\ --port 8080 \\ --env TARGET = world \\ --revision-name = world YAML apiVersion: serving.knative.dev/v1 kind: Service metadata: name: hello spec: template: spec: containers: - image: gcr.io/knative-samples/helloworld-go ports: - containerPort: 8080 env: - name: TARGET value: \"world\" Once you've created your YAML file (named something like \"hello.yaml\"): kubectl apply -f hello.yaml","title":"Deploying your first Knative Service: \"Hello world!\""},{"location":"getting-started/first-service/#expected-output","text":"After Knative has successfully created your service, you should see the following: Service hello created to latest revision 'hello-world' is available at URL: <service-url> Where <service-url> is the URL where your Knative Service can be reached. Note that the name \"world\" which you passed in as \"revision-name\" is being used to create the Revision 's name \"hello-world\" (we'll talk more about Revisions later).","title":"Expected Output"},{"location":"getting-started/first-service/#testing-your-deployment","text":"curl <service-url> Where <service-url> is the URL returned to you by the previous command. The output should be: Hello world! Congratulations , you've just created your first Knative Service!","title":"Testing your deployment"},{"location":"getting-started/first-source-trigger/","text":"Creating your first Source \u00b6 Now that we have a Broker , let's send some CloudEvents to using a Source . A Source is a Kubernetes custom resource (CR) that emits CloudEvents to a specified location (in our case, a Broker ). Basically, a Source is exactly what it sounds like: it's a \"source\" of CloudEvents. You can check the current state of sources in your deployment by running the following command: kn source list You should get: No sources found. You haven't deployed any Sources to your Knative deployment, so there shouldn't be any present when kn checks for them. Let's remedy this by creating our first Knative Source . kn service create cloudevents-player \\ --image ruromero/cloudevents-player:latest \\ --env BROKER_URL = <broker-url> Where <broker-url> is the URL of your In-Memory Broker which we discovered in the previous step. You should get: ... 'cloudevents-player-xxxxx-``' is available at URL: <service-url> Where <service-url> is the location where your Knative Service is hosted. The CloudEvents Player is a very useful web app that we can use to send and receive CloudEvents. If you open this url in your browser, you should be greeted by a slick UI which includes a form titled \"Create Event.\" Sending your first CloudEvent \u00b6 In the \"Create Event\" form, we are greeted by a number of fields, all of which are required. Field Description Event ID A unique ID (generally a UUID . Click the loop icon to generate a new one. Event Type An event type. Event Source An event source. Specversion Demarcates which CloudEvents spec you're using (should always be 1.0). Message The data section of the CloudEvent, a payload which is carrying the data you care to be delivered. Fill out the form with whatever you data you would like to and hit the \"SEND EVENT\" button. You should see this: Tip: Clicking the will show you the CloudEvent as the Broker sees it. The icon in the \"Status\" column implies that the event has been sent to our Broker which we specified through the environment variable BROKER_URL . But where has the event gone? Well, right now, nowhere! A Broker is simply a receptacle for events. In order for your events to be sent somewhere, you must create a Trigger which listens for your events and places them somewhere. Creating your first Trigger \u00b6 A Trigger represents a desire to subscribe to events from a specific broker. If, for example, you wanted to intake the CloudEvents our CloudEvents Player was emitting to our Broker you could create a Trigger , like so: kn trigger create cloudevents-player --sink cloudevents-player Notice that you specified a Sink in the creation of our Trigger which tells Knative where to put the Events this Trigger is listening for. You should see: Trigger 'cloudevents-player' successfully created in namespace 'default' . What CloudEvents is my Trigger listening for? Since we didn't specify a --filter in our kn command, our Trigger is listening for any CloudEvents coming into the Broker . Now, when we go back to the CloudEvents Player and send an Event, we see that CloudEvents are both sent and received: What if I want to filter on CloudEvent attributes? First, delete your existing Trigger: kn trigger delete cloudevents-player Now let's add a Trigger that listens for a certain CloudEvent Type kn trigger create cloudevents-player --sink cloudevents-player --filter type = com.example If you send a CloudEvent with type \"com.example,\" it will be reflected in the CloudEvents Player UI. Any other types will be ignored by the Trigger . You can filter on any aspect of the CloudEvent you would like to. In review, you have created a Knative Service (the CloudEvents Player) as your Source of CloudEvents which are sent through the Broker , routed by the Trigger and received by that same Knative Service which is also acting as a Sink for CloudEvents. Architecture Diagram //TODO Some people call this \"Event-Driven Architecture\" which can be used to create your own \"Functions as a Service\" on Kubernetes","title":"Routing CloudEvents with Sources, Brokers and Triggers"},{"location":"getting-started/first-source-trigger/#creating-your-first-source","text":"Now that we have a Broker , let's send some CloudEvents to using a Source . A Source is a Kubernetes custom resource (CR) that emits CloudEvents to a specified location (in our case, a Broker ). Basically, a Source is exactly what it sounds like: it's a \"source\" of CloudEvents. You can check the current state of sources in your deployment by running the following command: kn source list You should get: No sources found. You haven't deployed any Sources to your Knative deployment, so there shouldn't be any present when kn checks for them. Let's remedy this by creating our first Knative Source . kn service create cloudevents-player \\ --image ruromero/cloudevents-player:latest \\ --env BROKER_URL = <broker-url> Where <broker-url> is the URL of your In-Memory Broker which we discovered in the previous step. You should get: ... 'cloudevents-player-xxxxx-``' is available at URL: <service-url> Where <service-url> is the location where your Knative Service is hosted. The CloudEvents Player is a very useful web app that we can use to send and receive CloudEvents. If you open this url in your browser, you should be greeted by a slick UI which includes a form titled \"Create Event.\"","title":"Creating your first Source"},{"location":"getting-started/first-source-trigger/#sending-your-first-cloudevent","text":"In the \"Create Event\" form, we are greeted by a number of fields, all of which are required. Field Description Event ID A unique ID (generally a UUID . Click the loop icon to generate a new one. Event Type An event type. Event Source An event source. Specversion Demarcates which CloudEvents spec you're using (should always be 1.0). Message The data section of the CloudEvent, a payload which is carrying the data you care to be delivered. Fill out the form with whatever you data you would like to and hit the \"SEND EVENT\" button. You should see this: Tip: Clicking the will show you the CloudEvent as the Broker sees it. The icon in the \"Status\" column implies that the event has been sent to our Broker which we specified through the environment variable BROKER_URL . But where has the event gone? Well, right now, nowhere! A Broker is simply a receptacle for events. In order for your events to be sent somewhere, you must create a Trigger which listens for your events and places them somewhere.","title":"Sending your first CloudEvent"},{"location":"getting-started/first-source-trigger/#creating-your-first-trigger","text":"A Trigger represents a desire to subscribe to events from a specific broker. If, for example, you wanted to intake the CloudEvents our CloudEvents Player was emitting to our Broker you could create a Trigger , like so: kn trigger create cloudevents-player --sink cloudevents-player Notice that you specified a Sink in the creation of our Trigger which tells Knative where to put the Events this Trigger is listening for. You should see: Trigger 'cloudevents-player' successfully created in namespace 'default' . What CloudEvents is my Trigger listening for? Since we didn't specify a --filter in our kn command, our Trigger is listening for any CloudEvents coming into the Broker . Now, when we go back to the CloudEvents Player and send an Event, we see that CloudEvents are both sent and received: What if I want to filter on CloudEvent attributes? First, delete your existing Trigger: kn trigger delete cloudevents-player Now let's add a Trigger that listens for a certain CloudEvent Type kn trigger create cloudevents-player --sink cloudevents-player --filter type = com.example If you send a CloudEvent with type \"com.example,\" it will be reflected in the CloudEvents Player UI. Any other types will be ignored by the Trigger . You can filter on any aspect of the CloudEvent you would like to. In review, you have created a Knative Service (the CloudEvents Player) as your Source of CloudEvents which are sent through the Broker , routed by the Trigger and received by that same Knative Service which is also acting as a Sink for CloudEvents. Architecture Diagram //TODO Some people call this \"Event-Driven Architecture\" which can be used to create your own \"Functions as a Service\" on Kubernetes","title":"Creating your first Trigger"},{"location":"getting-started/first-traffic-split/","text":"Basics of Traffic Splitting \u00b6 The last super power of Knative Serving we'll go over is traffic splitting. Splitting traffic is useful for a number of very common modern infrastructure needs, such as blue/green deployments and canary deployments . Bringing these industry standards to bear on Kubernetes is as simple as a single CLI command on Knative or YAML tweak, let's see how! You may have noticed that when your Knative Service was created, Knative returned both a URL and a 'latest revision' for your Knative Service. But what happens if you make a change to your service? What is a Revisions ?\" You can think of a Revision as a stateless, autoscaling snapshot-in-time of application code and configuration. A new Revision will get created each and every time you make changes to your Knative Service. Knative Serving splits traffic between different Revisions of your Knative Service. Creating a new Revision \u00b6 Instead of \"world,\" let's have our Knative Service \"hello\" greet \"Knative.\" You can accomplish this by using the kn CLI or by editing the YAML file you made earlier. kn kn service update hello \\ --env TARGET = Knative \\ --revision-name = knative YAML //TODO Update revision name apiVersion: serving.knative.dev/v1 kind: Service metadata: name: hello spec: template: spec: containers: - image: gcr.io/knative-samples/helloworld-go ports: - containerPort: 8080 env: - name: TARGET value: \"Knative\" Once you've edited your existing YAML file: kubectl apply -f hello.yaml As before, Knative spits out some helpful information to the CLI: Service hello created to latest revision 'hello-knative' is available at URL: <service-url> Note, since we are updating an existing Knative Service, the URL doesn't change, but your new Revision should have the new name \"hello-knative\" Let's ping our Knative Service again to see the change: curl <service-url> The output should be: Hello Knative! Splitting Traffic between Revisions \u00b6 You may at this point be wondering, \"where did 'Hello world!' go?\" Revisions are a stateless snapshot-in-time of application code and configuration so your \"hello-world\" Revision is still available to you. We can easily see a list of our existing revisions with the kn CLI: kn kn revisions list kubectl Though the following example doesn't cover it, you can peak under the hood to Kubernetes to see the revisions as Kubernetes sees them. kubectl get revisions The output should be: NAME SERVICE TRAFFIC TAGS GENERATION AGE CONDITIONS READY REASON hello-knative hello 100 % 2 30s 3 OK / 4 True hello-world hello 1 5m 3 OK / 4 True The column most relevant for our purposes is \"TRAFFIC\". It looks like 100% of traffic is going to our latest Revision (\"hello-knative\") and 0% of traffic is going to the Revision we configured earlier (\"hello-world\") By default, when Knative creates a brand new Service it directs 100% of traffic to the pointer @latest , and updates @latest with the most recent Revision when we make a change to our Service. We can change this default behavior by specifying how much traffic we want each of our Revisions to receive. kn kn service update hello \\ --traffic @latest = 50 \\ --traffic hello-world = 50 @latest will always point to our \"latest\" Revision which, at the moment, is hello-knative . YAML apiVersion: serving.knative.dev/v1 kind: Route metadata: name: route-hello spec: traffic: - revisionName: hello-xxxxx-1 - percent: 50 - revisionName: hello-xxxxx-2 percent: 50 Once you've edited your existing YAML file: kubectl apply -f hello.yaml Now when we curl our Knative Service URL... curl <service-url> Hello Knative! curl <service-url> Hello world! Congratulations, you've successfully split traffic between 2 different Revisions . Up next, Knative Eventing!","title":"Traffic Splitting"},{"location":"getting-started/first-traffic-split/#basics-of-traffic-splitting","text":"The last super power of Knative Serving we'll go over is traffic splitting. Splitting traffic is useful for a number of very common modern infrastructure needs, such as blue/green deployments and canary deployments . Bringing these industry standards to bear on Kubernetes is as simple as a single CLI command on Knative or YAML tweak, let's see how! You may have noticed that when your Knative Service was created, Knative returned both a URL and a 'latest revision' for your Knative Service. But what happens if you make a change to your service? What is a Revisions ?\" You can think of a Revision as a stateless, autoscaling snapshot-in-time of application code and configuration. A new Revision will get created each and every time you make changes to your Knative Service. Knative Serving splits traffic between different Revisions of your Knative Service.","title":"Basics of Traffic Splitting"},{"location":"getting-started/first-traffic-split/#creating-a-new-revision","text":"Instead of \"world,\" let's have our Knative Service \"hello\" greet \"Knative.\" You can accomplish this by using the kn CLI or by editing the YAML file you made earlier. kn kn service update hello \\ --env TARGET = Knative \\ --revision-name = knative YAML //TODO Update revision name apiVersion: serving.knative.dev/v1 kind: Service metadata: name: hello spec: template: spec: containers: - image: gcr.io/knative-samples/helloworld-go ports: - containerPort: 8080 env: - name: TARGET value: \"Knative\" Once you've edited your existing YAML file: kubectl apply -f hello.yaml As before, Knative spits out some helpful information to the CLI: Service hello created to latest revision 'hello-knative' is available at URL: <service-url> Note, since we are updating an existing Knative Service, the URL doesn't change, but your new Revision should have the new name \"hello-knative\" Let's ping our Knative Service again to see the change: curl <service-url> The output should be: Hello Knative!","title":"Creating a new Revision"},{"location":"getting-started/first-traffic-split/#splitting-traffic-between-revisions","text":"You may at this point be wondering, \"where did 'Hello world!' go?\" Revisions are a stateless snapshot-in-time of application code and configuration so your \"hello-world\" Revision is still available to you. We can easily see a list of our existing revisions with the kn CLI: kn kn revisions list kubectl Though the following example doesn't cover it, you can peak under the hood to Kubernetes to see the revisions as Kubernetes sees them. kubectl get revisions The output should be: NAME SERVICE TRAFFIC TAGS GENERATION AGE CONDITIONS READY REASON hello-knative hello 100 % 2 30s 3 OK / 4 True hello-world hello 1 5m 3 OK / 4 True The column most relevant for our purposes is \"TRAFFIC\". It looks like 100% of traffic is going to our latest Revision (\"hello-knative\") and 0% of traffic is going to the Revision we configured earlier (\"hello-world\") By default, when Knative creates a brand new Service it directs 100% of traffic to the pointer @latest , and updates @latest with the most recent Revision when we make a change to our Service. We can change this default behavior by specifying how much traffic we want each of our Revisions to receive. kn kn service update hello \\ --traffic @latest = 50 \\ --traffic hello-world = 50 @latest will always point to our \"latest\" Revision which, at the moment, is hello-knative . YAML apiVersion: serving.knative.dev/v1 kind: Route metadata: name: route-hello spec: traffic: - revisionName: hello-xxxxx-1 - percent: 50 - revisionName: hello-xxxxx-2 percent: 50 Once you've edited your existing YAML file: kubectl apply -f hello.yaml Now when we curl our Knative Service URL... curl <service-url> Hello Knative! curl <service-url> Hello world! Congratulations, you've successfully split traffic between 2 different Revisions . Up next, Knative Eventing!","title":"Splitting Traffic between Revisions"},{"location":"getting-started/getting-started-eventing/","text":"Getting Started with Knative Eventing \u00b6 After you install Knative Eventing, you can create, send, and verify events. This guide shows how you can use a basic workflow for managing events. Before you start to manage events, you must create the objects needed to transport the events. Creating a Knative Eventing namespace \u00b6 Namespaces are used to group together and organize your Knative resources. Create a new namespace called event-example by entering the following command: kubectl create namespace event-example Adding a broker to the namespace \u00b6 The broker allows you to route events to different event sinks or consumers. Add a broker named default to your namespace by entering the following command: kubectl create -f - <<EOF apiVersion: eventing.knative.dev/v1 kind: broker metadata: name: default namespace: event-example EOF Verify that the broker is working correctly, by entering the following command: kubectl -n event-example get broker default This shows information about your broker. If the broker is working correctly, it shows a READY status of True : NAME READY REASON URL AGE default True http://broker-ingress.knative-eventing.svc.cluster.local/event-example/default 1m If READY is False , wait a few moments and then run the command again. If you continue to receive the False status, see the Debugging Guide to troubleshoot the issue. Creating event consumers \u00b6 In this step, you create two event consumers, hello-display and goodbye-display , to demonstrate how you can configure your event producers to target a specific consumer. To deploy the hello-display consumer to your cluster, run the following command: kubectl -n event-example apply -f - << EOF apiVersion: apps/v1 kind: Deployment metadata: name: hello-display spec: replicas: 1 selector: matchLabels: &labels app: hello-display template: metadata: labels: *labels spec: containers: - name: event-display image: gcr.io/knative-releases/knative.dev/eventing-contrib/cmd/event_display --- kind: Service apiVersion: v1 metadata: name: hello-display spec: selector: app: hello-display ports: - protocol: TCP port: 80 targetPort: 8080 EOF To deploy the goodbye-display consumer to your cluster, run the following command: kubectl -n event-example apply -f - << EOF apiVersion: apps/v1 kind: Deployment metadata: name: goodbye-display spec: replicas: 1 selector: matchLabels: &labels app: goodbye-display template: metadata: labels: *labels spec: containers: - name: event-display # Source code: https://github.com/knative/eventing-contrib/tree/main/cmd/event_display image: gcr.io/knative-releases/knative.dev/eventing-contrib/cmd/event_display --- kind: Service apiVersion: v1 metadata: name: goodbye-display spec: selector: app: goodbye-display ports: - protocol: TCP port: 80 targetPort: 8080 EOF Verify that the event consumers are working by entering the following command: kubectl -n event-example get deployments hello-display goodbye-display This lists the hello-display and goodbye-display consumers that you deployed: NAME READY UP-TO-DATE AVAILABLE AGE hello-display 1/1 1 1 26s goodbye-display 1/1 1 1 16s The number of replicas in the READY column should match the number of replicas in the AVAILABLE column. If the numbers do not match, see the Debugging Guide to troubleshoot the issue. Creating triggers \u00b6 A trigger defines the events that each event consumer receives. Brokers use triggers to forward events to the correct consumers. Each trigger can specify a filter that enables selection of relevant events based on the Cloud Event context attributes. Create a trigger by entering the following command: kubectl -n event-example apply -f - << EOF apiVersion: eventing.knative.dev/v1 kind: Trigger metadata: name: hello-display spec: broker: default filter: attributes: type: greeting subscriber: ref: apiVersion: v1 kind: Service name: hello-display EOF The command creates a trigger that sends all events of type greeting to your event consumer named hello-display . To add a second trigger, enter the following command: kubectl -n event-example apply -f - << EOF apiVersion: eventing.knative.dev/v1 kind: Trigger metadata: name: goodbye-display spec: broker: default filter: attributes: source: sendoff subscriber: ref: apiVersion: v1 kind: Service name: goodbye-display EOF The command creates a trigger that sends all events of source sendoff to your event consumer named goodbye-display . Verify that the triggers are working correctly by running the following command: kubectl -n event-example get triggers This returns the hello-display and goodbye-display triggers that you created: NAME READY REASON BROKER SUBSCRIBER_URI AGE goodbye-display True default http://goodbye-display.event-example.svc.cluster.local/ 9s hello-display True default http://hello-display.event-example.svc.cluster.local/ 16s If the triggers are correctly configured, they will be ready and pointing to the correct broker ( default ) and SUBSCRIBER_URI . The SUBSCRIBER_URI has a value similar to triggerName.namespaceName.svc.cluster.local . The exact value depends on the broker implementation. If this value looks incorrect, see the Debugging Guide to troubleshoot the issue. Creating a pod as an event producer \u00b6 This guide uses curl commands to manually send individual events as HTTP requests to the broker, and demonstrate how these events are received by the correct event consumer. The broker can only be accessed from within the cluster where Knative Eventing is installed. You must create a pod within that cluster to act as an event producer that will execute the curl commands. To create a pod, enter the following command: kubectl -n event-example apply -f - << EOF apiVersion: v1 kind: Pod metadata: labels: run: curl name: curl spec: containers: # This could be any image that we can SSH into and has curl. - image: radial/busyboxplus:curl imagePullPolicy: IfNotPresent name: curl resources: {} stdin: true terminationMessagePath: /dev/termination-log terminationMessagePolicy: File tty: true EOF Sending events to the broker \u00b6 SSH into the pod by running the following command: kubectl -n event-example attach curl -it You will see a prompt similar to the following: Defaulting container name to curl. Use 'kubectl describe pod/ -n event-example' to see all of the containers in this pod. If you don't see a command prompt, try pressing enter. [ root@curl:/ ]$ Make a HTTP request to the broker. To show the various types of events you can send, you will make three requests: To make the first request, which creates an event that has the type greeting , run the following in the SSH terminal: curl -v \"http://broker-ingress.knative-eventing.svc.cluster.local/event-example/default\" \\ -X POST \\ -H \"Ce-Id: say-hello\" \\ -H \"Ce-Specversion: 1.0\" \\ -H \"Ce-Type: greeting\" \\ -H \"Ce-Source: not-sendoff\" \\ -H \"Content-Type: application/json\" \\ -d '{\"msg\":\"Hello Knative!\"}' When the broker receives your event, hello-display will activate and send it to the event consumer of the same name. If the event has been received, you will receive a 202 Accepted response similar to the one below: < HTTP/1.1 202 Accepted < Content-Length: 0 < Date: Mon, 12 Aug 2019 19:48:18 GMT - To make the second request, which creates an event that has the source sendoff , run the following in the SSH terminal: curl -v \"http://broker-ingress.knative-eventing.svc.cluster.local/event-example/default\" \\ -X POST \\ -H \"Ce-Id: say-goodbye\" \\ -H \"Ce-Specversion: 1.0\" \\ -H \"Ce-Type: not-greeting\" \\ -H \"Ce-Source: sendoff\" \\ -H \"Content-Type: application/json\" \\ -d '{\"msg\":\"Goodbye Knative!\"}' When the broker receives your event, goodbye-display will activate and send the event to the event consumer of the same name. If the event has been received, you will receive a 202 Accepted response similar to the one below: < HTTP/1.1 202 Accepted < Content-Length: 0 < Date: Mon, 12 Aug 2019 19:48:18 GMT - To make the third request, which creates an event that has the type greeting and the source sendoff , run the following in the SSH terminal: curl -v \"http://broker-ingress.knative-eventing.svc.cluster.local/event-example/default\" \\ -X POST \\ -H \"Ce-Id: say-hello-goodbye\" \\ -H \"Ce-Specversion: 1.0\" \\ -H \"Ce-Type: greeting\" \\ -H \"Ce-Source: sendoff\" \\ -H \"Content-Type: application/json\" \\ -d '{\"msg\":\"Hello Knative! Goodbye Knative!\"}' When the broker receives your event, hello-display and goodbye-display will activate and send the event to the event consumers of the same name. If the event has been received, you will receive a 202 Accepted response similar to the one below: < HTTP/1.1 202 Accepted < Content-Length: 0 < Date: Mon, 12 Aug 2019 19:48:18 GMT Exit SSH by typing exit into the command prompt. You have sent two events to the hello-display event consumer and two events to the goodbye-display event consumer (note that say-hello-goodbye activates the trigger conditions for both hello-display and goodbye-display ). You will verify that these events were received correctly in the next section. Verifying that events were received \u00b6 After you send the events, verify that the events were received by the correct subscribers. Look at the logs for the hello-display event consumer by entering the following command: kubectl -n event-example logs -l app=hello-display --tail=100 This returns the Attributes and Data of the events you sent to hello-display : \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1.0 type: greeting source: not-sendoff id: say-hello time: 2019-05-20T17:59:43.81718488Z contenttype: application/json Extensions, knativehistory: default-broker-srk54-channel-24gls.event-example.svc.cluster.local Data, { \"msg\": \"Hello Knative!\" } \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1.0 type: greeting source: sendoff id: say-hello-goodbye time: 2019-05-20T17:59:54.211866425Z contenttype: application/json Extensions, knativehistory: default-broker-srk54-channel-24gls.event-example.svc.cluster.local Data, { \"msg\": \"Hello Knative! Goodbye Knative!\" } Look at the logs for the goodbye-display event consumer by entering the following command: kubectl -n event-example logs -l app=goodbye-display --tail=100 This returns the Attributes and Data of the events you sent to goodbye-display : \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1.0 type: not-greeting source: sendoff id: say-goodbye time: 2019-05-20T17:59:49.044926148Z contenttype: application/json Extensions, knativehistory: default-broker-srk54-channel-24gls.event-example.svc.cluster.local Data, { \"msg\": \"Goodbye Knative!\" } \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1.0 type: greeting source: sendoff id: say-hello-goodbye time: 2019-05-20T17:59:54.211866425Z contenttype: application/json Extensions, knativehistory: default-broker-srk54-channel-24gls.event-example.svc.cluster.local Data, { \"msg\": \"Hello Knative! Goodbye Knative!\" } Cleaning up example resources \u00b6 You can delete the event-example namespace and its associated resources from your cluster if you do not plan to use it again in the future. Delete the event-example namespace and all of its resources from your cluster by entering the following command: kubectl delete namespace event-example","title":"Getting Started with Knative Eventing"},{"location":"getting-started/getting-started-eventing/#getting-started-with-knative-eventing","text":"After you install Knative Eventing, you can create, send, and verify events. This guide shows how you can use a basic workflow for managing events. Before you start to manage events, you must create the objects needed to transport the events.","title":"Getting Started with Knative Eventing"},{"location":"getting-started/getting-started-eventing/#creating-a-knative-eventing-namespace","text":"Namespaces are used to group together and organize your Knative resources. Create a new namespace called event-example by entering the following command: kubectl create namespace event-example","title":"Creating a Knative Eventing namespace"},{"location":"getting-started/getting-started-eventing/#adding-a-broker-to-the-namespace","text":"The broker allows you to route events to different event sinks or consumers. Add a broker named default to your namespace by entering the following command: kubectl create -f - <<EOF apiVersion: eventing.knative.dev/v1 kind: broker metadata: name: default namespace: event-example EOF Verify that the broker is working correctly, by entering the following command: kubectl -n event-example get broker default This shows information about your broker. If the broker is working correctly, it shows a READY status of True : NAME READY REASON URL AGE default True http://broker-ingress.knative-eventing.svc.cluster.local/event-example/default 1m If READY is False , wait a few moments and then run the command again. If you continue to receive the False status, see the Debugging Guide to troubleshoot the issue.","title":"Adding a broker to the namespace"},{"location":"getting-started/getting-started-eventing/#creating-event-consumers","text":"In this step, you create two event consumers, hello-display and goodbye-display , to demonstrate how you can configure your event producers to target a specific consumer. To deploy the hello-display consumer to your cluster, run the following command: kubectl -n event-example apply -f - << EOF apiVersion: apps/v1 kind: Deployment metadata: name: hello-display spec: replicas: 1 selector: matchLabels: &labels app: hello-display template: metadata: labels: *labels spec: containers: - name: event-display image: gcr.io/knative-releases/knative.dev/eventing-contrib/cmd/event_display --- kind: Service apiVersion: v1 metadata: name: hello-display spec: selector: app: hello-display ports: - protocol: TCP port: 80 targetPort: 8080 EOF To deploy the goodbye-display consumer to your cluster, run the following command: kubectl -n event-example apply -f - << EOF apiVersion: apps/v1 kind: Deployment metadata: name: goodbye-display spec: replicas: 1 selector: matchLabels: &labels app: goodbye-display template: metadata: labels: *labels spec: containers: - name: event-display # Source code: https://github.com/knative/eventing-contrib/tree/main/cmd/event_display image: gcr.io/knative-releases/knative.dev/eventing-contrib/cmd/event_display --- kind: Service apiVersion: v1 metadata: name: goodbye-display spec: selector: app: goodbye-display ports: - protocol: TCP port: 80 targetPort: 8080 EOF Verify that the event consumers are working by entering the following command: kubectl -n event-example get deployments hello-display goodbye-display This lists the hello-display and goodbye-display consumers that you deployed: NAME READY UP-TO-DATE AVAILABLE AGE hello-display 1/1 1 1 26s goodbye-display 1/1 1 1 16s The number of replicas in the READY column should match the number of replicas in the AVAILABLE column. If the numbers do not match, see the Debugging Guide to troubleshoot the issue.","title":"Creating event consumers"},{"location":"getting-started/getting-started-eventing/#creating-triggers","text":"A trigger defines the events that each event consumer receives. Brokers use triggers to forward events to the correct consumers. Each trigger can specify a filter that enables selection of relevant events based on the Cloud Event context attributes. Create a trigger by entering the following command: kubectl -n event-example apply -f - << EOF apiVersion: eventing.knative.dev/v1 kind: Trigger metadata: name: hello-display spec: broker: default filter: attributes: type: greeting subscriber: ref: apiVersion: v1 kind: Service name: hello-display EOF The command creates a trigger that sends all events of type greeting to your event consumer named hello-display . To add a second trigger, enter the following command: kubectl -n event-example apply -f - << EOF apiVersion: eventing.knative.dev/v1 kind: Trigger metadata: name: goodbye-display spec: broker: default filter: attributes: source: sendoff subscriber: ref: apiVersion: v1 kind: Service name: goodbye-display EOF The command creates a trigger that sends all events of source sendoff to your event consumer named goodbye-display . Verify that the triggers are working correctly by running the following command: kubectl -n event-example get triggers This returns the hello-display and goodbye-display triggers that you created: NAME READY REASON BROKER SUBSCRIBER_URI AGE goodbye-display True default http://goodbye-display.event-example.svc.cluster.local/ 9s hello-display True default http://hello-display.event-example.svc.cluster.local/ 16s If the triggers are correctly configured, they will be ready and pointing to the correct broker ( default ) and SUBSCRIBER_URI . The SUBSCRIBER_URI has a value similar to triggerName.namespaceName.svc.cluster.local . The exact value depends on the broker implementation. If this value looks incorrect, see the Debugging Guide to troubleshoot the issue.","title":"Creating triggers"},{"location":"getting-started/getting-started-eventing/#creating-a-pod-as-an-event-producer","text":"This guide uses curl commands to manually send individual events as HTTP requests to the broker, and demonstrate how these events are received by the correct event consumer. The broker can only be accessed from within the cluster where Knative Eventing is installed. You must create a pod within that cluster to act as an event producer that will execute the curl commands. To create a pod, enter the following command: kubectl -n event-example apply -f - << EOF apiVersion: v1 kind: Pod metadata: labels: run: curl name: curl spec: containers: # This could be any image that we can SSH into and has curl. - image: radial/busyboxplus:curl imagePullPolicy: IfNotPresent name: curl resources: {} stdin: true terminationMessagePath: /dev/termination-log terminationMessagePolicy: File tty: true EOF","title":"Creating a pod as an event producer"},{"location":"getting-started/getting-started-eventing/#sending-events-to-the-broker","text":"SSH into the pod by running the following command: kubectl -n event-example attach curl -it You will see a prompt similar to the following: Defaulting container name to curl. Use 'kubectl describe pod/ -n event-example' to see all of the containers in this pod. If you don't see a command prompt, try pressing enter. [ root@curl:/ ]$ Make a HTTP request to the broker. To show the various types of events you can send, you will make three requests: To make the first request, which creates an event that has the type greeting , run the following in the SSH terminal: curl -v \"http://broker-ingress.knative-eventing.svc.cluster.local/event-example/default\" \\ -X POST \\ -H \"Ce-Id: say-hello\" \\ -H \"Ce-Specversion: 1.0\" \\ -H \"Ce-Type: greeting\" \\ -H \"Ce-Source: not-sendoff\" \\ -H \"Content-Type: application/json\" \\ -d '{\"msg\":\"Hello Knative!\"}' When the broker receives your event, hello-display will activate and send it to the event consumer of the same name. If the event has been received, you will receive a 202 Accepted response similar to the one below: < HTTP/1.1 202 Accepted < Content-Length: 0 < Date: Mon, 12 Aug 2019 19:48:18 GMT - To make the second request, which creates an event that has the source sendoff , run the following in the SSH terminal: curl -v \"http://broker-ingress.knative-eventing.svc.cluster.local/event-example/default\" \\ -X POST \\ -H \"Ce-Id: say-goodbye\" \\ -H \"Ce-Specversion: 1.0\" \\ -H \"Ce-Type: not-greeting\" \\ -H \"Ce-Source: sendoff\" \\ -H \"Content-Type: application/json\" \\ -d '{\"msg\":\"Goodbye Knative!\"}' When the broker receives your event, goodbye-display will activate and send the event to the event consumer of the same name. If the event has been received, you will receive a 202 Accepted response similar to the one below: < HTTP/1.1 202 Accepted < Content-Length: 0 < Date: Mon, 12 Aug 2019 19:48:18 GMT - To make the third request, which creates an event that has the type greeting and the source sendoff , run the following in the SSH terminal: curl -v \"http://broker-ingress.knative-eventing.svc.cluster.local/event-example/default\" \\ -X POST \\ -H \"Ce-Id: say-hello-goodbye\" \\ -H \"Ce-Specversion: 1.0\" \\ -H \"Ce-Type: greeting\" \\ -H \"Ce-Source: sendoff\" \\ -H \"Content-Type: application/json\" \\ -d '{\"msg\":\"Hello Knative! Goodbye Knative!\"}' When the broker receives your event, hello-display and goodbye-display will activate and send the event to the event consumers of the same name. If the event has been received, you will receive a 202 Accepted response similar to the one below: < HTTP/1.1 202 Accepted < Content-Length: 0 < Date: Mon, 12 Aug 2019 19:48:18 GMT Exit SSH by typing exit into the command prompt. You have sent two events to the hello-display event consumer and two events to the goodbye-display event consumer (note that say-hello-goodbye activates the trigger conditions for both hello-display and goodbye-display ). You will verify that these events were received correctly in the next section.","title":"Sending events to the broker"},{"location":"getting-started/getting-started-eventing/#verifying-that-events-were-received","text":"After you send the events, verify that the events were received by the correct subscribers. Look at the logs for the hello-display event consumer by entering the following command: kubectl -n event-example logs -l app=hello-display --tail=100 This returns the Attributes and Data of the events you sent to hello-display : \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1.0 type: greeting source: not-sendoff id: say-hello time: 2019-05-20T17:59:43.81718488Z contenttype: application/json Extensions, knativehistory: default-broker-srk54-channel-24gls.event-example.svc.cluster.local Data, { \"msg\": \"Hello Knative!\" } \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1.0 type: greeting source: sendoff id: say-hello-goodbye time: 2019-05-20T17:59:54.211866425Z contenttype: application/json Extensions, knativehistory: default-broker-srk54-channel-24gls.event-example.svc.cluster.local Data, { \"msg\": \"Hello Knative! Goodbye Knative!\" } Look at the logs for the goodbye-display event consumer by entering the following command: kubectl -n event-example logs -l app=goodbye-display --tail=100 This returns the Attributes and Data of the events you sent to goodbye-display : \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1.0 type: not-greeting source: sendoff id: say-goodbye time: 2019-05-20T17:59:49.044926148Z contenttype: application/json Extensions, knativehistory: default-broker-srk54-channel-24gls.event-example.svc.cluster.local Data, { \"msg\": \"Goodbye Knative!\" } \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1.0 type: greeting source: sendoff id: say-hello-goodbye time: 2019-05-20T17:59:54.211866425Z contenttype: application/json Extensions, knativehistory: default-broker-srk54-channel-24gls.event-example.svc.cluster.local Data, { \"msg\": \"Hello Knative! Goodbye Knative!\" }","title":"Verifying that events were received"},{"location":"getting-started/getting-started-eventing/#cleaning-up-example-resources","text":"You can delete the event-example namespace and its associated resources from your cluster if you do not plan to use it again in the future. Delete the event-example namespace and all of its resources from your cluster by entering the following command: kubectl delete namespace event-example","title":"Cleaning up example resources"},{"location":"getting-started/getting-started/","text":"Installing Knative with KonK (Knative on Kind) \u00b6 The fastest way to get started with Knative locally is to use the Konk (Knative-on-kind) script. This script installs a local Knative deployment on your local machine's Docker daemon using Kind . Pre-Requisites \u00b6 You will need to have Kind installed . Install Knative and Kubernetes on a local Docker Daemon using Konk curl -sL install.konk.dev | bash Need to upgrade Kind? brew upgrade kind Or follow the \"Kind\" link in the Pre-Requisites above Other Installation Options \u00b6 If you would like to customize the Knative installation or install it on a remote cluster, start by Checking the Pre-Requisites then come back here to finish the remaining steps! Warning You will need networking layer and DNS for Knative Serving as well as a broker for Knative Eventing if you want to follow this tutorial.","title":"Installing Knative Serving and Eventing"},{"location":"getting-started/getting-started/#installing-knative-with-konk-knative-on-kind","text":"The fastest way to get started with Knative locally is to use the Konk (Knative-on-kind) script. This script installs a local Knative deployment on your local machine's Docker daemon using Kind .","title":"Installing Knative with KonK (Knative on Kind)"},{"location":"getting-started/getting-started/#pre-requisites","text":"You will need to have Kind installed . Install Knative and Kubernetes on a local Docker Daemon using Konk curl -sL install.konk.dev | bash Need to upgrade Kind? brew upgrade kind Or follow the \"Kind\" link in the Pre-Requisites above","title":"Pre-Requisites"},{"location":"getting-started/getting-started/#other-installation-options","text":"If you would like to customize the Knative installation or install it on a remote cluster, start by Checking the Pre-Requisites then come back here to finish the remaining steps! Warning You will need networking layer and DNS for Knative Serving as well as a broker for Knative Eventing if you want to follow this tutorial.","title":"Other Installation Options"},{"location":"getting-started/install-cli/","text":"Installing a Command Line Interface (CLI) Tools \u00b6 (Required) kubectl \u00b6 You can use kubectl to apply the YAML files required to install Knative components, and also to create Knative resources, such as Knative Services and event sources using YAML. Installing the kubectl CLI Using Homebrew If you are on macOS and using Homebrew package manager, you can install kubectl with Homebrew. brew install kubectl From the Kubernetes Website See Install and Set Up kubectl . (Recommended) kn \u00b6 kn provides a quick and easy interface for creating Knative resources such as Knative Services and event sources, without the need to create or modify YAML files directly. kn also simplifies completion of otherwise complex procedures such as autoscaling and traffic splitting. Installing the kn CLI Using Homebrew For macOS, you can install kn by using Homebrew . brew install knative/client/kn Using a binary You can install kn by downloading the executable binary for your system and placing it in the system path. A link to the latest stable binary release is available on the kn release page . Installing kn using Go Check out the kn client repository: git clone https://github.com/knative/client.git cd client/ Build an executable binary: hack/build.sh -f Move kn into your system path, and verify that kn commands are working properly. For example: kn version Running kn using container images WARNING: Nightly container images include features which may not be included in the latest Knative release and are not considered to be stable. Links to images are available here: Latest release Nightly container image You can run kn from a container image. For example: docker run --rm -v \"$HOME/.kube/config:/root/.kube/config\" gcr.io/knative-releases/knative.dev/client/cmd/kn:latest service list NOTE: Running kn from a container image does not place the binary on a permanent path. This procedure must be repeated each time you want to use kn . Tip For more complex installations, such as nightly releases, see Install kn","title":"Installing a CLI"},{"location":"getting-started/install-cli/#installing-a-command-line-interface-cli-tools","text":"","title":"Installing a Command Line Interface (CLI) Tools"},{"location":"getting-started/install-cli/#required-kubectl","text":"You can use kubectl to apply the YAML files required to install Knative components, and also to create Knative resources, such as Knative Services and event sources using YAML. Installing the kubectl CLI Using Homebrew If you are on macOS and using Homebrew package manager, you can install kubectl with Homebrew. brew install kubectl From the Kubernetes Website See Install and Set Up kubectl .","title":"(Required) kubectl"},{"location":"getting-started/install-cli/#recommended-kn","text":"kn provides a quick and easy interface for creating Knative resources such as Knative Services and event sources, without the need to create or modify YAML files directly. kn also simplifies completion of otherwise complex procedures such as autoscaling and traffic splitting. Installing the kn CLI Using Homebrew For macOS, you can install kn by using Homebrew . brew install knative/client/kn Using a binary You can install kn by downloading the executable binary for your system and placing it in the system path. A link to the latest stable binary release is available on the kn release page . Installing kn using Go Check out the kn client repository: git clone https://github.com/knative/client.git cd client/ Build an executable binary: hack/build.sh -f Move kn into your system path, and verify that kn commands are working properly. For example: kn version Running kn using container images WARNING: Nightly container images include features which may not be included in the latest Knative release and are not considered to be stable. Links to images are available here: Latest release Nightly container image You can run kn from a container image. For example: docker run --rm -v \"$HOME/.kube/config:/root/.kube/config\" gcr.io/knative-releases/knative.dev/client/cmd/kn:latest service list NOTE: Running kn from a container image does not place the binary on a permanent path. This procedure must be repeated each time you want to use kn . Tip For more complex installations, such as nightly releases, see Install kn","title":"(Recommended) kn"},{"location":"getting-started/next-steps/","text":"Next Steps \u00b6 //TODO","title":"Next Steps"},{"location":"getting-started/next-steps/#next-steps","text":"//TODO","title":"Next Steps"},{"location":"install/","text":"Installing Knative \u00b6 Tip If you're looking for an easy way to install a local distribution of Knative for prototyping, check out our Quick Install w/ KonK You can install the Serving component, Eventing component, or both on your cluster by using one of the following deployment options: Using a YAML-based installation Using the Knative Operator . Following the documentation for vendor managed Knative offerings . You can also upgrade an existing Knative installation . NOTE: Knative installation instructions assume you are running Mac or Linux with a bash shell. Next steps \u00b6 Install the Knative CLI to use kn commands.","title":"Installing Knative"},{"location":"install/#installing-knative","text":"Tip If you're looking for an easy way to install a local distribution of Knative for prototyping, check out our Quick Install w/ KonK You can install the Serving component, Eventing component, or both on your cluster by using one of the following deployment options: Using a YAML-based installation Using the Knative Operator . Following the documentation for vendor managed Knative offerings . You can also upgrade an existing Knative installation . NOTE: Knative installation instructions assume you are running Mac or Linux with a bash shell.","title":"Installing Knative"},{"location":"install/#next-steps","text":"Install the Knative CLI to use kn commands.","title":"Next steps"},{"location":"install/check-install-version/","text":"Macro Rendering Error \u00b6 TemplateSyntaxError : expected token 'end of print statement', got 'string' Traceback (most recent call last): File \"/usr/local/lib/python3.9/site-packages/mkdocs_macros/plugin.py\", line 441, in render md_template = self.env.from_string(markdown) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 941, in from_string return cls.from_code(self, self.compile(source), globals, None) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 638, in compile self.handle_exception(source=source_hint) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 832, in handle_exception reraise(*rewrite_traceback_stack(source=source)) File \"/usr/local/lib/python3.9/site-packages/jinja2/_compat.py\", line 28, in reraise raise value.with_traceback(tb) File \"<unknown>\", line 9, in template jinja2.exceptions.TemplateSyntaxError: expected token 'end of print statement', got 'string'","title":"Checking the version of your Knative components"},{"location":"install/check-install-version/#macro-rendering-error","text":"TemplateSyntaxError : expected token 'end of print statement', got 'string' Traceback (most recent call last): File \"/usr/local/lib/python3.9/site-packages/mkdocs_macros/plugin.py\", line 441, in render md_template = self.env.from_string(markdown) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 941, in from_string return cls.from_code(self, self.compile(source), globals, None) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 638, in compile self.handle_exception(source=source_hint) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 832, in handle_exception reraise(*rewrite_traceback_stack(source=source)) File \"/usr/local/lib/python3.9/site-packages/jinja2/_compat.py\", line 28, in reraise raise value.with_traceback(tb) File \"<unknown>\", line 9, in template jinja2.exceptions.TemplateSyntaxError: expected token 'end of print statement', got 'string'","title":"Macro Rendering Error"},{"location":"install/install-eventing-with-yaml/","text":"Installing Knative Eventing using YAML files \u00b6 This topic describes how to install Knative Eventing by applying YAML files using the kubectl CLI. Prerequisites \u00b6 Before installation, you must meet the prerequisites. See Knative Prerequisites . Install the Eventing component \u00b6 To install the Eventing component: Install the required custom resource definitions (CRDs): kubectl apply -f http://github.com/knative/eventing/releases/download/v0.21.0/eventing-crds.yaml Install the core components of Eventing: kubectl apply -f http://github.com/knative/eventing/releases/download/v0.21.0/eventing-core.yaml Info For information about the YAML files in the Knative Serving and Eventing releases, see Installation files . Verify the installation \u00b6 Monitor the Knative components until all of the components show a STATUS of Running : kubectl get pods --namespace knative-eventing Optional: Install a default channel (messaging) layer \u00b6 The tabs below expand to show instructions for installing a default channel layer. Follow the procedure for the channel of your choice: Apache Kafka Channel First, Install Apache Kafka for Kubernetes Then install the Apache Kafka channel: curl -L \"http://github.com/knative-sandbox/eventing-kafka/releases/download/v0.21.0/channel-consolidated.yaml\" \\ | sed 's/REPLACE_WITH_CLUSTER_URL/my-cluster-kafka-bootstrap.kafka:9092/' \\ | kubectl apply -f - Tip To learn more about the Apache Kafka channel, try our sample Google Cloud Pub/Sub Channel Install the Google Cloud Pub/Sub channel: # This installs both the Channel and the GCP Sources. kubectl apply -f http://github.com/google/knative-gcp/releases/download/v0.21.0/cloud-run-events.yaml Tip To learn more about the Google Cloud Pub/Sub channel, try our sample In-Memory (standalone) The following command installs an implementation of channel that runs in-memory. This implementation is nice because it is simple and standalone, but it is kubectl apply -f http://github.com/knative/eventing/releases/download/v0.21.0/in-memory-channel.yaml NATS Channel First, Install NATS Streaming for Kubernetes Then install the NATS Streaming channel: kubectl apply -f http://github.com/knative-sandbox/eventing-natss/releases/download/v0.21.0/300-natss-channel.yaml Optional: Install a broker layer: \u00b6 The tabs below expand to show instructions for installing the broker layer. Follow the procedure for the broker of your choice: Apache Kafka Broker The following commands install the Apache Kafka broker, and run event routing in a system namespace, knative-eventing , by default. Install the Kafka controller by entering the following command: kubectl apply -f http://github.com/knative-sandbox/eventing-kafka-broker/releases/download/v0.21.0/eventing-kafka-controller.yaml Install the Kafka broker data plane by entering the following command: kubectl apply -f http://github.com/knative-sandbox/eventing-kafka-broker/releases/download/v0.21.0/eventing-kafka-broker.yaml For more information, see the Kafka broker documentation. MT-Channel-based The following command installs an implementation of broker that utilizes channels and runs event routing components in a System Namespace, providing a smaller and simpler installation. kubectl apply -f http://github.com/knative/eventing/releases/download/v0.21.0/mt-channel-broker.yaml To customize which broker channel implementation is used, update the following ConfigMap to specify which configurations are used for which namespaces: apiVersion : v1 kind : ConfigMap metadata : name : config-br-defaults namespace : knative-eventing data : default-br-config : | # This is the cluster-wide default broker channel. clusterDefault: brokerClass: MTChannelBasedBroker apiVersion: v1 kind: ConfigMap name: imc-channel namespace: knative-eventing # This allows you to specify different defaults per-namespace, # in this case the \"some-namespace\" namespace will use the Kafka # channel ConfigMap by default (only for example, you will need # to install kafka also to make use of this). namespaceDefaults: some-namespace: brokerClass: MTChannelBasedBroker apiVersion: v1 kind: ConfigMap name: kafka-channel namespace: knative-eventing The referenced imc-channel and kafka-channel example ConfigMaps would look like: apiVersion : v1 kind : ConfigMap metadata : name : imc-channel namespace : knative-eventing data : channelTemplateSpec : | apiVersion: messaging.knative.dev/v1 kind: InMemoryChannel --- apiVersion : v1 kind : ConfigMap metadata : name : kafka-channel namespace : knative-eventing data : channelTemplateSpec : | apiVersion: messaging.knative.dev/v1alpha1 kind: KafkaChannel spec: numPartitions: 3 replicationFactor: 1 Warning In order to use the KafkaChannel make sure it is installed on the cluster as discussed above. Next steps \u00b6 After installing Knative Eventing: To easily interact with Knative Eventing components, install the kn CLI To add optional enhancements to your installation, see Installing optional extensions Installing Knative Serving using YAML files","title":"Install Eventing with YAML"},{"location":"install/install-eventing-with-yaml/#installing-knative-eventing-using-yaml-files","text":"This topic describes how to install Knative Eventing by applying YAML files using the kubectl CLI.","title":"Installing Knative Eventing using YAML files"},{"location":"install/install-eventing-with-yaml/#prerequisites","text":"Before installation, you must meet the prerequisites. See Knative Prerequisites .","title":"Prerequisites"},{"location":"install/install-eventing-with-yaml/#install-the-eventing-component","text":"To install the Eventing component: Install the required custom resource definitions (CRDs): kubectl apply -f http://github.com/knative/eventing/releases/download/v0.21.0/eventing-crds.yaml Install the core components of Eventing: kubectl apply -f http://github.com/knative/eventing/releases/download/v0.21.0/eventing-core.yaml Info For information about the YAML files in the Knative Serving and Eventing releases, see Installation files .","title":"Install the Eventing component"},{"location":"install/install-eventing-with-yaml/#verify-the-installation","text":"Monitor the Knative components until all of the components show a STATUS of Running : kubectl get pods --namespace knative-eventing","title":"Verify the installation"},{"location":"install/install-eventing-with-yaml/#optional-install-a-default-channel-messaging-layer","text":"The tabs below expand to show instructions for installing a default channel layer. Follow the procedure for the channel of your choice: Apache Kafka Channel First, Install Apache Kafka for Kubernetes Then install the Apache Kafka channel: curl -L \"http://github.com/knative-sandbox/eventing-kafka/releases/download/v0.21.0/channel-consolidated.yaml\" \\ | sed 's/REPLACE_WITH_CLUSTER_URL/my-cluster-kafka-bootstrap.kafka:9092/' \\ | kubectl apply -f - Tip To learn more about the Apache Kafka channel, try our sample Google Cloud Pub/Sub Channel Install the Google Cloud Pub/Sub channel: # This installs both the Channel and the GCP Sources. kubectl apply -f http://github.com/google/knative-gcp/releases/download/v0.21.0/cloud-run-events.yaml Tip To learn more about the Google Cloud Pub/Sub channel, try our sample In-Memory (standalone) The following command installs an implementation of channel that runs in-memory. This implementation is nice because it is simple and standalone, but it is kubectl apply -f http://github.com/knative/eventing/releases/download/v0.21.0/in-memory-channel.yaml NATS Channel First, Install NATS Streaming for Kubernetes Then install the NATS Streaming channel: kubectl apply -f http://github.com/knative-sandbox/eventing-natss/releases/download/v0.21.0/300-natss-channel.yaml","title":"Optional: Install a default channel (messaging) layer"},{"location":"install/install-eventing-with-yaml/#optional-install-a-broker-layer","text":"The tabs below expand to show instructions for installing the broker layer. Follow the procedure for the broker of your choice: Apache Kafka Broker The following commands install the Apache Kafka broker, and run event routing in a system namespace, knative-eventing , by default. Install the Kafka controller by entering the following command: kubectl apply -f http://github.com/knative-sandbox/eventing-kafka-broker/releases/download/v0.21.0/eventing-kafka-controller.yaml Install the Kafka broker data plane by entering the following command: kubectl apply -f http://github.com/knative-sandbox/eventing-kafka-broker/releases/download/v0.21.0/eventing-kafka-broker.yaml For more information, see the Kafka broker documentation. MT-Channel-based The following command installs an implementation of broker that utilizes channels and runs event routing components in a System Namespace, providing a smaller and simpler installation. kubectl apply -f http://github.com/knative/eventing/releases/download/v0.21.0/mt-channel-broker.yaml To customize which broker channel implementation is used, update the following ConfigMap to specify which configurations are used for which namespaces: apiVersion : v1 kind : ConfigMap metadata : name : config-br-defaults namespace : knative-eventing data : default-br-config : | # This is the cluster-wide default broker channel. clusterDefault: brokerClass: MTChannelBasedBroker apiVersion: v1 kind: ConfigMap name: imc-channel namespace: knative-eventing # This allows you to specify different defaults per-namespace, # in this case the \"some-namespace\" namespace will use the Kafka # channel ConfigMap by default (only for example, you will need # to install kafka also to make use of this). namespaceDefaults: some-namespace: brokerClass: MTChannelBasedBroker apiVersion: v1 kind: ConfigMap name: kafka-channel namespace: knative-eventing The referenced imc-channel and kafka-channel example ConfigMaps would look like: apiVersion : v1 kind : ConfigMap metadata : name : imc-channel namespace : knative-eventing data : channelTemplateSpec : | apiVersion: messaging.knative.dev/v1 kind: InMemoryChannel --- apiVersion : v1 kind : ConfigMap metadata : name : kafka-channel namespace : knative-eventing data : channelTemplateSpec : | apiVersion: messaging.knative.dev/v1alpha1 kind: KafkaChannel spec: numPartitions: 3 replicationFactor: 1 Warning In order to use the KafkaChannel make sure it is installed on the cluster as discussed above.","title":"Optional: Install a broker layer:"},{"location":"install/install-eventing-with-yaml/#next-steps","text":"After installing Knative Eventing: To easily interact with Knative Eventing components, install the kn CLI To add optional enhancements to your installation, see Installing optional extensions Installing Knative Serving using YAML files","title":"Next steps"},{"location":"install/install-extensions/","text":"Installing optional extensions \u00b6 To add extra features to your Knative Serving or Eventing installation, you can install extensions by applying YAML files using the kubectl CLI. For information about the YAML files in the Knative Serving and Eventing releases, see Installation files . Prerequisites \u00b6 Before you install any optional extensions, you must install Knative Serving or Eventing. See Installing Serving using YAML files and Installing Eventing using YAML files . Install optional Serving extensions \u00b6 The tabs below expand to show instructions for installing each Serving extension. HPA autoscaling Knative also supports the use of the Kubernetes Horizontal Pod Autoscaler (HPA) for driving autoscaling decisions. The following command will install the components needed to support HPA-class autoscaling: kubectl apply -f http://github.com/knative/serving/releases/download/v0.21.0/serving-hpa.yaml TLS with cert-manager Knative supports automatically provisioning TLS certificates via cert-manager . The following commands will install the components needed to support the provisioning of TLS certificates via cert-manager. First, install cert-manager version 0.12.0 or higher Next, install the component that integrates Knative with cert-manager: kubectl apply -f http://github.com/knative/net-certmanager/releases/download/v0.21.0/release.yaml Now configure Knative to automatically configure TLS certificates . TLS via HTTP01 Knative supports automatically provisioning TLS certificates using Let's Encrypt HTTP01 challenges. The following commands will install the components needed to support that. First, install the net-http01 controller: kubectl apply -f http://github.com/knative/net-http01/releases/download/v0.21.0/release.yaml Next, configure the certificate.class to use this certificate type. kubectl patch configmap/config-network \\ --namespace knative-serving \\ --type merge \\ --patch '{\"data\":{\"certificate.class\":\"net-http01.certificate.networking.knative.dev\"}}' Lastly, enable auto-TLS. kubectl patch configmap/config-network \\ --namespace knative-serving \\ --type merge \\ --patch '{\"data\":{\"autoTLS\":\"Enabled\"}}' TLS wildcard support If you are using a Certificate implementation that supports provisioning wildcard certificates (e.g. cert-manager with a DNS01 issuer), then the most efficient way to provision certificates is with the namespace wildcard certificate controller. The following command will install the components needed to provision wildcard certificates in each namespace: kubectl apply -f http://github.com/knative/serving/releases/download/v0.21.0/serving-nscert.yaml Warning Note this will not work with HTTP01 either via cert-manager or the net-http01 options. DomainMapping CRD The DomainMapping CRD allows a user to map a Domain Name that they own to a specific Knative Service. kubectl apply -f http://github.com/knative/serving/releases/download/v0.21.0/serving-domainmapping-crds.yaml kubectl apply -f http://github.com/knative/serving/releases/download/v0.21.0/serving-domainmapping.yaml Install optional Eventing extensions \u00b6 The tabs below expand to show instructions for installing each Eventing extension. Apache Kafka Sink Install the Kafka controller: kubectl apply -f http://github.com/knative-sandbox/eventing-kafka-broker/releases/download/v0.21.0/eventing-kafka-controller.yaml Install the Kafka Sink data plane: kubectl apply -f http://github.com/knative-sandbox/eventing-kafka-broker/releases/download/v0.21.0/eventing-kafka-sink.yaml For more information, see the Kafka Sink documentation. Sugar Controller The following command installs the Eventing Sugar Controller: kubectl apply -f http://github.com/knative/eventing/releases/download/v0.21.0/eventing-sugar-controller.yaml The Knative Eventing Sugar Controller will react to special labels and annotations and produce Eventing resources. For example: When a Namespace is labeled with eventing.knative.dev/injection=enabled , the controller will create a default broker in that namespace. When a Trigger is annotated with eventing.knative.dev/injection=enabled , the controller will create a Broker named by that Trigger in the Trigger's Namespace. The following command enables the default Broker on a namespace (here default ): kubectl label namespace default eventing.knative.dev/injection = enabled Github Source The following command installs the single-tenant Github source: kubectl apply -f http://github.com/knative-sandbox/eventing-github/releases/download/v0.21.0/github.yaml The single-tenant GitHub source creates one Knative service per GitHub source. The following command installs the multi-tenant GitHub source: kubectl apply -f http://github.com/knative-sandbox/eventing-github/releases/download/v0.21.0/mt-github.yaml The multi-tenant GitHub source creates only one Knative service handling all GitHub sources in the cluster. This source does not support logging or tracing configuration yet. To learn more about the Github source, try our sample Apache Camel-K Source The following command installs the Apache Camel-K Source: kubectl apply -f http://github.com/knative-sandbox/eventing-camel/releases/download/v0.21.0/camel.yaml To learn more about the Apache Camel-K source, try our sample Apache Kafka Source The following command installs the Apache Kafka Source: kubectl apply -f http://github.com/knative-sandbox/eventing-kafka/releases/download/v0.21.0/source.yaml To learn more about the Apache Kafka source, try our sample GCP Sources The following command installs the GCP Sources: # This installs both the Sources and the Channel. kubectl apply -f http://github.com/google/knative-gcp/releases/download/v0.21.0/cloud-run-events.yaml To learn more about the Cloud Pub/Sub source, try our sample . To learn more about the Cloud Storage source, try our sample . To learn more about the Cloud Scheduler source, try our sample . To learn more about the Cloud Audit Logs source, try our sample . Apache CouchDB Source The following command installs the Apache CouchDB Source: kubectl apply -f http://github.com/knative-sandbox/eventing-couchdb/releases/download/v0.21.0/couchdb.yaml To learn more about the Apache CouchDB source, read the documentation . VMware Sources and Bindings The following command installs the VMware Sources and Bindings: kubectl apply -f http://github.com/vmware-tanzu/sources-for-knative/releases/download/v0.21.0/release.yaml To learn more about the VMware sources and bindings, try our samples . Next steps \u00b6 To easily interact with Knative Services and Eventing components, install the kn CLI","title":"Install optional extensions"},{"location":"install/install-extensions/#installing-optional-extensions","text":"To add extra features to your Knative Serving or Eventing installation, you can install extensions by applying YAML files using the kubectl CLI. For information about the YAML files in the Knative Serving and Eventing releases, see Installation files .","title":"Installing optional extensions"},{"location":"install/install-extensions/#prerequisites","text":"Before you install any optional extensions, you must install Knative Serving or Eventing. See Installing Serving using YAML files and Installing Eventing using YAML files .","title":"Prerequisites"},{"location":"install/install-extensions/#install-optional-serving-extensions","text":"The tabs below expand to show instructions for installing each Serving extension. HPA autoscaling Knative also supports the use of the Kubernetes Horizontal Pod Autoscaler (HPA) for driving autoscaling decisions. The following command will install the components needed to support HPA-class autoscaling: kubectl apply -f http://github.com/knative/serving/releases/download/v0.21.0/serving-hpa.yaml TLS with cert-manager Knative supports automatically provisioning TLS certificates via cert-manager . The following commands will install the components needed to support the provisioning of TLS certificates via cert-manager. First, install cert-manager version 0.12.0 or higher Next, install the component that integrates Knative with cert-manager: kubectl apply -f http://github.com/knative/net-certmanager/releases/download/v0.21.0/release.yaml Now configure Knative to automatically configure TLS certificates . TLS via HTTP01 Knative supports automatically provisioning TLS certificates using Let's Encrypt HTTP01 challenges. The following commands will install the components needed to support that. First, install the net-http01 controller: kubectl apply -f http://github.com/knative/net-http01/releases/download/v0.21.0/release.yaml Next, configure the certificate.class to use this certificate type. kubectl patch configmap/config-network \\ --namespace knative-serving \\ --type merge \\ --patch '{\"data\":{\"certificate.class\":\"net-http01.certificate.networking.knative.dev\"}}' Lastly, enable auto-TLS. kubectl patch configmap/config-network \\ --namespace knative-serving \\ --type merge \\ --patch '{\"data\":{\"autoTLS\":\"Enabled\"}}' TLS wildcard support If you are using a Certificate implementation that supports provisioning wildcard certificates (e.g. cert-manager with a DNS01 issuer), then the most efficient way to provision certificates is with the namespace wildcard certificate controller. The following command will install the components needed to provision wildcard certificates in each namespace: kubectl apply -f http://github.com/knative/serving/releases/download/v0.21.0/serving-nscert.yaml Warning Note this will not work with HTTP01 either via cert-manager or the net-http01 options. DomainMapping CRD The DomainMapping CRD allows a user to map a Domain Name that they own to a specific Knative Service. kubectl apply -f http://github.com/knative/serving/releases/download/v0.21.0/serving-domainmapping-crds.yaml kubectl apply -f http://github.com/knative/serving/releases/download/v0.21.0/serving-domainmapping.yaml","title":"Install optional Serving extensions"},{"location":"install/install-extensions/#install-optional-eventing-extensions","text":"The tabs below expand to show instructions for installing each Eventing extension. Apache Kafka Sink Install the Kafka controller: kubectl apply -f http://github.com/knative-sandbox/eventing-kafka-broker/releases/download/v0.21.0/eventing-kafka-controller.yaml Install the Kafka Sink data plane: kubectl apply -f http://github.com/knative-sandbox/eventing-kafka-broker/releases/download/v0.21.0/eventing-kafka-sink.yaml For more information, see the Kafka Sink documentation. Sugar Controller The following command installs the Eventing Sugar Controller: kubectl apply -f http://github.com/knative/eventing/releases/download/v0.21.0/eventing-sugar-controller.yaml The Knative Eventing Sugar Controller will react to special labels and annotations and produce Eventing resources. For example: When a Namespace is labeled with eventing.knative.dev/injection=enabled , the controller will create a default broker in that namespace. When a Trigger is annotated with eventing.knative.dev/injection=enabled , the controller will create a Broker named by that Trigger in the Trigger's Namespace. The following command enables the default Broker on a namespace (here default ): kubectl label namespace default eventing.knative.dev/injection = enabled Github Source The following command installs the single-tenant Github source: kubectl apply -f http://github.com/knative-sandbox/eventing-github/releases/download/v0.21.0/github.yaml The single-tenant GitHub source creates one Knative service per GitHub source. The following command installs the multi-tenant GitHub source: kubectl apply -f http://github.com/knative-sandbox/eventing-github/releases/download/v0.21.0/mt-github.yaml The multi-tenant GitHub source creates only one Knative service handling all GitHub sources in the cluster. This source does not support logging or tracing configuration yet. To learn more about the Github source, try our sample Apache Camel-K Source The following command installs the Apache Camel-K Source: kubectl apply -f http://github.com/knative-sandbox/eventing-camel/releases/download/v0.21.0/camel.yaml To learn more about the Apache Camel-K source, try our sample Apache Kafka Source The following command installs the Apache Kafka Source: kubectl apply -f http://github.com/knative-sandbox/eventing-kafka/releases/download/v0.21.0/source.yaml To learn more about the Apache Kafka source, try our sample GCP Sources The following command installs the GCP Sources: # This installs both the Sources and the Channel. kubectl apply -f http://github.com/google/knative-gcp/releases/download/v0.21.0/cloud-run-events.yaml To learn more about the Cloud Pub/Sub source, try our sample . To learn more about the Cloud Storage source, try our sample . To learn more about the Cloud Scheduler source, try our sample . To learn more about the Cloud Audit Logs source, try our sample . Apache CouchDB Source The following command installs the Apache CouchDB Source: kubectl apply -f http://github.com/knative-sandbox/eventing-couchdb/releases/download/v0.21.0/couchdb.yaml To learn more about the Apache CouchDB source, read the documentation . VMware Sources and Bindings The following command installs the VMware Sources and Bindings: kubectl apply -f http://github.com/vmware-tanzu/sources-for-knative/releases/download/v0.21.0/release.yaml To learn more about the VMware sources and bindings, try our samples .","title":"Install optional Eventing extensions"},{"location":"install/install-extensions/#next-steps","text":"To easily interact with Knative Services and Eventing components, install the kn CLI","title":"Next steps"},{"location":"install/install-serving-with-yaml/","text":"Installing Knative Serving using YAML files \u00b6 This topic describes how to install Knative Serving by applying YAML files using the kubectl CLI. Prerequisites \u00b6 Before installation, you must meet the prerequisites. See Knative Prerequisites . Install the Serving component \u00b6 To install the serving component: Install the required custom resources: kubectl apply -f http://github.com/knative/serving/releases/download/v0.21.0/serving-crds.yaml Install the core components of Knative Serving: kubectl apply -f http://github.com/knative/serving/releases/download/v0.21.0/serving-core.yaml Info For information about the YAML files in the Knative Serving and Eventing releases, see Installation files . Install a networking layer \u00b6 The tabs below expand to show instructions for installing a networking layer. Follow the procedure for the networking layer of your choice: Kourier (Choose this if you are not sure) The following commands install Kourier and enable its Knative integration. Install the Knative Kourier controller: kubectl apply -f http://github.com/knative/net-kourier/releases/download/v0.21.0/kourier.yaml To configure Knative Serving to use Kourier by default: kubectl patch configmap/config-network \\ --namespace knative-serving \\ --type merge \\ --patch '{\"data\":{\"ingress.class\":\"kourier.ingress.networking.knative.dev\"}}' Fetch the External IP or CNAME: kubectl --namespace kourier-system get service kourier Tip Save this to use in the Configure DNS section. Ambassador The following commands install Ambassador and enable its Knative integration. Create a namespace to install Ambassador in: kubectl create namespace ambassador Install Ambassador: kubectl apply --namespace ambassador \\ -f https://getambassador.io/yaml/ambassador/ambassador-crds.yaml \\ -f https://getambassador.io/yaml/ambassador/ambassador-rbac.yaml \\ -f https://getambassador.io/yaml/ambassador/ambassador-service.yaml Give Ambassador the required permissions: kubectl patch clusterrolebinding ambassador -p '{\"subjects\":[{\"kind\": \"ServiceAccount\", \"name\": \"ambassador\", \"namespace\": \"ambassador\"}]}' Enable Knative support in Ambassador: kubectl set env --namespace ambassador deployments/ambassador AMBASSADOR_KNATIVE_SUPPORT = true To configure Knative Serving to use Ambassador by default: kubectl patch configmap/config-network \\ --namespace knative-serving \\ --type merge \\ --patch '{\"data\":{\"ingress.class\":\"ambassador.ingress.networking.knative.dev\"}}' Fetch the External IP or CNAME: kubectl --namespace ambassador get service ambassador Tip Save this to use in the Configure DNS section. Contour The following commands install Contour and enable its Knative integration. Install a properly configured Contour: kubectl apply -f http://github.com/knative/net-contour/releases/download/v0.21.0/contour.yaml Install the Knative Contour controller: kubectl apply -f http://github.com/knative/net-contour/releases/download/v0.21.0/net-contour.yaml To configure Knative Serving to use Contour by default: kubectl patch configmap/config-network \\ --namespace knative-serving \\ --type merge \\ --patch '{\"data\":{\"ingress.class\":\"contour.ingress.networking.knative.dev\"}}' Fetch the External IP or CNAME: kubectl --namespace contour-external get service envoy Tip Save this to use in the Configure DNS section. Gloo For a detailed guide on Gloo integration, see Installing Gloo for Knative in the Gloo documentation. The following commands install Gloo and enable its Knative integration. Make sure glooctl is installed (version 1.3.x and higher recommended): glooctl version If it is not installed, you can install the latest version using: curl -sL https://run.solo.io/gloo/install | sh export PATH = $HOME /.gloo/bin: $PATH Or following the Gloo CLI install instructions . Install Gloo and the Knative integration: glooctl install knative --install-knative = false Fetch the External IP or CNAME: glooctl proxy url --name knative-external-proxy Tip Save this to use in the Configure DNS section. Istio The following commands install Istio and enable its Knative integration. Install a properly configured Istio ( Advanced installation ) kubectl apply -f http://github.com/knative/net-istio/releases/download/v0.21.0/istio.yaml Install the Knative Istio controller: kubectl apply -f http://github.com/knative/net-istio/releases/download/v0.21.0/net-istio.yaml Fetch the External IP or CNAME: kubectl --namespace istio-system get service istio-ingressgateway Tip Save this to use in the Configure DNS section. Kong The following commands install Kong and enable its Knative integration. Install Kong Ingress Controller: kubectl apply -f https://raw.githubusercontent.com/Kong/kubernetes-ingress-controller/0.9.x/deploy/single/all-in-one-dbless.yaml To configure Knative Serving to use Kong by default: kubectl patch configmap/config-network \\ --namespace knative-serving \\ --type merge \\ --patch '{\"data\":{\"ingress.class\":\"kong\"}}' Fetch the External IP or CNAME: kubectl --namespace kong get service kong-proxy Tip Save this to use in the Configure DNS section. Verify the installation \u00b6 Monitor the Knative components until all of the components show a STATUS of Running or Completed : kubectl get pods --namespace knative-serving Configure DNS \u00b6 You can configure DNS to prevent the need to run curl commands with a host header. The tabs below expand to show instructions for configuring DNS. Follow the procedure for the DNS of your choice: Magic DNS (xip.io) We ship a simple Kubernetes Job called \"default domain\" that will (see caveats) configure Knative Serving to use xip.io as the default DNS suffix. kubectl apply -f http://github.com/knative/serving/releases/download/v0.21.0/serving-default-domain.yaml CAVEAT This will only work if the cluster LoadBalancer service exposes an IPv4 address or hostname, so it will not work with IPv6 clusters or local setups like Minikube. For these, see \"Real DNS\" or \"Temporary DNS\". Real DNS To configure DNS for Knative, take the External IP or CNAME from setting up networking, and configure it with your DNS provider as follows: If the networking layer produced an External IP address, then configure a wildcard A record for the domain: # Here knative.example.com is the domain suffix for your cluster *.knative.example.com == A 35 .233.41.212 If the networking layer produced a CNAME, then configure a CNAME record for the domain: # Here knative.example.com is the domain suffix for your cluster *.knative.example.com == CNAME a317a278525d111e89f272a164fd35fb-1510370581.eu-central-1.elb.amazonaws.com Once your DNS provider has been configured, direct Knative to use that domain: # Replace knative.example.com with your domain suffix kubectl patch configmap/config-domain \\ --namespace knative-serving \\ --type merge \\ --patch '{\"data\":{\"knative.example.com\":\"\"}}' Temporary DNS Info If you are using curl to access the sample applications, or your own Knative app, and are unable to use the \"Magic DNS (xip.io)\" or \"Real DNS\" methods, there is a temporary approach. This is useful for those who wish to evaluate Knative without altering their DNS configuration, as per the \"Real DNS\" method, or cannot use the \"Magic DNS\" method due to using, for example, minikube locally or IPv6 clusters. To access your application using curl using this method: After starting your application, get the URL of your application: kubectl get ksvc Verify the output NAME URL LATESTCREATED LATESTREADY READY REASON helloworld-go http://helloworld-go.default.example.com helloworld-go-vqjlf helloworld-go-vqjlf True Instruct curl to connect to the External IP or CNAME defined by the networking layer in section 3 above, and use the -H \"Host:\" command-line option to specify the Knative application's host name. For example, if the networking layer defines your External IP and port to be http://192.168.39.228:32198 and you wish to access the above helloworld-go application, use: curl -H \"Host: helloworld-go.default.example.com\" http://192.168.39.228:32198 Verify the output In the case of the provided helloworld-go sample application, the output should, using the default configuration, be: Hello Go Sample v1! Refer to the \"Real DNS\" method for a permanent solution. Next steps \u00b6 After installing Knative Serving: Installing Knative Eventing using YAML files To add optional enhancements to your installation, see Installing optional extensions . To easily interact with Knative Services, install the kn CLI","title":"Install Serving with YAML"},{"location":"install/install-serving-with-yaml/#installing-knative-serving-using-yaml-files","text":"This topic describes how to install Knative Serving by applying YAML files using the kubectl CLI.","title":"Installing Knative Serving using YAML files"},{"location":"install/install-serving-with-yaml/#prerequisites","text":"Before installation, you must meet the prerequisites. See Knative Prerequisites .","title":"Prerequisites"},{"location":"install/install-serving-with-yaml/#install-the-serving-component","text":"To install the serving component: Install the required custom resources: kubectl apply -f http://github.com/knative/serving/releases/download/v0.21.0/serving-crds.yaml Install the core components of Knative Serving: kubectl apply -f http://github.com/knative/serving/releases/download/v0.21.0/serving-core.yaml Info For information about the YAML files in the Knative Serving and Eventing releases, see Installation files .","title":"Install the Serving component"},{"location":"install/install-serving-with-yaml/#install-a-networking-layer","text":"The tabs below expand to show instructions for installing a networking layer. Follow the procedure for the networking layer of your choice: Kourier (Choose this if you are not sure) The following commands install Kourier and enable its Knative integration. Install the Knative Kourier controller: kubectl apply -f http://github.com/knative/net-kourier/releases/download/v0.21.0/kourier.yaml To configure Knative Serving to use Kourier by default: kubectl patch configmap/config-network \\ --namespace knative-serving \\ --type merge \\ --patch '{\"data\":{\"ingress.class\":\"kourier.ingress.networking.knative.dev\"}}' Fetch the External IP or CNAME: kubectl --namespace kourier-system get service kourier Tip Save this to use in the Configure DNS section. Ambassador The following commands install Ambassador and enable its Knative integration. Create a namespace to install Ambassador in: kubectl create namespace ambassador Install Ambassador: kubectl apply --namespace ambassador \\ -f https://getambassador.io/yaml/ambassador/ambassador-crds.yaml \\ -f https://getambassador.io/yaml/ambassador/ambassador-rbac.yaml \\ -f https://getambassador.io/yaml/ambassador/ambassador-service.yaml Give Ambassador the required permissions: kubectl patch clusterrolebinding ambassador -p '{\"subjects\":[{\"kind\": \"ServiceAccount\", \"name\": \"ambassador\", \"namespace\": \"ambassador\"}]}' Enable Knative support in Ambassador: kubectl set env --namespace ambassador deployments/ambassador AMBASSADOR_KNATIVE_SUPPORT = true To configure Knative Serving to use Ambassador by default: kubectl patch configmap/config-network \\ --namespace knative-serving \\ --type merge \\ --patch '{\"data\":{\"ingress.class\":\"ambassador.ingress.networking.knative.dev\"}}' Fetch the External IP or CNAME: kubectl --namespace ambassador get service ambassador Tip Save this to use in the Configure DNS section. Contour The following commands install Contour and enable its Knative integration. Install a properly configured Contour: kubectl apply -f http://github.com/knative/net-contour/releases/download/v0.21.0/contour.yaml Install the Knative Contour controller: kubectl apply -f http://github.com/knative/net-contour/releases/download/v0.21.0/net-contour.yaml To configure Knative Serving to use Contour by default: kubectl patch configmap/config-network \\ --namespace knative-serving \\ --type merge \\ --patch '{\"data\":{\"ingress.class\":\"contour.ingress.networking.knative.dev\"}}' Fetch the External IP or CNAME: kubectl --namespace contour-external get service envoy Tip Save this to use in the Configure DNS section. Gloo For a detailed guide on Gloo integration, see Installing Gloo for Knative in the Gloo documentation. The following commands install Gloo and enable its Knative integration. Make sure glooctl is installed (version 1.3.x and higher recommended): glooctl version If it is not installed, you can install the latest version using: curl -sL https://run.solo.io/gloo/install | sh export PATH = $HOME /.gloo/bin: $PATH Or following the Gloo CLI install instructions . Install Gloo and the Knative integration: glooctl install knative --install-knative = false Fetch the External IP or CNAME: glooctl proxy url --name knative-external-proxy Tip Save this to use in the Configure DNS section. Istio The following commands install Istio and enable its Knative integration. Install a properly configured Istio ( Advanced installation ) kubectl apply -f http://github.com/knative/net-istio/releases/download/v0.21.0/istio.yaml Install the Knative Istio controller: kubectl apply -f http://github.com/knative/net-istio/releases/download/v0.21.0/net-istio.yaml Fetch the External IP or CNAME: kubectl --namespace istio-system get service istio-ingressgateway Tip Save this to use in the Configure DNS section. Kong The following commands install Kong and enable its Knative integration. Install Kong Ingress Controller: kubectl apply -f https://raw.githubusercontent.com/Kong/kubernetes-ingress-controller/0.9.x/deploy/single/all-in-one-dbless.yaml To configure Knative Serving to use Kong by default: kubectl patch configmap/config-network \\ --namespace knative-serving \\ --type merge \\ --patch '{\"data\":{\"ingress.class\":\"kong\"}}' Fetch the External IP or CNAME: kubectl --namespace kong get service kong-proxy Tip Save this to use in the Configure DNS section.","title":"Install a networking layer"},{"location":"install/install-serving-with-yaml/#verify-the-installation","text":"Monitor the Knative components until all of the components show a STATUS of Running or Completed : kubectl get pods --namespace knative-serving","title":"Verify the installation"},{"location":"install/install-serving-with-yaml/#configure-dns","text":"You can configure DNS to prevent the need to run curl commands with a host header. The tabs below expand to show instructions for configuring DNS. Follow the procedure for the DNS of your choice: Magic DNS (xip.io) We ship a simple Kubernetes Job called \"default domain\" that will (see caveats) configure Knative Serving to use xip.io as the default DNS suffix. kubectl apply -f http://github.com/knative/serving/releases/download/v0.21.0/serving-default-domain.yaml CAVEAT This will only work if the cluster LoadBalancer service exposes an IPv4 address or hostname, so it will not work with IPv6 clusters or local setups like Minikube. For these, see \"Real DNS\" or \"Temporary DNS\". Real DNS To configure DNS for Knative, take the External IP or CNAME from setting up networking, and configure it with your DNS provider as follows: If the networking layer produced an External IP address, then configure a wildcard A record for the domain: # Here knative.example.com is the domain suffix for your cluster *.knative.example.com == A 35 .233.41.212 If the networking layer produced a CNAME, then configure a CNAME record for the domain: # Here knative.example.com is the domain suffix for your cluster *.knative.example.com == CNAME a317a278525d111e89f272a164fd35fb-1510370581.eu-central-1.elb.amazonaws.com Once your DNS provider has been configured, direct Knative to use that domain: # Replace knative.example.com with your domain suffix kubectl patch configmap/config-domain \\ --namespace knative-serving \\ --type merge \\ --patch '{\"data\":{\"knative.example.com\":\"\"}}' Temporary DNS Info If you are using curl to access the sample applications, or your own Knative app, and are unable to use the \"Magic DNS (xip.io)\" or \"Real DNS\" methods, there is a temporary approach. This is useful for those who wish to evaluate Knative without altering their DNS configuration, as per the \"Real DNS\" method, or cannot use the \"Magic DNS\" method due to using, for example, minikube locally or IPv6 clusters. To access your application using curl using this method: After starting your application, get the URL of your application: kubectl get ksvc Verify the output NAME URL LATESTCREATED LATESTREADY READY REASON helloworld-go http://helloworld-go.default.example.com helloworld-go-vqjlf helloworld-go-vqjlf True Instruct curl to connect to the External IP or CNAME defined by the networking layer in section 3 above, and use the -H \"Host:\" command-line option to specify the Knative application's host name. For example, if the networking layer defines your External IP and port to be http://192.168.39.228:32198 and you wish to access the above helloworld-go application, use: curl -H \"Host: helloworld-go.default.example.com\" http://192.168.39.228:32198 Verify the output In the case of the provided helloworld-go sample application, the output should, using the default configuration, be: Hello Go Sample v1! Refer to the \"Real DNS\" method for a permanent solution.","title":"Configure DNS"},{"location":"install/install-serving-with-yaml/#next-steps","text":"After installing Knative Serving: Installing Knative Eventing using YAML files To add optional enhancements to your installation, see Installing optional extensions . To easily interact with Knative Services, install the kn CLI","title":"Next steps"},{"location":"install/installation-files/","text":"Installation files \u00b6 This guide provides reference information about the YAML files in the Knative Serving and Eventing releases. The YAML files in the releases include: The custom resource definitions (CRDs) and core components required to install Knative. Optional components that you can apply to customize your installation. For information about installing these files, see Installing Serving using YAML files and Installing Eventing using YAML files . Knative Serving installation files \u00b6 The table below describes the installation files in the Knative Serving release: File name Description Dependencies serving-core.yaml Required: Knative Serving core components. serving-crds.yaml serving-crds.yaml Required: Knative Serving core CRDs. none serving-default-domain.yaml Configures Knative Serving to use http://xip.io as the default DNS suffix. serving-core.yaml serving-domainmapping-crds.yaml CRDs used by the Domain Mapping feature. none serving-domainmapping.yaml Components used by the Domain Mapping feature. serving-domainmapping-crds.yaml serving-hpa.yaml Components to autoscale Knative revisions through the Kubernetes Horizontal Pod Autoscaler. serving-core.yaml serving-nscert.yaml Components to provision TLS wildcard certificates. serving-core.yaml serving-post-install-jobs.yaml Additional jobs after installing serving-core.yaml . Currently it is the same as serving-storage-version-migration.yaml . serving-core.yaml serving-storage-version-migration.yaml Migrates the storage version of Knative resources, including Service, Route, Revision, and Configuration, from v1alpha1 and v1beta1 to v1 . Required by upgrade from version 0.18 to 0.19. serving-core.yaml Knative Eventing installation files \u00b6 The table below describes the installation files in the Knative Eventing release: File name Description Dependencies eventing-core.yaml Required: Knative Eventing core components. eventing-crds.yaml eventing-crds.yaml Required: Knative Eventing core CRDs. none eventing-post-install.yaml Jobs required for upgrading to a new minor version. eventing-core.yaml, eventing-crds.yaml eventing-sugar-controller.yaml Reconciler that watches for labels and annotations on certain resources to inject eventing components. eventing-core.yaml eventing.yaml Combines eventing-core.yaml , mt-channel-broker.yaml , and in-memory-channel.yaml . none in-memory-channel.yaml Components to configure In-Memory Channels. eventing-core.yaml mt-channel-broker.yaml Components to configure Multi-Tenant (MT) Channel Broker. eventing-core.yaml","title":"Installation Files"},{"location":"install/installation-files/#installation-files","text":"This guide provides reference information about the YAML files in the Knative Serving and Eventing releases. The YAML files in the releases include: The custom resource definitions (CRDs) and core components required to install Knative. Optional components that you can apply to customize your installation. For information about installing these files, see Installing Serving using YAML files and Installing Eventing using YAML files .","title":"Installation files"},{"location":"install/installation-files/#knative-serving-installation-files","text":"The table below describes the installation files in the Knative Serving release: File name Description Dependencies serving-core.yaml Required: Knative Serving core components. serving-crds.yaml serving-crds.yaml Required: Knative Serving core CRDs. none serving-default-domain.yaml Configures Knative Serving to use http://xip.io as the default DNS suffix. serving-core.yaml serving-domainmapping-crds.yaml CRDs used by the Domain Mapping feature. none serving-domainmapping.yaml Components used by the Domain Mapping feature. serving-domainmapping-crds.yaml serving-hpa.yaml Components to autoscale Knative revisions through the Kubernetes Horizontal Pod Autoscaler. serving-core.yaml serving-nscert.yaml Components to provision TLS wildcard certificates. serving-core.yaml serving-post-install-jobs.yaml Additional jobs after installing serving-core.yaml . Currently it is the same as serving-storage-version-migration.yaml . serving-core.yaml serving-storage-version-migration.yaml Migrates the storage version of Knative resources, including Service, Route, Revision, and Configuration, from v1alpha1 and v1beta1 to v1 . Required by upgrade from version 0.18 to 0.19. serving-core.yaml","title":"Knative Serving installation files"},{"location":"install/installation-files/#knative-eventing-installation-files","text":"The table below describes the installation files in the Knative Eventing release: File name Description Dependencies eventing-core.yaml Required: Knative Eventing core components. eventing-crds.yaml eventing-crds.yaml Required: Knative Eventing core CRDs. none eventing-post-install.yaml Jobs required for upgrading to a new minor version. eventing-core.yaml, eventing-crds.yaml eventing-sugar-controller.yaml Reconciler that watches for labels and annotations on certain resources to inject eventing components. eventing-core.yaml eventing.yaml Combines eventing-core.yaml , mt-channel-broker.yaml , and in-memory-channel.yaml . none in-memory-channel.yaml Components to configure In-Memory Channels. eventing-core.yaml mt-channel-broker.yaml Components to configure Multi-Tenant (MT) Channel Broker. eventing-core.yaml","title":"Knative Eventing installation files"},{"location":"install/installing-istio/","text":"Installing Istio for Knative \u00b6 This guide walks you through manually installing and customizing Istio for use with Knative. If your cloud platform offers a managed Istio installation, we recommend installing Istio that way, unless you need to customize your installation. Before you begin \u00b6 You need: A Kubernetes cluster created. istioctl (v1.7 or later) installed. Supported Istio versions \u00b6 The current known-to-be-stable version of Istio tested in conjunction with Knative is v1.8.2 . Versions in the 1.7 line are generally fine too. 1.8.0 and 1.8.1 have bugs that don't work with Knative. Installing Istio \u00b6 When you install Istio, there are a few options depending on your goals. For a basic Istio installation suitable for most Knative use cases, follow the Installing Istio without sidecar injection instructions. If you're familiar with Istio and know what kind of installation you want, read through the options and choose the installation that suits your needs. You can easily customize your Istio installation with istioctl . The below sections cover a few useful Istio configurations and their benefits. Choosing an Istio installation \u00b6 You can install Istio with or without a service mesh: Installing Istio without sidecar injection (Recommended default installation) Installing Istio with sidecar injection If you want to get up and running with Knative quickly, we recommend installing Istio without automatic sidecar injection. This install is also recommended for users who don't need the Istio service mesh, or who want to enable the service mesh by manually injecting the Istio sidecars . Installing Istio without sidecar injection \u00b6 Enter the following command to install Istio: cat << EOF > ./istio-minimal-operator.yaml apiVersion: install.istio.io/v1alpha1 kind: IstioOperator spec: values: global: proxy: autoInject: disabled useMCP: false # The third-party-jwt is not enabled on all k8s. # See: https://istio.io/docs/ops/best-practices/security/#configure-third-party-service-account-tokens jwtPolicy: first-party-jwt addonComponents: pilot: enabled: true components: ingressGateways: - name: istio-ingressgateway enabled: true EOF istioctl install -f istio-minimal-operator.yaml Installing Istio with sidecar injection \u00b6 If you want to enable the Istio service mesh, you must enable automatic sidecar injection . The Istio service mesh provides a few benefits: Allows you to turn on mutual TLS , which secures service-to-service traffic within the cluster. Allows you to use the Istio authorization policy , controlling the access to each Knative service based on Istio service roles. To automatic sidecar injection, set autoInject: enabled in addition to above operator configuration. global: proxy: autoInject: enabled Using Istio mTLS feature \u00b6 Since there are some networking communications between knative-serving namespace and the namespace where your services running on, you need additional preparations for mTLS enabled environment. Enable sidecar container on knative-serving system namespace. kubectl label namespace knative-serving istio-injection = enabled Set PeerAuthentication to PERMISSIVE on knative-serving system namespace. cat <<EOF | kubectl apply -f - apiVersion: \"security.istio.io/v1beta1\" kind: \"PeerAuthentication\" metadata: name: \"default\" namespace: \"knative-serving\" spec: mtls: mode: PERMISSIVE EOF After you install the cluster local gateway, your service and deployment for the local gateway is named knative-local-gateway . Updating the config-istio configmap to use a non-default local gateway \u00b6 If you create a custom service and deployment for local gateway with a name other than knative-local-gateway , you need to update gateway configmap config-istio under the knative-serving namespace. Edit the config-istio configmap: kubectl edit configmap config-istio -n knative-serving Replace the local-gateway.knative-serving.knative-local-gateway field with the custom service. As an example, if you name both the service and deployment custom-local-gateway under the namespace istio-system , it should be updated to: custom-local-gateway.istio-system.svc.cluster.local As an example, if both the custom service and deployment are labeled with custom: custom-local-gateway , not the default istio: knative-local-gateway , you must update gateway instance knative-local-gateway in the knative-serving namespace: kubectl edit gateway knative-local-gateway -n knative-serving Replace the label selector with the label of your service: istio: knative-local-gateway For the service above, it should be updated to: custom: custom-local-gateway If there is a change in service ports (compared to that of knative-local-gateway ), update the port info in the gateway accordingly. Verifying your Istio install \u00b6 View the status of your Istio installation to make sure the install was successful. It might take a few seconds, so rerun the following command until all of the pods show a STATUS of Running or Completed : kubectl get pods --namespace istio-system Tip: You can append the --watch flag to the kubectl get commands to view the pod status in realtime. You use CTRL + C to exit watch mode. Configuring DNS \u00b6 Knative dispatches to different services based on their hostname, so it is recommended to have DNS properly configured. To do this, begin by looking up the external IP address that Istio received: $ kubectl get svc -nistio-system NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE istio-ingressgateway LoadBalancer 10.0.2.24 34.83.80.117 15020:32206/TCP,80:30742/TCP,443:30996/TCP 2m14s istio-pilot ClusterIP 10.0.3.27 <none> 15010/TCP,15011/TCP,8080/TCP,15014/TCP 2m14s This external IP can be used with your DNS provider with a wildcard A record. However, for a basic non-production set up, this external IP address can be used with xip.io in the config-domain ConfigMap in knative-serving . You can edit this by using the following command: kubectl edit cm config-domain --namespace knative-serving Given the external IP above, change the content to: apiVersion: v1 kind: ConfigMap metadata: name: config-domain namespace: knative-serving data: # xip.io is a \"magic\" DNS provider, which resolves all DNS lookups for: # *.{ip}.xip.io to {ip}. 34.83.80.117.xip.io: \"\" Istio resources \u00b6 For the official Istio installation guide, see the Istio Kubernetes Getting Started Guide . For the full list of available configs when installing Istio with istioctl , see the Istio Installation Options reference . Clean up Istio \u00b6 See the Uninstall Istio . What's next \u00b6 Install Knative . Try the Getting Started with App Deployment guide for Knative serving.","title":"Installing Istio for Knative"},{"location":"install/installing-istio/#installing-istio-for-knative","text":"This guide walks you through manually installing and customizing Istio for use with Knative. If your cloud platform offers a managed Istio installation, we recommend installing Istio that way, unless you need to customize your installation.","title":"Installing Istio for Knative"},{"location":"install/installing-istio/#before-you-begin","text":"You need: A Kubernetes cluster created. istioctl (v1.7 or later) installed.","title":"Before you begin"},{"location":"install/installing-istio/#supported-istio-versions","text":"The current known-to-be-stable version of Istio tested in conjunction with Knative is v1.8.2 . Versions in the 1.7 line are generally fine too. 1.8.0 and 1.8.1 have bugs that don't work with Knative.","title":"Supported Istio versions"},{"location":"install/installing-istio/#installing-istio","text":"When you install Istio, there are a few options depending on your goals. For a basic Istio installation suitable for most Knative use cases, follow the Installing Istio without sidecar injection instructions. If you're familiar with Istio and know what kind of installation you want, read through the options and choose the installation that suits your needs. You can easily customize your Istio installation with istioctl . The below sections cover a few useful Istio configurations and their benefits.","title":"Installing Istio"},{"location":"install/installing-istio/#choosing-an-istio-installation","text":"You can install Istio with or without a service mesh: Installing Istio without sidecar injection (Recommended default installation) Installing Istio with sidecar injection If you want to get up and running with Knative quickly, we recommend installing Istio without automatic sidecar injection. This install is also recommended for users who don't need the Istio service mesh, or who want to enable the service mesh by manually injecting the Istio sidecars .","title":"Choosing an Istio installation"},{"location":"install/installing-istio/#installing-istio-without-sidecar-injection","text":"Enter the following command to install Istio: cat << EOF > ./istio-minimal-operator.yaml apiVersion: install.istio.io/v1alpha1 kind: IstioOperator spec: values: global: proxy: autoInject: disabled useMCP: false # The third-party-jwt is not enabled on all k8s. # See: https://istio.io/docs/ops/best-practices/security/#configure-third-party-service-account-tokens jwtPolicy: first-party-jwt addonComponents: pilot: enabled: true components: ingressGateways: - name: istio-ingressgateway enabled: true EOF istioctl install -f istio-minimal-operator.yaml","title":"Installing Istio without sidecar injection"},{"location":"install/installing-istio/#installing-istio-with-sidecar-injection","text":"If you want to enable the Istio service mesh, you must enable automatic sidecar injection . The Istio service mesh provides a few benefits: Allows you to turn on mutual TLS , which secures service-to-service traffic within the cluster. Allows you to use the Istio authorization policy , controlling the access to each Knative service based on Istio service roles. To automatic sidecar injection, set autoInject: enabled in addition to above operator configuration. global: proxy: autoInject: enabled","title":"Installing Istio with sidecar injection"},{"location":"install/installing-istio/#using-istio-mtls-feature","text":"Since there are some networking communications between knative-serving namespace and the namespace where your services running on, you need additional preparations for mTLS enabled environment. Enable sidecar container on knative-serving system namespace. kubectl label namespace knative-serving istio-injection = enabled Set PeerAuthentication to PERMISSIVE on knative-serving system namespace. cat <<EOF | kubectl apply -f - apiVersion: \"security.istio.io/v1beta1\" kind: \"PeerAuthentication\" metadata: name: \"default\" namespace: \"knative-serving\" spec: mtls: mode: PERMISSIVE EOF After you install the cluster local gateway, your service and deployment for the local gateway is named knative-local-gateway .","title":"Using Istio mTLS feature"},{"location":"install/installing-istio/#updating-the-config-istio-configmap-to-use-a-non-default-local-gateway","text":"If you create a custom service and deployment for local gateway with a name other than knative-local-gateway , you need to update gateway configmap config-istio under the knative-serving namespace. Edit the config-istio configmap: kubectl edit configmap config-istio -n knative-serving Replace the local-gateway.knative-serving.knative-local-gateway field with the custom service. As an example, if you name both the service and deployment custom-local-gateway under the namespace istio-system , it should be updated to: custom-local-gateway.istio-system.svc.cluster.local As an example, if both the custom service and deployment are labeled with custom: custom-local-gateway , not the default istio: knative-local-gateway , you must update gateway instance knative-local-gateway in the knative-serving namespace: kubectl edit gateway knative-local-gateway -n knative-serving Replace the label selector with the label of your service: istio: knative-local-gateway For the service above, it should be updated to: custom: custom-local-gateway If there is a change in service ports (compared to that of knative-local-gateway ), update the port info in the gateway accordingly.","title":"Updating the config-istio configmap to use a non-default local gateway"},{"location":"install/installing-istio/#verifying-your-istio-install","text":"View the status of your Istio installation to make sure the install was successful. It might take a few seconds, so rerun the following command until all of the pods show a STATUS of Running or Completed : kubectl get pods --namespace istio-system Tip: You can append the --watch flag to the kubectl get commands to view the pod status in realtime. You use CTRL + C to exit watch mode.","title":"Verifying your Istio install"},{"location":"install/installing-istio/#configuring-dns","text":"Knative dispatches to different services based on their hostname, so it is recommended to have DNS properly configured. To do this, begin by looking up the external IP address that Istio received: $ kubectl get svc -nistio-system NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE istio-ingressgateway LoadBalancer 10.0.2.24 34.83.80.117 15020:32206/TCP,80:30742/TCP,443:30996/TCP 2m14s istio-pilot ClusterIP 10.0.3.27 <none> 15010/TCP,15011/TCP,8080/TCP,15014/TCP 2m14s This external IP can be used with your DNS provider with a wildcard A record. However, for a basic non-production set up, this external IP address can be used with xip.io in the config-domain ConfigMap in knative-serving . You can edit this by using the following command: kubectl edit cm config-domain --namespace knative-serving Given the external IP above, change the content to: apiVersion: v1 kind: ConfigMap metadata: name: config-domain namespace: knative-serving data: # xip.io is a \"magic\" DNS provider, which resolves all DNS lookups for: # *.{ip}.xip.io to {ip}. 34.83.80.117.xip.io: \"\"","title":"Configuring DNS"},{"location":"install/installing-istio/#istio-resources","text":"For the official Istio installation guide, see the Istio Kubernetes Getting Started Guide . For the full list of available configs when installing Istio with istioctl , see the Istio Installation Options reference .","title":"Istio resources"},{"location":"install/installing-istio/#clean-up-istio","text":"See the Uninstall Istio .","title":"Clean up Istio"},{"location":"install/installing-istio/#whats-next","text":"Install Knative . Try the Getting Started with App Deployment guide for Knative serving.","title":"What's next"},{"location":"install/knative-offerings/","text":"Knative Offerings \u00b6 Knative has a rich community with many vendors participating, and many of those vendors offer commercial Knative products. Please check with each of these vendors for what is or is not supported. Here is a list of commercial Knative products (alphabetically): Gardener : Install Knative in Gardener's vanilla Kubernetes clusters to add an extra layer of serverless runtime. Google Cloud Run for Anthos : Extend Google Kubernetes Engine with a flexible serverless development platform. With Cloud Run for Anthos, you get the operational flexibility of Kubernetes with the developer experience of serverless, allowing you to deploy and manage Knative-based services on your own cluster, and trigger them with events from Google, 3rd-party sources, and your own applications. Google Cloud Run : A fully-managed Knative-based serverless platform. With no Kubernetes cluster to manage, Cloud Run lets you go from container to production in seconds. IBM Cloud Code Engine : A fully-managed serverless platform that runs all your containerized workloads, including http-driven application, batch jobs or event-driven functions. Red Hat Openshift Serverless : enables stateful, stateless, and serverless workloads to all run on a single multi-cloud container platform with automated operations. Developers can use a single platform for hosting their microservices, legacy, and serverless applications. TriggerMesh Cloud : A fully-managed Knative and Tekton cloud-native integration platform. With support for AWS, Azure and Google event sources and brokers.","title":"Using a Knative-based Offering"},{"location":"install/knative-offerings/#knative-offerings","text":"Knative has a rich community with many vendors participating, and many of those vendors offer commercial Knative products. Please check with each of these vendors for what is or is not supported. Here is a list of commercial Knative products (alphabetically): Gardener : Install Knative in Gardener's vanilla Kubernetes clusters to add an extra layer of serverless runtime. Google Cloud Run for Anthos : Extend Google Kubernetes Engine with a flexible serverless development platform. With Cloud Run for Anthos, you get the operational flexibility of Kubernetes with the developer experience of serverless, allowing you to deploy and manage Knative-based services on your own cluster, and trigger them with events from Google, 3rd-party sources, and your own applications. Google Cloud Run : A fully-managed Knative-based serverless platform. With no Kubernetes cluster to manage, Cloud Run lets you go from container to production in seconds. IBM Cloud Code Engine : A fully-managed serverless platform that runs all your containerized workloads, including http-driven application, batch jobs or event-driven functions. Red Hat Openshift Serverless : enables stateful, stateless, and serverless workloads to all run on a single multi-cloud container platform with automated operations. Developers can use a single platform for hosting their microservices, legacy, and serverless applications. TriggerMesh Cloud : A fully-managed Knative and Tekton cloud-native integration platform. With support for AWS, Azure and Google event sources and brokers.","title":"Knative Offerings"},{"location":"install/knative-with-operators/","text":"Knative Operator installation \u00b6 Knative provides a Kubernetes Operator to install, configure and manage Knative. You can install the Serving component, Eventing component, or both on your cluster. NOTE: The Knative Operator is still in Alpha phase. It has not been tested in a production environment, and should be used for development or test purposes only. Prerequisites \u00b6 You have a cluster that uses Kubernetes v1.18 or newer. You have installed the kubectl CLI. If you have only one node in your cluster, you will need at least 6 CPUs, 6 GB of memory, and 30 GB of disk storage. If you have multiple nodes in your cluster, for each node you will need at least 2 CPUs, 4 GB of memory, and 20 GB of disk storage. Your Kubernetes cluster must have access to the internet, since Kubernetes needs to be able to fetch images, such as gcr.io/knative-releases/knative.dev/operator/cmd/operator:<version> . You have installed Istio . Installing the latest release \u00b6 You can find information about the different released versions of the Knative Operator on the Releases page . Install the latest stable Operator release: kubectl apply -f http://github.com/knative/operator/releases/download/{'provider': 'mike'}/operator.yaml Verify your installation \u00b6 Verify your installation: kubectl get deployment knative-operator If the operator is installed correctly, the deployment shows a Ready status: NAME READY UP-TO-DATE AVAILABLE AGE knative-operator 1/1 1 1 19h Track the log \u00b6 Track the log of the operator: kubectl logs -f deploy/knative-operator Installing the Knative Serving component \u00b6 Create and apply the Knative Serving CR: \u00b6 Install Current Serving Install Current Serving (default) You can install the latest available Knative Serving in the operator by applying a YAML file containing the following: apiVersion: v1 kind: Namespace metadata: name: knative-serving --- apiVersion: operator.knative.dev/v1alpha1 kind: KnativeServing metadata: name: knative-serving namespace: knative-serving If you do not specify a version by using spec_version, the operator defaults to the latest available version. Install Future Knative Serving You do not need to upgrade the operator to a newer version to install new releases of Knative Serving. If Knative Serving launches a new version, e.g. $spec_version , you can install it by applying a YAML file containing the following: apiVersion: v1 kind: Namespace metadata: name: knative-serving --- apiVersion: operator.knative.dev/v1alpha1 kind: KnativeServing metadata: name: knative-serving namespace: knative-serving spec: version: $spec_version manifests: - URL: https://github.com/knative/serving/releases/download/v${VERSION}/serving-core.yaml - URL: https://github.com/knative/serving/releases/download/v${VERSION}/serving-hpa.yaml - URL: https://github.com/knative/serving/releases/download/v${VERSION}/serving-post-install-jobs.yaml - URL: https://github.com/knative/net-istio/releases/download/v${VERSION}/net-istio.yaml The field $spec_version is used to set the version of Knative Serving. Replace $spec_version with the correct version number. The tag ${VERSION} is automatically replaced with the version number from spec_version by the operator. The field spec.manifests is used to specify one or multiple URL links of Knative Serving component. Do not forget to add the valid URL of the Knative network ingress plugin. Knative Serving component is still tightly-coupled with a network ingress plugin in the operator. As in the above example, you can use net-istio . The ordering of the URLs is critical. Put the manifest you want to apply first on the top. Install Customized Knative Serving The operator provides you the flexibility to install customized Knative Serving based your own requirements. As long as the manifests of customized Knative Serving are accessible to the operator, they can be installed. There are two modes available for you to install the customized manifests: overwrite mode and append mode. With the overwrite mode, you need to define all the manifests for Knative Serving to install, because the operator will no long install any available default manifests. With the append mode, you only need to define your customized manifests, and the customized manifests are installed, after default manifests are applied. You can use the overwrite mode to customize all the Knative Serving manifests. For example, the version of the customized Knative Serving is $spec_version , and it is available at https://my-serving/serving.yaml . You choose net-istio as the ingress plugin, which is available at https://my-net-istio/net-istio.yaml . You can create the content of Serving CR as below to install your Knative Serving and the istio ingress: apiVersion: v1 kind: Namespace metadata: name: knative-serving --- apiVersion: operator.knative.dev/v1alpha1 kind: KnativeServing metadata: name: knative-serving namespace: knative-serving spec: version: $spec_version manifests: - URL: https://my-serving/serving.yaml - URL: https://my-net-istio/net-istio.yaml You can make the customized Knative Serving available in one or multiple links, as the spec.manifests supports a list of links. The ordering of the URLs is critical. Put the manifest you want to apply first on the top. We strongly recommend you to specify the version and the valid links to the customized Knative Serving, by leveraging both spec_version and spec.manifests . Do not skip either field. You can use the append mode to add your customized manifests into the default manifests. For example, you only customize a few resources, and make them available at https://my-serving/serving-custom.yaml . You still need to install the default Knative Serving. In this case, you can create the content of Serving CR as below: apiVersion: v1 kind: Namespace metadata: name: knative-serving --- apiVersion: operator.knative.dev/v1alpha1 kind: KnativeServing metadata: name: knative-serving namespace: knative-serving spec: version: $spec_version additionalManifests: - URL: https://my-serving/serving-custom.yaml Knative operator will install the default manifests of Knative Serving at the version $spec_version , and then install your customized manifests based on them. Verify the Knative Serving deployment: \u00b6 ``` kubectl get deployment -n knative-serving ``` If Knative Serving has been successfully deployed, all deployments of the Knative Serving will show `READY` status. Here is a sample output: ``` NAME READY UP-TO-DATE AVAILABLE AGE activator 1/1 1 1 18s autoscaler 1/1 1 1 18s autoscaler-hpa 1/1 1 1 14s controller 1/1 1 1 18s istio-webhook 1/1 1 1 12s networking-istio 1/1 1 1 12s webhook 1/1 1 1 17s ``` 3. Check the status of Knative Serving Custom Resource: ``` kubectl get KnativeServing knative-serving -n knative-serving ``` If Knative Serving is successfully installed, you should see: ``` NAME VERSION READY REASON knative-serving <version number> True ``` Installing with Different Networking Layers \u00b6 Installing the Knative Serving component with different network layers Knative Operator can configure Knative Serving component with different network layer options. Istio is the default network layer, if the ingress is not specified in the Knative Serving CR. Click on each tab below to see how you can configure Knative Serving with different ingresses: Ambassador The following commands install Ambassador and enable its Knative integration. Create a namespace to install Ambassador in: kubectl create namespace ambassador Install Ambassador: kubectl apply --namespace ambassador \\ --filename https://getambassador.io/yaml/ambassador/ambassador-crds.yaml \\ --filename https://getambassador.io/yaml/ambassador/ambassador-rbac.yaml \\ --filename https://getambassador.io/yaml/ambassador/ambassador-service.yaml Give Ambassador the required permissions: kubectl patch clusterrolebinding ambassador -p '{\"subjects\":[{\"kind\": \"ServiceAccount\", \"name\": \"ambassador\", \"namespace\": \"ambassador\"}]}' Enable Knative support in Ambassador: kubectl set env --namespace ambassador deployments/ambassador AMBASSADOR_KNATIVE_SUPPORT = true To configure Knative Serving to use Ambassador, apply the content of the Serving CR as below: cat <<-EOF | kubectl apply -f - apiVersion: operator.knative.dev/v1alpha1 kind: KnativeServing metadata: name: knative-serving namespace: knative-serving spec: config: network: ingress.class: \"ambassador.ingress.networking.knative.dev\" EOF Fetch the External IP or CNAME: kubectl --namespace ambassador get service ambassador Save this for configuring DNS below. Contour The following commands install Contour and enable its Knative integration. Install a properly configured Contour: kubectl apply --filename http://github.com/knative/net-contour/releases/download/ { 'provider' : 'mike' } /contour.yaml To configure Knative Serving to use Contour, apply the content of the Serving CR as below: cat <<-EOF | kubectl apply -f - apiVersion: operator.knative.dev/v1alpha1 kind: KnativeServing metadata: name: knative-serving namespace: knative-serving spec: ingress: contour: enabled: true config: network: ingress.class: \"contour.ingress.networking.knative.dev\" EOF Fetch the External IP or CNAME: kubectl --namespace contour-external get service envoy Save this for configuring DNS below. Gloo For a detailed guide on Gloo integration, see Installing Gloo for Knative in the Gloo documentation. The following commands install Gloo and enable its Knative integration. Make sure glooctl is installed (version 1.3.x and higher recommended): glooctl version If it is not installed, you can install the latest version using: curl -sL https://run.solo.io/gloo/install | sh export PATH = $HOME /.gloo/bin: $PATH Or following the Gloo CLI install instructions . Install Gloo and the Knative integration: glooctl install knative --install-knative = false To configure Knative Serving to use Gloo, apply the content of the Serving CR as below: cat <<-EOF | kubectl apply -f - apiVersion: operator.knative.dev/v1alpha1 kind: KnativeServing metadata: name: knative-serving namespace: knative-serving EOF There is no need to configure the ingress class to use the gloo. Fetch the External IP or CNAME: glooctl proxy url --name knative-external-proxy Save this for configuring DNS below. Kong The following commands install Kong and enable its Knative integration. Install Kong Ingress Controller: kubectl apply --filename https://raw.githubusercontent.com/Kong/kubernetes-ingress-controller/0.9.x/deploy/single/all-in-one-dbless.yaml To configure Knative Serving to use Kong, apply the content of the Serving CR as below: cat <<-EOF | kubectl apply -f - apiVersion: operator.knative.dev/v1alpha1 kind: KnativeServing metadata: name: knative-serving namespace: knative-serving spec: config: network: ingress.class: \"kong\" EOF Fetch the External IP or CNAME: kubectl --namespace kong get service kong-proxy Save this for configuring DNS below. Kourier The following commands install Kourier and enable its Knative integration. To configure Knative Serving to use Kourier, apply the content of the Serving CR as below: cat <<-EOF | kubectl apply -f - apiVersion: operator.knative.dev/v1alpha1 kind: KnativeServing metadata: name: knative-serving namespace: knative-serving spec: ingress: kourier: enabled: true config: network: ingress.class: \"kourier.ingress.networking.knative.dev\" EOF Fetch the External IP or CNAME: kubectl --namespace knative-serving get service kourier Save this for configuring DNS below. Configure DNS \u00b6 Magic DNS (xip.io) We ship a simple Kubernetes Job called \"default domain\" that will (see caveats) configure Knative Serving to use xip.io as the default DNS suffix. kubectl apply --filename http://github.com/knative/serving/releases/download/ { 'provider' : 'mike' } /serving-default-domain.yaml Caveat : This will only work if the cluster LoadBalancer service exposes an IPv4 address or hostname, so it will not work with IPv6 clusters or local setups like Minikube. For these, see \"Real DNS\" or \"Temporary DNS\". Real DNS To configure DNS for Knative, take the External IP or CNAME from setting up networking, and configure it with your DNS provider as follows: If the networking layer produced an External IP address, then configure a wildcard A record for the domain: # Here knative.example.com is the domain suffix for your cluster *.knative.example.com == A 35.233.41.212 If the networking layer produced a CNAME, then configure a CNAME record for the domain: # Here knative.example.com is the domain suffix for your cluster *.knative.example.com == CNAME a317a278525d111e89f272a164fd35fb-1510370581.eu-central-1.elb.amazonaws.com Once your DNS provider has been configured, add the following section into your existing Serving CR, and apply it: # Replace knative.example.com with your domain suffix apiVersion: operator.knative.dev/v1alpha1 kind: KnativeServing metadata: name: knative-serving namespace: knative-serving spec: config: domain: \"knative.example.com\" : \"\" ... Temporary DNS If you are using curl to access the sample applications, or your own Knative app, and are unable to use the \"Magic DNS (xip.io)\" or \"Real DNS\" methods, there is a temporary approach. This is useful for those who wish to evaluate Knative without altering their DNS configuration, as per the \"Real DNS\" method, or cannot use the \"Magic DNS\" method due to using, for example, minikube locally or IPv6 clusters. To access your application using curl using this method: After starting your application, get the URL of your application: kubectl get ksvc The output should be similar to: NAME URL LATESTCREATED LATESTREADY READY REASON helloworld-go http://helloworld-go.default.example.com helloworld-go-vqjlf helloworld-go-vqjlf True Instruct curl to connect to the External IP or CNAME defined by the networking layer in section 3 above, and use the -H \"Host:\" command-line option to specify the Knative application's host name. For example, if the networking layer defines your External IP and port to be http://192.168.39.228:32198 and you wish to access the above helloworld-go application, use: curl -H \"Host: helloworld-go.default.example.com\" http://192.168.39.228:32198 In the case of the provided helloworld-go sample application, using the default configuration, the output should be: Hello Go Sample v1! Refer to the \"Real DNS\" method for a permanent solution. Monitor the Knative components until all of the components show a STATUS of Running or Completed : kubectl get pods --namespace knative-serving Installing the Knative Eventing component \u00b6 Create and apply the Knative Eventing CR: You can install the latest available Knative Eventing in the operator by applying a YAML file containing the following: Install Current Evenintg (default) apiVersion: v1 kind: Namespace metadata: name: knative-eventing --- apiVersion: operator.knative.dev/v1alpha1 kind: KnativeEventing metadata: name: knative-eventing namespace: knative-eventing If you do not specify a version by using spec.version, the operator defaults to the latest available version. Install Future Knative Eventing You do not need to upgrade the operator to a newer version to install new releases of Knative Eventing. If Knative Eventing launches a new version, e.g. $spec_version , you can install it by applying a YAML file containing the following: apiVersion: v1 kind: Namespace metadata: name: knative-eventing --- apiVersion: operator.knative.dev/v1alpha1 kind: KnativeEventing metadata: name: knative-eventing namespace: knative-eventing spec: version: $spec_version manifests: - URL: https://github.com/knative/eventing/releases/download/v${VERSION}/eventing.yaml - URL: https://github.com/knative/eventing/releases/download/v${VERSION}/eventing-post-install-jobs.yaml The field spec.version is used to set the version of Knative Eventing. Replace $spec_version with the correct version number. The tag ${VERSION} is automatically replaced with the version number from spec.version by the operator. The field spec.manifests is used to specify one or multiple URL links of Knative Eventing component. Do not forget to add the valid URL of the Knative network ingress plugin. The ordering of the URLs is critical. Put the manifest you want to apply first on the top. Install Customized Knative Evening The operator provides you the flexibility to install customized Knative Eventing based your own requirements. As long as the manifests of customized Knative Eventing are accessible to the operator, they can be installed. There are two modes available for you to install the customized manifests: overwrite mode and append mode. With the overwrite mode, you need to define all the manifests for Knative Eventing to install, because the operator will no long install any available default manifests. With the append mode, you only need to define your customized manifests, and the customized manifests are installed, after default manifests are applied. You can use the overwrite mode to customize all the Knative Eventing manifests. For example, the version of the customized Knative Eventing is $spec_version , and it is available at https://my-eventing/eventing.yaml . You can create the content of Eventing CR as below to install your Knative Eventing: apiVersion: v1 kind: Namespace metadata: name: knative-eventing --- apiVersion: operator.knative.dev/v1alpha1 kind: KnativeEventing metadata: name: knative-eventing namespace: knative-eventing spec: version: $spec_version manifests: - URL: https://my-eventing/eventing.yaml You can make the customized Knative Eventing available in one or multiple links, as the spec.manifests supports a list of links. The ordering of the URLs is critical. Put the manifest you want to apply first on the top. We strongly recommend you to specify the version and the valid links to the customized Knative Eventing, by leveraging both spec.version and spec.manifests . Do not skip either field. You can use the append mode to add your customized manifests into the default manifests. For example, you only customize a few resources, and make them available at https://my-eventing/eventing-custom.yaml . You still need to install the default Knative eventing. In this case, you can create the content of Eventing CR as below: apiVersion: v1 kind: Namespace metadata: name: knative-eventing --- apiVersion: operator.knative.dev/v1alpha1 kind: KnativeEventing metadata: name: knative-eventing namespace: knative-eventing spec: version: $spec_version additionalManifests: - URL: https://my-eventing/eventing-custom.yaml Knative operator will install the default manifests of Knative Eventing at the version $spec_version , and then install your customized manifests based on them. Verify the Knative Eventing deployment: \u00b6 kubectl get deployment -n knative-eventing If Knative Eventing has been successfully deployed, all deployments of the Knative Eventing will show READY status. Here is a sample output: NAME READY UP-TO-DATE AVAILABLE AGE broker-controller 1/1 1 1 63s broker-filter 1/1 1 1 62s broker-ingress 1/1 1 1 62s eventing-controller 1/1 1 1 67s eventing-webhook 1/1 1 1 67s imc-controller 1/1 1 1 59s imc-dispatcher 1/1 1 1 59s mt-broker-controller 1/1 1 1 62s Check the status of Knative Eventing Custom Resource: \u00b6 kubectl get KnativeEventing knative-eventing -n knative-eventing If Knative Eventing is successfully installed, you should see: NAME VERSION READY REASON knative-eventing <version number> True Uninstall Knative \u00b6 Removing the Knative Serving component \u00b6 Remove the Knative Serving CR: kubectl delete KnativeServing knative-serving -n knative-serving Removing Knative Eventing component \u00b6 Remove the Knative Eventing CR: kubectl delete KnativeEventing knative-eventing -n knative-eventing Knative operator prevents unsafe removal of Knative resources. Even if the Knative Serving and Knative Eventing CRs are successfully removed, all the CRDs in Knative are still kept in the cluster. All your resources relying on Knative CRDs can still work. Removing the Knative Operator: \u00b6 If you have installed Knative using the Release page, remove the operator using the following command: kubectl delete -f http://github.com/knative/operator/releases/download/{'provider': 'mike'}/operator.yaml If you have installed Knative from source, uninstall it using the following command while in the root directory for the source: ko delete -f config/ What's next \u00b6 Configure Knative Serving using Operator Configure Knative Eventing using Operator","title":"Installing with the Operator"},{"location":"install/knative-with-operators/#knative-operator-installation","text":"Knative provides a Kubernetes Operator to install, configure and manage Knative. You can install the Serving component, Eventing component, or both on your cluster. NOTE: The Knative Operator is still in Alpha phase. It has not been tested in a production environment, and should be used for development or test purposes only.","title":"Knative Operator installation"},{"location":"install/knative-with-operators/#prerequisites","text":"You have a cluster that uses Kubernetes v1.18 or newer. You have installed the kubectl CLI. If you have only one node in your cluster, you will need at least 6 CPUs, 6 GB of memory, and 30 GB of disk storage. If you have multiple nodes in your cluster, for each node you will need at least 2 CPUs, 4 GB of memory, and 20 GB of disk storage. Your Kubernetes cluster must have access to the internet, since Kubernetes needs to be able to fetch images, such as gcr.io/knative-releases/knative.dev/operator/cmd/operator:<version> . You have installed Istio .","title":"Prerequisites"},{"location":"install/knative-with-operators/#installing-the-latest-release","text":"You can find information about the different released versions of the Knative Operator on the Releases page . Install the latest stable Operator release: kubectl apply -f http://github.com/knative/operator/releases/download/{'provider': 'mike'}/operator.yaml","title":"Installing the latest release"},{"location":"install/knative-with-operators/#verify-your-installation","text":"Verify your installation: kubectl get deployment knative-operator If the operator is installed correctly, the deployment shows a Ready status: NAME READY UP-TO-DATE AVAILABLE AGE knative-operator 1/1 1 1 19h","title":"Verify your installation"},{"location":"install/knative-with-operators/#track-the-log","text":"Track the log of the operator: kubectl logs -f deploy/knative-operator","title":"Track the log"},{"location":"install/knative-with-operators/#installing-the-knative-serving-component","text":"","title":"Installing the Knative Serving component"},{"location":"install/knative-with-operators/#create-and-apply-the-knative-serving-cr","text":"Install Current Serving Install Current Serving (default) You can install the latest available Knative Serving in the operator by applying a YAML file containing the following: apiVersion: v1 kind: Namespace metadata: name: knative-serving --- apiVersion: operator.knative.dev/v1alpha1 kind: KnativeServing metadata: name: knative-serving namespace: knative-serving If you do not specify a version by using spec_version, the operator defaults to the latest available version. Install Future Knative Serving You do not need to upgrade the operator to a newer version to install new releases of Knative Serving. If Knative Serving launches a new version, e.g. $spec_version , you can install it by applying a YAML file containing the following: apiVersion: v1 kind: Namespace metadata: name: knative-serving --- apiVersion: operator.knative.dev/v1alpha1 kind: KnativeServing metadata: name: knative-serving namespace: knative-serving spec: version: $spec_version manifests: - URL: https://github.com/knative/serving/releases/download/v${VERSION}/serving-core.yaml - URL: https://github.com/knative/serving/releases/download/v${VERSION}/serving-hpa.yaml - URL: https://github.com/knative/serving/releases/download/v${VERSION}/serving-post-install-jobs.yaml - URL: https://github.com/knative/net-istio/releases/download/v${VERSION}/net-istio.yaml The field $spec_version is used to set the version of Knative Serving. Replace $spec_version with the correct version number. The tag ${VERSION} is automatically replaced with the version number from spec_version by the operator. The field spec.manifests is used to specify one or multiple URL links of Knative Serving component. Do not forget to add the valid URL of the Knative network ingress plugin. Knative Serving component is still tightly-coupled with a network ingress plugin in the operator. As in the above example, you can use net-istio . The ordering of the URLs is critical. Put the manifest you want to apply first on the top. Install Customized Knative Serving The operator provides you the flexibility to install customized Knative Serving based your own requirements. As long as the manifests of customized Knative Serving are accessible to the operator, they can be installed. There are two modes available for you to install the customized manifests: overwrite mode and append mode. With the overwrite mode, you need to define all the manifests for Knative Serving to install, because the operator will no long install any available default manifests. With the append mode, you only need to define your customized manifests, and the customized manifests are installed, after default manifests are applied. You can use the overwrite mode to customize all the Knative Serving manifests. For example, the version of the customized Knative Serving is $spec_version , and it is available at https://my-serving/serving.yaml . You choose net-istio as the ingress plugin, which is available at https://my-net-istio/net-istio.yaml . You can create the content of Serving CR as below to install your Knative Serving and the istio ingress: apiVersion: v1 kind: Namespace metadata: name: knative-serving --- apiVersion: operator.knative.dev/v1alpha1 kind: KnativeServing metadata: name: knative-serving namespace: knative-serving spec: version: $spec_version manifests: - URL: https://my-serving/serving.yaml - URL: https://my-net-istio/net-istio.yaml You can make the customized Knative Serving available in one or multiple links, as the spec.manifests supports a list of links. The ordering of the URLs is critical. Put the manifest you want to apply first on the top. We strongly recommend you to specify the version and the valid links to the customized Knative Serving, by leveraging both spec_version and spec.manifests . Do not skip either field. You can use the append mode to add your customized manifests into the default manifests. For example, you only customize a few resources, and make them available at https://my-serving/serving-custom.yaml . You still need to install the default Knative Serving. In this case, you can create the content of Serving CR as below: apiVersion: v1 kind: Namespace metadata: name: knative-serving --- apiVersion: operator.knative.dev/v1alpha1 kind: KnativeServing metadata: name: knative-serving namespace: knative-serving spec: version: $spec_version additionalManifests: - URL: https://my-serving/serving-custom.yaml Knative operator will install the default manifests of Knative Serving at the version $spec_version , and then install your customized manifests based on them.","title":"Create and apply the Knative Serving CR:"},{"location":"install/knative-with-operators/#verify-the-knative-serving-deployment","text":"``` kubectl get deployment -n knative-serving ``` If Knative Serving has been successfully deployed, all deployments of the Knative Serving will show `READY` status. Here is a sample output: ``` NAME READY UP-TO-DATE AVAILABLE AGE activator 1/1 1 1 18s autoscaler 1/1 1 1 18s autoscaler-hpa 1/1 1 1 14s controller 1/1 1 1 18s istio-webhook 1/1 1 1 12s networking-istio 1/1 1 1 12s webhook 1/1 1 1 17s ``` 3. Check the status of Knative Serving Custom Resource: ``` kubectl get KnativeServing knative-serving -n knative-serving ``` If Knative Serving is successfully installed, you should see: ``` NAME VERSION READY REASON knative-serving <version number> True ```","title":"Verify the Knative Serving deployment:"},{"location":"install/knative-with-operators/#installing-with-different-networking-layers","text":"Installing the Knative Serving component with different network layers Knative Operator can configure Knative Serving component with different network layer options. Istio is the default network layer, if the ingress is not specified in the Knative Serving CR. Click on each tab below to see how you can configure Knative Serving with different ingresses: Ambassador The following commands install Ambassador and enable its Knative integration. Create a namespace to install Ambassador in: kubectl create namespace ambassador Install Ambassador: kubectl apply --namespace ambassador \\ --filename https://getambassador.io/yaml/ambassador/ambassador-crds.yaml \\ --filename https://getambassador.io/yaml/ambassador/ambassador-rbac.yaml \\ --filename https://getambassador.io/yaml/ambassador/ambassador-service.yaml Give Ambassador the required permissions: kubectl patch clusterrolebinding ambassador -p '{\"subjects\":[{\"kind\": \"ServiceAccount\", \"name\": \"ambassador\", \"namespace\": \"ambassador\"}]}' Enable Knative support in Ambassador: kubectl set env --namespace ambassador deployments/ambassador AMBASSADOR_KNATIVE_SUPPORT = true To configure Knative Serving to use Ambassador, apply the content of the Serving CR as below: cat <<-EOF | kubectl apply -f - apiVersion: operator.knative.dev/v1alpha1 kind: KnativeServing metadata: name: knative-serving namespace: knative-serving spec: config: network: ingress.class: \"ambassador.ingress.networking.knative.dev\" EOF Fetch the External IP or CNAME: kubectl --namespace ambassador get service ambassador Save this for configuring DNS below. Contour The following commands install Contour and enable its Knative integration. Install a properly configured Contour: kubectl apply --filename http://github.com/knative/net-contour/releases/download/ { 'provider' : 'mike' } /contour.yaml To configure Knative Serving to use Contour, apply the content of the Serving CR as below: cat <<-EOF | kubectl apply -f - apiVersion: operator.knative.dev/v1alpha1 kind: KnativeServing metadata: name: knative-serving namespace: knative-serving spec: ingress: contour: enabled: true config: network: ingress.class: \"contour.ingress.networking.knative.dev\" EOF Fetch the External IP or CNAME: kubectl --namespace contour-external get service envoy Save this for configuring DNS below. Gloo For a detailed guide on Gloo integration, see Installing Gloo for Knative in the Gloo documentation. The following commands install Gloo and enable its Knative integration. Make sure glooctl is installed (version 1.3.x and higher recommended): glooctl version If it is not installed, you can install the latest version using: curl -sL https://run.solo.io/gloo/install | sh export PATH = $HOME /.gloo/bin: $PATH Or following the Gloo CLI install instructions . Install Gloo and the Knative integration: glooctl install knative --install-knative = false To configure Knative Serving to use Gloo, apply the content of the Serving CR as below: cat <<-EOF | kubectl apply -f - apiVersion: operator.knative.dev/v1alpha1 kind: KnativeServing metadata: name: knative-serving namespace: knative-serving EOF There is no need to configure the ingress class to use the gloo. Fetch the External IP or CNAME: glooctl proxy url --name knative-external-proxy Save this for configuring DNS below. Kong The following commands install Kong and enable its Knative integration. Install Kong Ingress Controller: kubectl apply --filename https://raw.githubusercontent.com/Kong/kubernetes-ingress-controller/0.9.x/deploy/single/all-in-one-dbless.yaml To configure Knative Serving to use Kong, apply the content of the Serving CR as below: cat <<-EOF | kubectl apply -f - apiVersion: operator.knative.dev/v1alpha1 kind: KnativeServing metadata: name: knative-serving namespace: knative-serving spec: config: network: ingress.class: \"kong\" EOF Fetch the External IP or CNAME: kubectl --namespace kong get service kong-proxy Save this for configuring DNS below. Kourier The following commands install Kourier and enable its Knative integration. To configure Knative Serving to use Kourier, apply the content of the Serving CR as below: cat <<-EOF | kubectl apply -f - apiVersion: operator.knative.dev/v1alpha1 kind: KnativeServing metadata: name: knative-serving namespace: knative-serving spec: ingress: kourier: enabled: true config: network: ingress.class: \"kourier.ingress.networking.knative.dev\" EOF Fetch the External IP or CNAME: kubectl --namespace knative-serving get service kourier Save this for configuring DNS below.","title":"Installing with Different Networking Layers"},{"location":"install/knative-with-operators/#configure-dns","text":"Magic DNS (xip.io) We ship a simple Kubernetes Job called \"default domain\" that will (see caveats) configure Knative Serving to use xip.io as the default DNS suffix. kubectl apply --filename http://github.com/knative/serving/releases/download/ { 'provider' : 'mike' } /serving-default-domain.yaml Caveat : This will only work if the cluster LoadBalancer service exposes an IPv4 address or hostname, so it will not work with IPv6 clusters or local setups like Minikube. For these, see \"Real DNS\" or \"Temporary DNS\". Real DNS To configure DNS for Knative, take the External IP or CNAME from setting up networking, and configure it with your DNS provider as follows: If the networking layer produced an External IP address, then configure a wildcard A record for the domain: # Here knative.example.com is the domain suffix for your cluster *.knative.example.com == A 35.233.41.212 If the networking layer produced a CNAME, then configure a CNAME record for the domain: # Here knative.example.com is the domain suffix for your cluster *.knative.example.com == CNAME a317a278525d111e89f272a164fd35fb-1510370581.eu-central-1.elb.amazonaws.com Once your DNS provider has been configured, add the following section into your existing Serving CR, and apply it: # Replace knative.example.com with your domain suffix apiVersion: operator.knative.dev/v1alpha1 kind: KnativeServing metadata: name: knative-serving namespace: knative-serving spec: config: domain: \"knative.example.com\" : \"\" ... Temporary DNS If you are using curl to access the sample applications, or your own Knative app, and are unable to use the \"Magic DNS (xip.io)\" or \"Real DNS\" methods, there is a temporary approach. This is useful for those who wish to evaluate Knative without altering their DNS configuration, as per the \"Real DNS\" method, or cannot use the \"Magic DNS\" method due to using, for example, minikube locally or IPv6 clusters. To access your application using curl using this method: After starting your application, get the URL of your application: kubectl get ksvc The output should be similar to: NAME URL LATESTCREATED LATESTREADY READY REASON helloworld-go http://helloworld-go.default.example.com helloworld-go-vqjlf helloworld-go-vqjlf True Instruct curl to connect to the External IP or CNAME defined by the networking layer in section 3 above, and use the -H \"Host:\" command-line option to specify the Knative application's host name. For example, if the networking layer defines your External IP and port to be http://192.168.39.228:32198 and you wish to access the above helloworld-go application, use: curl -H \"Host: helloworld-go.default.example.com\" http://192.168.39.228:32198 In the case of the provided helloworld-go sample application, using the default configuration, the output should be: Hello Go Sample v1! Refer to the \"Real DNS\" method for a permanent solution. Monitor the Knative components until all of the components show a STATUS of Running or Completed : kubectl get pods --namespace knative-serving","title":"Configure DNS"},{"location":"install/knative-with-operators/#installing-the-knative-eventing-component","text":"Create and apply the Knative Eventing CR: You can install the latest available Knative Eventing in the operator by applying a YAML file containing the following: Install Current Evenintg (default) apiVersion: v1 kind: Namespace metadata: name: knative-eventing --- apiVersion: operator.knative.dev/v1alpha1 kind: KnativeEventing metadata: name: knative-eventing namespace: knative-eventing If you do not specify a version by using spec.version, the operator defaults to the latest available version. Install Future Knative Eventing You do not need to upgrade the operator to a newer version to install new releases of Knative Eventing. If Knative Eventing launches a new version, e.g. $spec_version , you can install it by applying a YAML file containing the following: apiVersion: v1 kind: Namespace metadata: name: knative-eventing --- apiVersion: operator.knative.dev/v1alpha1 kind: KnativeEventing metadata: name: knative-eventing namespace: knative-eventing spec: version: $spec_version manifests: - URL: https://github.com/knative/eventing/releases/download/v${VERSION}/eventing.yaml - URL: https://github.com/knative/eventing/releases/download/v${VERSION}/eventing-post-install-jobs.yaml The field spec.version is used to set the version of Knative Eventing. Replace $spec_version with the correct version number. The tag ${VERSION} is automatically replaced with the version number from spec.version by the operator. The field spec.manifests is used to specify one or multiple URL links of Knative Eventing component. Do not forget to add the valid URL of the Knative network ingress plugin. The ordering of the URLs is critical. Put the manifest you want to apply first on the top. Install Customized Knative Evening The operator provides you the flexibility to install customized Knative Eventing based your own requirements. As long as the manifests of customized Knative Eventing are accessible to the operator, they can be installed. There are two modes available for you to install the customized manifests: overwrite mode and append mode. With the overwrite mode, you need to define all the manifests for Knative Eventing to install, because the operator will no long install any available default manifests. With the append mode, you only need to define your customized manifests, and the customized manifests are installed, after default manifests are applied. You can use the overwrite mode to customize all the Knative Eventing manifests. For example, the version of the customized Knative Eventing is $spec_version , and it is available at https://my-eventing/eventing.yaml . You can create the content of Eventing CR as below to install your Knative Eventing: apiVersion: v1 kind: Namespace metadata: name: knative-eventing --- apiVersion: operator.knative.dev/v1alpha1 kind: KnativeEventing metadata: name: knative-eventing namespace: knative-eventing spec: version: $spec_version manifests: - URL: https://my-eventing/eventing.yaml You can make the customized Knative Eventing available in one or multiple links, as the spec.manifests supports a list of links. The ordering of the URLs is critical. Put the manifest you want to apply first on the top. We strongly recommend you to specify the version and the valid links to the customized Knative Eventing, by leveraging both spec.version and spec.manifests . Do not skip either field. You can use the append mode to add your customized manifests into the default manifests. For example, you only customize a few resources, and make them available at https://my-eventing/eventing-custom.yaml . You still need to install the default Knative eventing. In this case, you can create the content of Eventing CR as below: apiVersion: v1 kind: Namespace metadata: name: knative-eventing --- apiVersion: operator.knative.dev/v1alpha1 kind: KnativeEventing metadata: name: knative-eventing namespace: knative-eventing spec: version: $spec_version additionalManifests: - URL: https://my-eventing/eventing-custom.yaml Knative operator will install the default manifests of Knative Eventing at the version $spec_version , and then install your customized manifests based on them.","title":"Installing the Knative Eventing component"},{"location":"install/knative-with-operators/#verify-the-knative-eventing-deployment","text":"kubectl get deployment -n knative-eventing If Knative Eventing has been successfully deployed, all deployments of the Knative Eventing will show READY status. Here is a sample output: NAME READY UP-TO-DATE AVAILABLE AGE broker-controller 1/1 1 1 63s broker-filter 1/1 1 1 62s broker-ingress 1/1 1 1 62s eventing-controller 1/1 1 1 67s eventing-webhook 1/1 1 1 67s imc-controller 1/1 1 1 59s imc-dispatcher 1/1 1 1 59s mt-broker-controller 1/1 1 1 62s","title":"Verify the Knative Eventing deployment:"},{"location":"install/knative-with-operators/#check-the-status-of-knative-eventing-custom-resource","text":"kubectl get KnativeEventing knative-eventing -n knative-eventing If Knative Eventing is successfully installed, you should see: NAME VERSION READY REASON knative-eventing <version number> True","title":"Check the status of Knative Eventing Custom Resource:"},{"location":"install/knative-with-operators/#uninstall-knative","text":"","title":"Uninstall Knative"},{"location":"install/knative-with-operators/#removing-the-knative-serving-component","text":"Remove the Knative Serving CR: kubectl delete KnativeServing knative-serving -n knative-serving","title":"Removing the Knative Serving component"},{"location":"install/knative-with-operators/#removing-knative-eventing-component","text":"Remove the Knative Eventing CR: kubectl delete KnativeEventing knative-eventing -n knative-eventing Knative operator prevents unsafe removal of Knative resources. Even if the Knative Serving and Knative Eventing CRs are successfully removed, all the CRDs in Knative are still kept in the cluster. All your resources relying on Knative CRDs can still work.","title":"Removing Knative Eventing component"},{"location":"install/knative-with-operators/#removing-the-knative-operator","text":"If you have installed Knative using the Release page, remove the operator using the following command: kubectl delete -f http://github.com/knative/operator/releases/download/{'provider': 'mike'}/operator.yaml If you have installed Knative from source, uninstall it using the following command while in the root directory for the source: ko delete -f config/","title":"Removing the Knative Operator:"},{"location":"install/knative-with-operators/#whats-next","text":"Configure Knative Serving using Operator Configure Knative Eventing using Operator","title":"What's next"},{"location":"install/prerequisites/","text":"Prerequisites \u00b6 Tip If you're installing Knative for the first time, a better place to start may be Getting Started . Before installing Knative, you must meet the following prerequisites: System requirements \u00b6 For prototyping purposes , Knative will work on most local deployments of Kubernetes. For example, you can use a local, one-node cluster that has 2 CPU and 4GB of memory. For production purposes , it is recommended that: - If you have only one node in your cluster, you will need at least 6 CPUs, 6 GB of memory, and 30 GB of disk storage. - If you have multiple nodes in your cluster, for each node you will need at least 2 CPUs, 4 GB of memory, and 20 GB of disk storage. NOTE: The system requirements provided are recommendations only. The requirements for your installation may vary, depending on whether you use optional components, such as a networking layer. Prerequisites \u00b6 Before installation, you must meet the following prerequisites: You have a cluster that uses Kubernetes v1.18 or newer. You have installed the kubectl CLI . Your Kubernetes cluster must have access to the internet, since Kubernetes needs to be able to fetch images. (To pull from a private registry, see Deploying images from a private container registry ) Next Steps: Install Knative Serving and Eventing \u00b6 You can install the Serving component, Eventing component, or both on your cluster. If you're planning on installing both, we recommend starting with Knative Serving. Installing Knative Serving using YAML files Installing Knative Eventing using YAML files","title":"Prerequisites"},{"location":"install/prerequisites/#prerequisites","text":"Tip If you're installing Knative for the first time, a better place to start may be Getting Started . Before installing Knative, you must meet the following prerequisites:","title":"Prerequisites"},{"location":"install/prerequisites/#system-requirements","text":"For prototyping purposes , Knative will work on most local deployments of Kubernetes. For example, you can use a local, one-node cluster that has 2 CPU and 4GB of memory. For production purposes , it is recommended that: - If you have only one node in your cluster, you will need at least 6 CPUs, 6 GB of memory, and 30 GB of disk storage. - If you have multiple nodes in your cluster, for each node you will need at least 2 CPUs, 4 GB of memory, and 20 GB of disk storage. NOTE: The system requirements provided are recommendations only. The requirements for your installation may vary, depending on whether you use optional components, such as a networking layer.","title":"System requirements"},{"location":"install/prerequisites/#prerequisites_1","text":"Before installation, you must meet the following prerequisites: You have a cluster that uses Kubernetes v1.18 or newer. You have installed the kubectl CLI . Your Kubernetes cluster must have access to the internet, since Kubernetes needs to be able to fetch images. (To pull from a private registry, see Deploying images from a private container registry )","title":"Prerequisites"},{"location":"install/prerequisites/#next-steps-install-knative-serving-and-eventing","text":"You can install the Serving component, Eventing component, or both on your cluster. If you're planning on installing both, we recommend starting with Knative Serving. Installing Knative Serving using YAML files Installing Knative Eventing using YAML files","title":"Next Steps: Install Knative Serving and Eventing"},{"location":"install/upgrade-installation-with-operator/","text":"Upgrading your installation with Knative operator \u00b6 The Knative operator supports a straightforward upgrade process. It supports upgrading the Knative component by a single minor version number. For example, if you have v0.17 installed, you must upgrade to v0.18 before attempting to upgrade to v0.19. The attribute spec.version is the only field you need to change in the Serving or Eventing CR to perform an upgrade. You do not need to specify the version in terms of the patch number, because the Knative Operator will match the latest available patch number, as long as you specify major.minor for the version. For example, you only need to specify 0.19 to upgrade to the latest v0.19 release. There is no need to know the exact patch number. The Knative Operator implements a minus 3 principle to support the Knative versions, which means the current version of the Operator can support Knative with the version back 3 in terms of the minor number. For example, if the current version of the Operator is 0.19.x, it bundles and supports the installation of Knative with the versions, 0.16.x, 0.17.x, 0.18.x and 0.19.x. Before you begin \u00b6 Knative Operator maximizes the automation for the upgrade process, all you need to know is the current version of your Knative, the target version of your Knative, and the namespaces for your Knative installation. In the following instruction, Knative Serving and the Serving custom resource are installed in the knative-serving namespace, and Knative Eventing and the Eventing custom resource are installed in the knative-eventing namespace. Check the current version of the installed Knative \u00b6 If you want to check the version of the installed Knative Serving, you can apply the following command: kubectl get KnativeServing knative-serving --namespace knative-serving If your current version for Knative Serving is 0.19.x, you will get the result as below: NAME VERSION READY REASON knative-serving 0.19.0 True As Knative only supports the upgrade with one single minor version, the target version is 0.20 for Knative Serving. The status True means the Serving CR and Knative Serving are in good status. If you want to check the version of the installed Knative Eventing, you can apply the following command: kubectl get KnativeEventing knative-eventing --namespace knative-eventing If your current version for Knative Eventing is 0.19.x, you will get the result as below: NAME VERSION READY REASON knative-eventing 0.19.0 True As Knative only supports the upgrade with one single minor version, the target version is 0.20 for Knative Eventing. The status True means the Eventing CR and Knative Eventing are in good status. Performing the upgrade \u00b6 To upgrade, apply the Operator CRs with the same spec, but a different target version for the attribute spec.version . If your existing Serving CR is as below: apiVersion: operator.knative.dev/v1alpha1 kind: KnativeServing metadata: name: knative-serving namespace: knative-serving spec: version: \"0.19\" then apply the following CR to upgrade to 0.20: apiVersion: operator.knative.dev/v1alpha1 kind: KnativeServing metadata: name: knative-serving namespace: knative-serving spec: version: \"0.20\" If your existing Eventing CR is as below: apiVersion: operator.knative.dev/v1alpha1 kind: KnativeEventing metadata: name: knative-eventing namespace: knative-eventing spec: version: \"0.19\" then apply the following CR to upgrade to 0.20: apiVersion: operator.knative.dev/v1alpha1 kind: KnativeEventing metadata: name: knative-eventing namespace: knative-eventing spec: version: \"0.20\" Verifying the upgrade \u00b6 To confirm that your Knative components have successfully upgraded, view the status of their pods in the relevant namespaces. All pods will restart during the upgrade and their age will reset. If you upgraded Knative Serving and Eventing, enter the following commands to get information about the pods for each namespace: kubectl get pods --namespace knative-serving kubectl get pods --namespace knative-eventing These commands return something similar to: NAME READY STATUS RESTARTS AGE activator-6875896748-gdjgs 1 /1 Running 0 58s autoscaler-6bbc885cfd-vkrgg 1 /1 Running 0 57s autoscaler-hpa-5cdd7c6b69-hxzv4 1 /1 Running 0 55s controller-64dd4bd56-wzb2k 1 /1 Running 0 57s istio-webhook-75cc84fbd4-dkcgt 1 /1 Running 0 50s networking-istio-6dcbd4b5f4-mxm8q 1 /1 Running 0 51s storage-version-migration-serving-serving-0.20.0-82hjt 0 /1 Completed 0 50s webhook-75f5d4845d-zkrdt 1 /1 Running 0 56s NAME READY STATUS RESTARTS AGE eventing-controller-6bc59c9fd7-6svbm 1 /1 Running 0 38s eventing-webhook-85cd479f87-4dwxh 1 /1 Running 0 38s imc-controller-97c4fd87c-t9mnm 1 /1 Running 0 33s imc-dispatcher-c6db95ffd-ln4mc 1 /1 Running 0 33s mt-broker-controller-5f87fbd5d9-m69cd 1 /1 Running 0 32s mt-broker-filter-5b9c64cbd5-d27p4 1 /1 Running 0 32s mt-broker-ingress-55c66fdfdf-gn56g 1 /1 Running 0 32s storage-version-migration-eventing-0.20.0-fvgqf 0 /1 Completed 0 31s sugar-controller-684d5cfdbb-67vsv 1 /1 Running 0 31s You can also verify the status of Knative by checking the CRs: kubectl get KnativeServing knative-serving --namespace knative-serving kubectl get KnativeEventing knative-eventing --namespace knative-eventing These commands return something similar to: NAME VERSION READY REASON knative-serving 0.20.0 True NAME VERSION READY REASON knative-eventing 0.20.0 True Rollback \u00b6 If the upgrade fails, you can always have a rollback solution to restore your Knative to the current version. If your current version is 0.19, you can apply the following CR to restore Knative Serving and Eventing. For Knative Serving: apiVersion: operator.knative.dev/v1alpha1 kind: KnativeServing metadata: name: knative-serving namespace: knative-serving spec: version: \"0.19\" For Knative Eventing: apiVersion: operator.knative.dev/v1alpha1 kind: KnativeEventing metadata: name: knative-eventing namespace: knative-eventing spec: version: \"0.19\"","title":"Upgrading your installation with Knative operator"},{"location":"install/upgrade-installation-with-operator/#upgrading-your-installation-with-knative-operator","text":"The Knative operator supports a straightforward upgrade process. It supports upgrading the Knative component by a single minor version number. For example, if you have v0.17 installed, you must upgrade to v0.18 before attempting to upgrade to v0.19. The attribute spec.version is the only field you need to change in the Serving or Eventing CR to perform an upgrade. You do not need to specify the version in terms of the patch number, because the Knative Operator will match the latest available patch number, as long as you specify major.minor for the version. For example, you only need to specify 0.19 to upgrade to the latest v0.19 release. There is no need to know the exact patch number. The Knative Operator implements a minus 3 principle to support the Knative versions, which means the current version of the Operator can support Knative with the version back 3 in terms of the minor number. For example, if the current version of the Operator is 0.19.x, it bundles and supports the installation of Knative with the versions, 0.16.x, 0.17.x, 0.18.x and 0.19.x.","title":"Upgrading your installation with Knative operator"},{"location":"install/upgrade-installation-with-operator/#before-you-begin","text":"Knative Operator maximizes the automation for the upgrade process, all you need to know is the current version of your Knative, the target version of your Knative, and the namespaces for your Knative installation. In the following instruction, Knative Serving and the Serving custom resource are installed in the knative-serving namespace, and Knative Eventing and the Eventing custom resource are installed in the knative-eventing namespace.","title":"Before you begin"},{"location":"install/upgrade-installation-with-operator/#check-the-current-version-of-the-installed-knative","text":"If you want to check the version of the installed Knative Serving, you can apply the following command: kubectl get KnativeServing knative-serving --namespace knative-serving If your current version for Knative Serving is 0.19.x, you will get the result as below: NAME VERSION READY REASON knative-serving 0.19.0 True As Knative only supports the upgrade with one single minor version, the target version is 0.20 for Knative Serving. The status True means the Serving CR and Knative Serving are in good status. If you want to check the version of the installed Knative Eventing, you can apply the following command: kubectl get KnativeEventing knative-eventing --namespace knative-eventing If your current version for Knative Eventing is 0.19.x, you will get the result as below: NAME VERSION READY REASON knative-eventing 0.19.0 True As Knative only supports the upgrade with one single minor version, the target version is 0.20 for Knative Eventing. The status True means the Eventing CR and Knative Eventing are in good status.","title":"Check the current version of the installed Knative"},{"location":"install/upgrade-installation-with-operator/#performing-the-upgrade","text":"To upgrade, apply the Operator CRs with the same spec, but a different target version for the attribute spec.version . If your existing Serving CR is as below: apiVersion: operator.knative.dev/v1alpha1 kind: KnativeServing metadata: name: knative-serving namespace: knative-serving spec: version: \"0.19\" then apply the following CR to upgrade to 0.20: apiVersion: operator.knative.dev/v1alpha1 kind: KnativeServing metadata: name: knative-serving namespace: knative-serving spec: version: \"0.20\" If your existing Eventing CR is as below: apiVersion: operator.knative.dev/v1alpha1 kind: KnativeEventing metadata: name: knative-eventing namespace: knative-eventing spec: version: \"0.19\" then apply the following CR to upgrade to 0.20: apiVersion: operator.knative.dev/v1alpha1 kind: KnativeEventing metadata: name: knative-eventing namespace: knative-eventing spec: version: \"0.20\"","title":"Performing the upgrade"},{"location":"install/upgrade-installation-with-operator/#verifying-the-upgrade","text":"To confirm that your Knative components have successfully upgraded, view the status of their pods in the relevant namespaces. All pods will restart during the upgrade and their age will reset. If you upgraded Knative Serving and Eventing, enter the following commands to get information about the pods for each namespace: kubectl get pods --namespace knative-serving kubectl get pods --namespace knative-eventing These commands return something similar to: NAME READY STATUS RESTARTS AGE activator-6875896748-gdjgs 1 /1 Running 0 58s autoscaler-6bbc885cfd-vkrgg 1 /1 Running 0 57s autoscaler-hpa-5cdd7c6b69-hxzv4 1 /1 Running 0 55s controller-64dd4bd56-wzb2k 1 /1 Running 0 57s istio-webhook-75cc84fbd4-dkcgt 1 /1 Running 0 50s networking-istio-6dcbd4b5f4-mxm8q 1 /1 Running 0 51s storage-version-migration-serving-serving-0.20.0-82hjt 0 /1 Completed 0 50s webhook-75f5d4845d-zkrdt 1 /1 Running 0 56s NAME READY STATUS RESTARTS AGE eventing-controller-6bc59c9fd7-6svbm 1 /1 Running 0 38s eventing-webhook-85cd479f87-4dwxh 1 /1 Running 0 38s imc-controller-97c4fd87c-t9mnm 1 /1 Running 0 33s imc-dispatcher-c6db95ffd-ln4mc 1 /1 Running 0 33s mt-broker-controller-5f87fbd5d9-m69cd 1 /1 Running 0 32s mt-broker-filter-5b9c64cbd5-d27p4 1 /1 Running 0 32s mt-broker-ingress-55c66fdfdf-gn56g 1 /1 Running 0 32s storage-version-migration-eventing-0.20.0-fvgqf 0 /1 Completed 0 31s sugar-controller-684d5cfdbb-67vsv 1 /1 Running 0 31s You can also verify the status of Knative by checking the CRs: kubectl get KnativeServing knative-serving --namespace knative-serving kubectl get KnativeEventing knative-eventing --namespace knative-eventing These commands return something similar to: NAME VERSION READY REASON knative-serving 0.20.0 True NAME VERSION READY REASON knative-eventing 0.20.0 True","title":"Verifying the upgrade"},{"location":"install/upgrade-installation-with-operator/#rollback","text":"If the upgrade fails, you can always have a rollback solution to restore your Knative to the current version. If your current version is 0.19, you can apply the following CR to restore Knative Serving and Eventing. For Knative Serving: apiVersion: operator.knative.dev/v1alpha1 kind: KnativeServing metadata: name: knative-serving namespace: knative-serving spec: version: \"0.19\" For Knative Eventing: apiVersion: operator.knative.dev/v1alpha1 kind: KnativeEventing metadata: name: knative-eventing namespace: knative-eventing spec: version: \"0.19\"","title":"Rollback"},{"location":"install/upgrade-installation/","text":"Macro Rendering Error \u00b6 TemplateSyntaxError : unexpected '<' Traceback (most recent call last): File \"/usr/local/lib/python3.9/site-packages/mkdocs_macros/plugin.py\", line 441, in render md_template = self.env.from_string(markdown) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 941, in from_string return cls.from_code(self, self.compile(source), globals, None) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 638, in compile self.handle_exception(source=source_hint) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 832, in handle_exception reraise(*rewrite_traceback_stack(source=source)) File \"/usr/local/lib/python3.9/site-packages/jinja2/_compat.py\", line 28, in reraise raise value.with_traceback(tb) File \"<unknown>\", line 59, in template jinja2.exceptions.TemplateSyntaxError: unexpected '<'","title":"Upgrading your installation"},{"location":"install/upgrade-installation/#macro-rendering-error","text":"TemplateSyntaxError : unexpected '<' Traceback (most recent call last): File \"/usr/local/lib/python3.9/site-packages/mkdocs_macros/plugin.py\", line 441, in render md_template = self.env.from_string(markdown) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 941, in from_string return cls.from_code(self, self.compile(source), globals, None) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 638, in compile self.handle_exception(source=source_hint) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 832, in handle_exception reraise(*rewrite_traceback_stack(source=source)) File \"/usr/local/lib/python3.9/site-packages/jinja2/_compat.py\", line 28, in reraise raise value.with_traceback(tb) File \"<unknown>\", line 59, in template jinja2.exceptions.TemplateSyntaxError: unexpected '<'","title":"Macro Rendering Error"},{"location":"install/collecting-logs/","text":"This document describes how to set up Fluent Bit , a log processor and forwarder, to collect your kubernetes logs in a central directory. This is not required for running Knative, but can be helpful with Knative Serving , which will automatically delete pods (and their associated logs) when they are no longer needed. Note that Fluent Bit supports exporting to a number of other log providers; if you already have an existing log provider (for example, Splunk, Datadog, ElasticSearch, or Stackdriver), then you may only need the second part of setting up and configuring log forwarders . Setting up log collection consists of two pieces: running a log forwarding DaemonSet on each node, and running a collector somewhere in the cluster (in our example, we use a StatefulSet which stores logs on a Kubernetes PersistentVolumeClaim, but you could also use a HostPath). Setting up the collector \u00b6 It's useful to set up the collector before the forwarders, because you'll need the address of the collector when configuring the forwarders, and the forwarders may queue logs until the collector is ready. The fluent-bit-collector.yaml defines a StatefulSet as well as a Kubernetes Service which allows accessing and reading the logs from within the cluster. The supplied configuration will create the monitoring configuration in a namespace called logging . You can apply the configuration with: kubectl apply --filename https://github.com/knative/docs/raw/main/docs/install/collecting-logs/fluent-bit-collector.yaml The default configuration will classify logs into Knative, apps (pods with an app= label which aren't Knative), and the default to logging with the pod name; this can be changed by updating the log-collector-config ConfigMap before or after installation. Once the ConfigMap is updated, you'll need to restart Fluent Bit (for example, by deleting the pod and letting the StatefulSet recreate it). To access the logs through your web browser: kubectl port-forward --namespace logging service/log-collector 8080 :80 And then visit http://localhost:8080/. You can also open a shell in the nginx pod and search the logs using unix tools: kubectl exec --namespace logging --stdin --tty --container nginx log-collector-0 Setting up the forwarders \u00b6 For the most part, you can follow the Fluent Bit directions for installing on Kubernetes . Those directions will set up a Fluent Bit DaemonSet which forwards logs to ElasticSearch by default; when the directions call for creating the ConfigMap, you'll want to either replace the elasticsearch configuration with this fluent-bit-configmap.yaml or add the following block to the ConfigMap and update the @INCLUDE output-elasticsearch.conf to be @INCLUDE output-forward.conf . output-forward.conf : | [OUTPUT] Name forward Host log-collector.logging Port 24224 Require_ack_response True If you are using a different log collection infrastructure (Splunk, for example), follow the directions in the FluentBit documentation on how to configure your forwarders.","title":"Index"},{"location":"install/collecting-logs/#setting-up-the-collector","text":"It's useful to set up the collector before the forwarders, because you'll need the address of the collector when configuring the forwarders, and the forwarders may queue logs until the collector is ready. The fluent-bit-collector.yaml defines a StatefulSet as well as a Kubernetes Service which allows accessing and reading the logs from within the cluster. The supplied configuration will create the monitoring configuration in a namespace called logging . You can apply the configuration with: kubectl apply --filename https://github.com/knative/docs/raw/main/docs/install/collecting-logs/fluent-bit-collector.yaml The default configuration will classify logs into Knative, apps (pods with an app= label which aren't Knative), and the default to logging with the pod name; this can be changed by updating the log-collector-config ConfigMap before or after installation. Once the ConfigMap is updated, you'll need to restart Fluent Bit (for example, by deleting the pod and letting the StatefulSet recreate it). To access the logs through your web browser: kubectl port-forward --namespace logging service/log-collector 8080 :80 And then visit http://localhost:8080/. You can also open a shell in the nginx pod and search the logs using unix tools: kubectl exec --namespace logging --stdin --tty --container nginx log-collector-0","title":"Setting up the collector"},{"location":"install/collecting-logs/#setting-up-the-forwarders","text":"For the most part, you can follow the Fluent Bit directions for installing on Kubernetes . Those directions will set up a Fluent Bit DaemonSet which forwards logs to ElasticSearch by default; when the directions call for creating the ConfigMap, you'll want to either replace the elasticsearch configuration with this fluent-bit-configmap.yaml or add the following block to the ConfigMap and update the @INCLUDE output-elasticsearch.conf to be @INCLUDE output-forward.conf . output-forward.conf : | [OUTPUT] Name forward Host log-collector.logging Port 24224 Require_ack_response True If you are using a different log collection infrastructure (Splunk, for example), follow the directions in the FluentBit documentation on how to configure your forwarders.","title":"Setting up the forwarders"},{"location":"install/collecting-logs/_index/","text":"Macro Rendering Error \u00b6 TemplateSyntaxError : unexpected '%' Traceback (most recent call last): File \"/usr/local/lib/python3.9/site-packages/mkdocs_macros/plugin.py\", line 441, in render md_template = self.env.from_string(markdown) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 941, in from_string return cls.from_code(self, self.compile(source), globals, None) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 638, in compile self.handle_exception(source=source_hint) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 832, in handle_exception reraise(*rewrite_traceback_stack(source=source)) File \"/usr/local/lib/python3.9/site-packages/jinja2/_compat.py\", line 28, in reraise raise value.with_traceback(tb) File \"<unknown>\", line 3, in template jinja2.exceptions.TemplateSyntaxError: unexpected '%'","title":"Collecting Logs with Fluentbit"},{"location":"install/collecting-logs/_index/#macro-rendering-error","text":"TemplateSyntaxError : unexpected '%' Traceback (most recent call last): File \"/usr/local/lib/python3.9/site-packages/mkdocs_macros/plugin.py\", line 441, in render md_template = self.env.from_string(markdown) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 941, in from_string return cls.from_code(self, self.compile(source), globals, None) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 638, in compile self.handle_exception(source=source_hint) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 832, in handle_exception reraise(*rewrite_traceback_stack(source=source)) File \"/usr/local/lib/python3.9/site-packages/jinja2/_compat.py\", line 28, in reraise raise value.with_traceback(tb) File \"<unknown>\", line 3, in template jinja2.exceptions.TemplateSyntaxError: unexpected '%'","title":"Macro Rendering Error"},{"location":"install/collecting-metrics/_index/","text":"Collecting Metrics with OpenTelemetry \u00b6 This document describes how to set up the OpenTelemetry Collector to receive metrics from the Knative infrastructure components and distribute them to Prometheus. OpenTelemetry is a CNCF an observability framework for cloud-native software. The project provides a collection of tools, APIs, and SDKs. You use it to instrument, generate, collect, and export telemetry data (metrics, logs, and traces) for analysis in order to understand your software's performance and behavior. OpenTelemetry allows Knative to build provider-agnostic instrumentation into the platform, so that it's easy to export metrics to multiple monitoring services without needing to rebuild or reconfigure the Knative binaries. Setting up the collector \u00b6 The collector provides a long-lived location where various Knative components can push metrics (and eventually traces) to be retained and collected by a monitoring service. For this example, we'll configure a single collector instance using a ConfigMap and a Deployment. For more complex deployments, some of this can be automated using the opentelemetry-operator , but it's also easy to manage this service directly. Note that you can attach other components (node agents, other services); this is just a simple sample. First, create a namespace for the collector to run in: kubectl create namespace metrics And then create a Deployment, Service, and ConfigMap for the collector: kubectl apply --filename https://raw.githubusercontent.com/knative/docs/master/docs/install/collecting-metrics/collector.yaml Finally, update the config-observability ConfigMap in Knative Serving and Eventing kubectl patch --namespace knative-serving configmap/config-observability \\ --type merge \\ --patch '{\"data\":{\"metrics.backend-destination\":\"opencensus\",\"request-metrics-backend-destination\":\"opencensus\",\"metrics.opencensus-address\":\"otel-collector.metrics:55678\"}}' kubectl patch --namespace knative-eventing configmap/config-observability \\ --type merge \\ --patch '{\"data\":{\"metrics.backend-destination\":\"opencensus\",\"metrics.opencensus-address\":\"otel-collector.metrics:55678\"}}' You can check that metrics are being forwarded by loading the Prometheus export port on the collector: kubectl port-forward --namespace metrics deployment/otel-collector 8889 And then fetch http://localhost:8889/metrics to see the exported metrics. Setting up Prometheus \u00b6 Prometheus is an open-source tool for collecting and aggregating timeseries metrics. Full configuration of Prometheus can be found at the website, but this document will provide a simple setup for scraping the OpenTelemetry Collector we set up in the previous section. Install the Prometheus Operator . Note that the provided manifest installs the operator into the default namespace. If you want to install into another namespace, you'll need to download the YAML manifest and update all the namespace references to your target namespace. kubectl apply --filename https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/master/bundle.yaml You'll then need to set up a ServiceMonitor object to track the OpenTelemetry Collector, as well as a ServiceAccount with the ability to read Kubernetes services and pods (so that Prometheus can track the resource endpoints) and finally a Prometheus object to instantiate the actual Prometheus instance. kubectl apply --filename prometheus.yaml By default, the Prometheus instance will only be exposed on a private service named prometheus-operated ; to access the console in your web browser, run: kubectl port-forward --namespace metrics service/prometheus-operated 9090 And then access the console in your browser via http://localhost:9090.","title":"Collecting Metrics with OpenTelemetry"},{"location":"install/collecting-metrics/_index/#collecting-metrics-with-opentelemetry","text":"This document describes how to set up the OpenTelemetry Collector to receive metrics from the Knative infrastructure components and distribute them to Prometheus. OpenTelemetry is a CNCF an observability framework for cloud-native software. The project provides a collection of tools, APIs, and SDKs. You use it to instrument, generate, collect, and export telemetry data (metrics, logs, and traces) for analysis in order to understand your software's performance and behavior. OpenTelemetry allows Knative to build provider-agnostic instrumentation into the platform, so that it's easy to export metrics to multiple monitoring services without needing to rebuild or reconfigure the Knative binaries.","title":"Collecting Metrics with OpenTelemetry"},{"location":"install/collecting-metrics/_index/#setting-up-the-collector","text":"The collector provides a long-lived location where various Knative components can push metrics (and eventually traces) to be retained and collected by a monitoring service. For this example, we'll configure a single collector instance using a ConfigMap and a Deployment. For more complex deployments, some of this can be automated using the opentelemetry-operator , but it's also easy to manage this service directly. Note that you can attach other components (node agents, other services); this is just a simple sample. First, create a namespace for the collector to run in: kubectl create namespace metrics And then create a Deployment, Service, and ConfigMap for the collector: kubectl apply --filename https://raw.githubusercontent.com/knative/docs/master/docs/install/collecting-metrics/collector.yaml Finally, update the config-observability ConfigMap in Knative Serving and Eventing kubectl patch --namespace knative-serving configmap/config-observability \\ --type merge \\ --patch '{\"data\":{\"metrics.backend-destination\":\"opencensus\",\"request-metrics-backend-destination\":\"opencensus\",\"metrics.opencensus-address\":\"otel-collector.metrics:55678\"}}' kubectl patch --namespace knative-eventing configmap/config-observability \\ --type merge \\ --patch '{\"data\":{\"metrics.backend-destination\":\"opencensus\",\"metrics.opencensus-address\":\"otel-collector.metrics:55678\"}}' You can check that metrics are being forwarded by loading the Prometheus export port on the collector: kubectl port-forward --namespace metrics deployment/otel-collector 8889 And then fetch http://localhost:8889/metrics to see the exported metrics.","title":"Setting up the collector"},{"location":"install/collecting-metrics/_index/#setting-up-prometheus","text":"Prometheus is an open-source tool for collecting and aggregating timeseries metrics. Full configuration of Prometheus can be found at the website, but this document will provide a simple setup for scraping the OpenTelemetry Collector we set up in the previous section. Install the Prometheus Operator . Note that the provided manifest installs the operator into the default namespace. If you want to install into another namespace, you'll need to download the YAML manifest and update all the namespace references to your target namespace. kubectl apply --filename https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/master/bundle.yaml You'll then need to set up a ServiceMonitor object to track the OpenTelemetry Collector, as well as a ServiceAccount with the ability to read Kubernetes services and pods (so that Prometheus can track the resource endpoints) and finally a Prometheus object to instantiate the actual Prometheus instance. kubectl apply --filename prometheus.yaml By default, the Prometheus instance will only be exposed on a private service named prometheus-operated ; to access the console in your web browser, run: kubectl port-forward --namespace metrics service/prometheus-operated 9090 And then access the console in your browser via http://localhost:9090.","title":"Setting up Prometheus"},{"location":"install/operator/configuring-eventing-cr/","text":"Configuring the Eventing Operator custom resource \u00b6 You can configure the Knative Eventing operator by modifying settings in the KnativeEventing custom resource (CR). NOTE: Kubernetes spec level policies cannot be configured using the Knative Operators. Installing a specific version of Eventing \u00b6 Cluster administrators can install a specific version of Knative Eventing by using the spec.version field. For example, if you want to install Knative Eventing v0.19.0, you can apply the following KnativeEventing CR: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : version : 0.19.0 If spec.version is not specified, the Knative Operator will install the latest available version of Knative Eventing. If users specify an invalid or unavailable version, the Knative Operator will do nothing. The Knative Operator always includes the latest 3 minor release versions. If Knative Eventing is already managed by the Operator, updating the spec.version field in the KnativeEventing CR enables upgrading or downgrading the Knative Eventing version, without requiring modifications to the Operator. Note that the Knative Operator only permits upgrades or downgrades by one minor release version at a time. For example, if the current Knative Eventing deployment is version 0.18.x, you must upgrade to 0.19.x before upgrading to 0.20.x. Configuring Knative Eventing using ConfigMaps \u00b6 The Operator manages the Knative Eventing installation. It overwrites any updates to ConfigMaps which are used to configure Knative Eventing. The KnativeEventing CR allows you to set values for these ConfigMaps by using the Operator. All Knative Eventing ConfigMaps are created in the same namespace as the KnativeEventing CR. You can use the KnativeEventing CR as a unique entry point to edit all ConfigMaps. Knative Eventing has multiple ConfigMaps that are named with the prefix config- . The spec.config in the KnativeEventing CR has one <name> entry for each ConfigMap, named config-<name> , with a value which will be used for the ConfigMap data . Setting a default channel \u00b6 If you are using different channel implementations, like the KafkaChannel, or you want a specific configuration of the InMemoryChannel to be the default configuration, you can change the default behavior by updating the default-ch-webhook ConfigMap. You can do this by modifying the KnativeEventing CR: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : config : default-ch-webhook : default-ch-config : | clusterDefault: apiVersion: messaging.knative.dev/v1beta1 kind: KafkaChannel spec: numPartitions: 10 replicationFactor: 1 namespaceDefaults: my-namespace: apiVersion: messaging.knative.dev/v1 kind: InMemoryChannel spec: delivery: backoffDelay: PT0.5S backoffPolicy: exponential retry: 5 NOTE: The clusterDefault setting determines the global, cluster-wide default channel type. You can configure channel defaults for individual namespaces by using the namespaceDefaults setting. Setting the default channel for the broker \u00b6 If you are using a channel-based broker, you can change the default channel type for the broker from InMemoryChannel to KafkaChannel, by updating the config-br-default-channel ConfigMap. You can do this by modifying the KnativeEventing CR: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : config : config-br-default-channel : channelTemplateSpec : | apiVersion: messaging.knative.dev/v1beta1 kind: KafkaChannel spec: numPartitions: 6 replicationFactor: 1 Private repository and private secrets \u00b6 The Knative Eventing Operator CR is configured the same way as the Knative Serving Operator CR. See the documentation on Private repository and private secret . Knative Eventing also specifies only one container within each Deployment resource. However, the container does not use the same name as its parent Deployment, which means that the container name in Knative Eventing is not the same unique identifier as it is in Knative Serving. List of containers within each Deployment resource: Component Deployment name Container name Core eventing eventing-controller eventing-controller Core eventing eventing-webhook eventing-webhook Eventing Broker broker-controller eventing-controller In-Memory Channel imc-controller controller In-Memory Channel imc-dispatcher dispatcher The default field can still be used to replace the images in a predefined format. However, if the container name is not a unique identifier, for example eventing-controller , you must use the override field to replace it, by specifying deployment/container as the unique key. Some images are defined by using the environment variable in Knative Eventing. They can be replaced by taking advantage of the override field. Download images in a predefined format without secrets \u00b6 This example shows how you can define custom image links that can be defined in the KnativeEventing CR using the simplified format docker.io/knative-images/${NAME}:{CUSTOM-TAG} . In the example below: The custom tag latest is used for all images. All image links are accessible without using secrets. Images are defined in the accepted format docker.io/knative-images/${NAME}:{CUSTOM-TAG} . Push images to the following image tags: Deployment Container Docker image eventing-controller eventing-controller docker.io/knative-images/eventing-controller:latest eventing-webhook docker.io/knative-images/eventing-webhook:latest broker-controller eventing-controller docker.io/knative-images/broker-eventing-controller:latest controller docker.io/knative-images/controller:latest dispatcher docker.io/knative-images/dispatcher:latest Define your the KnativeEventing CR with following content: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : registry : default : docker.io/knative-images/${NAME}:latest override : broker-controller/eventing-controller : docker.io/knative-images-repo1/broker-eventing-controller:latest - `${NAME}` maps to the container name in each `Deployment` resource. - `default` is used to define the image format for all containers, except the container `eventing-controller` in the deployment `broker-controller`. To replace the image for this container, use the `override` field to specify individually, by using `broker-controller/eventing-controller` as the key. Download images from different repositories without secrets \u00b6 If your custom image links are not defined in a uniform format, you will need to individually include each link in the KnativeEventing CR. For example, to define the following list of images: Deployment Container Docker Image eventing-controller eventing-controller docker.io/knative-images/eventing-controller:latest eventing-webhook docker.io/knative-images/eventing-webhook:latest controller docker.io/knative-images/controller:latest dispatcher docker.io/knative-images/dispatcher:latest broker-controller eventing-controller docker.io/knative-images/broker-eventing-controller:latest The KnativeEventing CR must be modified to include the full list. For example: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : registry : override : eventing-controller/eventing-controller : docker.io/knative-images-repo1/eventing-controller:latest eventing-webhook/eventing-webhook : docker.io/knative-images-repo2/eventing-webhook:latest imc-controller/controller : docker.io/knative-images-repo3/imc-controller:latest imc-dispatcher/dispatcher : docker.io/knative-images-repo4/imc-dispatcher:latest broker-controller/eventing-controller : docker.io/knative-images-repo5/broker-eventing-controller:latest If you want to replace the image defined by the environment variable, you must modify the KnativeEventing CR. For example, if you want to replace the image defined by the environment variable DISPATCHER_IMAGE , in the container controller , of the deployment imc-controller , and the target image is docker.io/knative-images-repo5/DISPATCHER_IMAGE:latest , the KnativeEventing CR would be as follows: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : registry : override : eventing-controller/eventing-controller : docker.io/knative-images-repo1/eventing-controller:latest eventing-webhook/eventing-webhook : docker.io/knative-images-repo2/eventing-webhook:latest imc-controller/controller : docker.io/knative-images-repo3/imc-controller:latest imc-dispatcher/dispatcher : docker.io/knative-images-repo4/imc-dispatcher:latest broker-controller/eventing-controller : docker.io/knative-images-repo5/broker-eventing-controller:latest DISPATCHER_IMAGE : docker.io/knative-images-repo5/DISPATCHER_IMAGE:latest Download images with secrets \u00b6 If your image repository requires private secrets for access, you must append the imagePullSecrets attribute to the KnativeEventing CR. This example uses a secret named regcred . Refer to the Kubernetes documentation to create your own private secrets. After you create the secret, edit the KnativeEventing CR: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : registry : ... imagePullSecrets : - name : regcred The field imagePullSecrets requires a list of secrets. You can add multiple secrets to access the images: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : registry : ... imagePullSecrets : - name : regcred - name : regcred-2 ... Configuring the default broker class \u00b6 Knative Eventing allows you to define a default broker class when the user does not specify one. The Operator provides two broker classes by default: ChannelBasedBroker and MTChannelBasedBroker. The field defaultBrokerClass indicates which class to use; if empty, the ChannelBasedBroker is used. The following example CR specifies MTChannelBasedBroker as the default: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : defaultBrokerClass : MTChannelBasedBroker System resource settings \u00b6 The KnativeEventing CR allows you to configure system resources for Knative system containers. Requests and limits can be configured for the following containers: eventing-controller eventing-webhook imc-controller imc-dispatcher mt-broker-ingress mt-broker-ingress mt-broker-controller To override resource settings for a specific container, you must create an entry in the spec.resources list with the container name and the Kubernetes resource settings . For example, the following KnativeEventing CR configures the eventing-webhook container to request 0.3 CPU and 100MB of RAM, and sets hard limits of 1 CPU, 250MB RAM, and 4GB of local storage: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : resources : - container : eventing-webhook requests : cpu : 300m memory : 100Mi limits : cpu : 1000m memory : 250Mi","title":"Configuring the Eventing Operator custom resource"},{"location":"install/operator/configuring-eventing-cr/#configuring-the-eventing-operator-custom-resource","text":"You can configure the Knative Eventing operator by modifying settings in the KnativeEventing custom resource (CR). NOTE: Kubernetes spec level policies cannot be configured using the Knative Operators.","title":"Configuring the Eventing Operator custom resource"},{"location":"install/operator/configuring-eventing-cr/#installing-a-specific-version-of-eventing","text":"Cluster administrators can install a specific version of Knative Eventing by using the spec.version field. For example, if you want to install Knative Eventing v0.19.0, you can apply the following KnativeEventing CR: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : version : 0.19.0 If spec.version is not specified, the Knative Operator will install the latest available version of Knative Eventing. If users specify an invalid or unavailable version, the Knative Operator will do nothing. The Knative Operator always includes the latest 3 minor release versions. If Knative Eventing is already managed by the Operator, updating the spec.version field in the KnativeEventing CR enables upgrading or downgrading the Knative Eventing version, without requiring modifications to the Operator. Note that the Knative Operator only permits upgrades or downgrades by one minor release version at a time. For example, if the current Knative Eventing deployment is version 0.18.x, you must upgrade to 0.19.x before upgrading to 0.20.x.","title":"Installing a specific version of Eventing"},{"location":"install/operator/configuring-eventing-cr/#configuring-knative-eventing-using-configmaps","text":"The Operator manages the Knative Eventing installation. It overwrites any updates to ConfigMaps which are used to configure Knative Eventing. The KnativeEventing CR allows you to set values for these ConfigMaps by using the Operator. All Knative Eventing ConfigMaps are created in the same namespace as the KnativeEventing CR. You can use the KnativeEventing CR as a unique entry point to edit all ConfigMaps. Knative Eventing has multiple ConfigMaps that are named with the prefix config- . The spec.config in the KnativeEventing CR has one <name> entry for each ConfigMap, named config-<name> , with a value which will be used for the ConfigMap data .","title":"Configuring Knative Eventing using ConfigMaps"},{"location":"install/operator/configuring-eventing-cr/#setting-a-default-channel","text":"If you are using different channel implementations, like the KafkaChannel, or you want a specific configuration of the InMemoryChannel to be the default configuration, you can change the default behavior by updating the default-ch-webhook ConfigMap. You can do this by modifying the KnativeEventing CR: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : config : default-ch-webhook : default-ch-config : | clusterDefault: apiVersion: messaging.knative.dev/v1beta1 kind: KafkaChannel spec: numPartitions: 10 replicationFactor: 1 namespaceDefaults: my-namespace: apiVersion: messaging.knative.dev/v1 kind: InMemoryChannel spec: delivery: backoffDelay: PT0.5S backoffPolicy: exponential retry: 5 NOTE: The clusterDefault setting determines the global, cluster-wide default channel type. You can configure channel defaults for individual namespaces by using the namespaceDefaults setting.","title":"Setting a default channel"},{"location":"install/operator/configuring-eventing-cr/#setting-the-default-channel-for-the-broker","text":"If you are using a channel-based broker, you can change the default channel type for the broker from InMemoryChannel to KafkaChannel, by updating the config-br-default-channel ConfigMap. You can do this by modifying the KnativeEventing CR: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : config : config-br-default-channel : channelTemplateSpec : | apiVersion: messaging.knative.dev/v1beta1 kind: KafkaChannel spec: numPartitions: 6 replicationFactor: 1","title":"Setting the default channel for the broker"},{"location":"install/operator/configuring-eventing-cr/#private-repository-and-private-secrets","text":"The Knative Eventing Operator CR is configured the same way as the Knative Serving Operator CR. See the documentation on Private repository and private secret . Knative Eventing also specifies only one container within each Deployment resource. However, the container does not use the same name as its parent Deployment, which means that the container name in Knative Eventing is not the same unique identifier as it is in Knative Serving. List of containers within each Deployment resource: Component Deployment name Container name Core eventing eventing-controller eventing-controller Core eventing eventing-webhook eventing-webhook Eventing Broker broker-controller eventing-controller In-Memory Channel imc-controller controller In-Memory Channel imc-dispatcher dispatcher The default field can still be used to replace the images in a predefined format. However, if the container name is not a unique identifier, for example eventing-controller , you must use the override field to replace it, by specifying deployment/container as the unique key. Some images are defined by using the environment variable in Knative Eventing. They can be replaced by taking advantage of the override field.","title":"Private repository and private secrets"},{"location":"install/operator/configuring-eventing-cr/#download-images-in-a-predefined-format-without-secrets","text":"This example shows how you can define custom image links that can be defined in the KnativeEventing CR using the simplified format docker.io/knative-images/${NAME}:{CUSTOM-TAG} . In the example below: The custom tag latest is used for all images. All image links are accessible without using secrets. Images are defined in the accepted format docker.io/knative-images/${NAME}:{CUSTOM-TAG} . Push images to the following image tags: Deployment Container Docker image eventing-controller eventing-controller docker.io/knative-images/eventing-controller:latest eventing-webhook docker.io/knative-images/eventing-webhook:latest broker-controller eventing-controller docker.io/knative-images/broker-eventing-controller:latest controller docker.io/knative-images/controller:latest dispatcher docker.io/knative-images/dispatcher:latest Define your the KnativeEventing CR with following content: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : registry : default : docker.io/knative-images/${NAME}:latest override : broker-controller/eventing-controller : docker.io/knative-images-repo1/broker-eventing-controller:latest - `${NAME}` maps to the container name in each `Deployment` resource. - `default` is used to define the image format for all containers, except the container `eventing-controller` in the deployment `broker-controller`. To replace the image for this container, use the `override` field to specify individually, by using `broker-controller/eventing-controller` as the key.","title":"Download images in a predefined format without secrets"},{"location":"install/operator/configuring-eventing-cr/#download-images-from-different-repositories-without-secrets","text":"If your custom image links are not defined in a uniform format, you will need to individually include each link in the KnativeEventing CR. For example, to define the following list of images: Deployment Container Docker Image eventing-controller eventing-controller docker.io/knative-images/eventing-controller:latest eventing-webhook docker.io/knative-images/eventing-webhook:latest controller docker.io/knative-images/controller:latest dispatcher docker.io/knative-images/dispatcher:latest broker-controller eventing-controller docker.io/knative-images/broker-eventing-controller:latest The KnativeEventing CR must be modified to include the full list. For example: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : registry : override : eventing-controller/eventing-controller : docker.io/knative-images-repo1/eventing-controller:latest eventing-webhook/eventing-webhook : docker.io/knative-images-repo2/eventing-webhook:latest imc-controller/controller : docker.io/knative-images-repo3/imc-controller:latest imc-dispatcher/dispatcher : docker.io/knative-images-repo4/imc-dispatcher:latest broker-controller/eventing-controller : docker.io/knative-images-repo5/broker-eventing-controller:latest If you want to replace the image defined by the environment variable, you must modify the KnativeEventing CR. For example, if you want to replace the image defined by the environment variable DISPATCHER_IMAGE , in the container controller , of the deployment imc-controller , and the target image is docker.io/knative-images-repo5/DISPATCHER_IMAGE:latest , the KnativeEventing CR would be as follows: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : registry : override : eventing-controller/eventing-controller : docker.io/knative-images-repo1/eventing-controller:latest eventing-webhook/eventing-webhook : docker.io/knative-images-repo2/eventing-webhook:latest imc-controller/controller : docker.io/knative-images-repo3/imc-controller:latest imc-dispatcher/dispatcher : docker.io/knative-images-repo4/imc-dispatcher:latest broker-controller/eventing-controller : docker.io/knative-images-repo5/broker-eventing-controller:latest DISPATCHER_IMAGE : docker.io/knative-images-repo5/DISPATCHER_IMAGE:latest","title":"Download images from different repositories without secrets"},{"location":"install/operator/configuring-eventing-cr/#download-images-with-secrets","text":"If your image repository requires private secrets for access, you must append the imagePullSecrets attribute to the KnativeEventing CR. This example uses a secret named regcred . Refer to the Kubernetes documentation to create your own private secrets. After you create the secret, edit the KnativeEventing CR: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : registry : ... imagePullSecrets : - name : regcred The field imagePullSecrets requires a list of secrets. You can add multiple secrets to access the images: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : registry : ... imagePullSecrets : - name : regcred - name : regcred-2 ...","title":"Download images with secrets"},{"location":"install/operator/configuring-eventing-cr/#configuring-the-default-broker-class","text":"Knative Eventing allows you to define a default broker class when the user does not specify one. The Operator provides two broker classes by default: ChannelBasedBroker and MTChannelBasedBroker. The field defaultBrokerClass indicates which class to use; if empty, the ChannelBasedBroker is used. The following example CR specifies MTChannelBasedBroker as the default: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : defaultBrokerClass : MTChannelBasedBroker","title":"Configuring the default broker class"},{"location":"install/operator/configuring-eventing-cr/#system-resource-settings","text":"The KnativeEventing CR allows you to configure system resources for Knative system containers. Requests and limits can be configured for the following containers: eventing-controller eventing-webhook imc-controller imc-dispatcher mt-broker-ingress mt-broker-ingress mt-broker-controller To override resource settings for a specific container, you must create an entry in the spec.resources list with the container name and the Kubernetes resource settings . For example, the following KnativeEventing CR configures the eventing-webhook container to request 0.3 CPU and 100MB of RAM, and sets hard limits of 1 CPU, 250MB RAM, and 4GB of local storage: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : resources : - container : eventing-webhook requests : cpu : 300m memory : 100Mi limits : cpu : 1000m memory : 250Mi","title":"System resource settings"},{"location":"install/operator/configuring-serving-cr/","text":"Configuring the Serving Operator Custom Resource \u00b6 The Knative Serving operator can be configured with these options: Version Configuration Serving Configuration by ConfigMap Private repository and private secret SSL certificate for controller Knative ingress gateway Cluster local gateway High availability System Resource Settings Override system deployments Version Configuration \u00b6 Cluster administrators can install a specific version of Knative Serving by using the spec.version field. For example, if you want to install Knative Serving 0.16.0, you can apply the following KnativeServing custom resource: apiVersion: operator.knative.dev/v1alpha1 kind: KnativeServing metadata: name: knative-serving namespace: knative-serving spec: version: 0.16.0 If spec.version is not specified, the Knative Operator will install the latest available version of Knative Serving. If users specify an invalid or unavailable version, the Knative Operator will do nothing. The Knative Operator always includes the latest 3 minor release versions. For example, if the current version of the Knative Operator is 0.16.x, the earliest version of Knative Serving available through the Operator is 0.14.0. If Knative Serving is already managed by the Operator, updating the spec.version field in the KnativeServing resource enables upgrading or downgrading the Knative Serving version, without needing to change the Operator. Note that the Knative Operator only permits upgrades or downgrades by one minor release version at a time. For example, if the current Knative Serving deployment is version 0.14.x, you must upgrade to 0.15.x before upgrading to 0.16.x. Serving Configuration by ConfigMap \u00b6 The Operator manages the Knative Serving installation. It overwrites any updates to ConfigMaps which are used to configure Knative Serving. The KnativeServing custom resource (CR) allows you to set values for these ConfigMaps by using the Operator. Knative Serving has multiple ConfigMaps that are named with the prefix config- . The spec.config in the KnativeServing CR has one <name> entry for each ConfigMap, named config-<name> , with a value which will be used for the ConfigMap data . In the setup a custom domain example , you can see the content of the ConfigMap config-domain is: apiVersion: v1 kind: ConfigMap metadata: name: config-domain namespace: knative-serving data: example.org: | selector: app: prod example.com: \"\" Using the operator, specify the ConfigMap config-domain using the operator CR: apiVersion: operator.knative.dev/v1alpha1 kind: KnativeServing metadata: name: knative-serving namespace: knative-serving spec: config: domain: example.org: | selector: app: prod example.com: \"\" You can apply values to multiple ConfigMaps. This example sets stable-window to 60s in config-autoscaler as well as specifying config-domain : apiVersion: operator.knative.dev/v1alpha1 kind: KnativeServing metadata: name: knative-serving namespace: knative-serving spec: config: domain: example.org: | selector: app: prod example.com: \"\" autoscaler: stable-window: \"60s\" All the ConfigMaps are created in the same namespace as the operator CR. You can use the operator CR as the unique entry point to edit all of them. Private repository and private secrets \u00b6 You can use the spec.registry section of the operator CR to change the image references to point to a private registry or specify imagePullSecrets : default : this field defines a image reference template for all Knative images. The format is example-registry.io/custom/path/${NAME}:{CUSTOM-TAG} . If you use the same tag for all your images, the only difference is the image name. ${NAME} is a pre-defined variable in the operator corresponding to the container name. If you name the images in your private repo to align with the container names ( activator , autoscaler , controller , webhook , autoscaler-hpa , networking-istio , and queue-proxy ), the default argument should be sufficient. override : a map from container name to the full registry location. This section is only needed when the registry images do not match the common naming format. For containers whose name matches a key, the value is used in preference to the image name calculated by default . If a container's name does not match a key in override , the template in default is used. imagePullSecrets : a list of Secret names used when pulling Knative container images. The Secrets must be created in the same namespace as the Knative Serving Deployments. See deploying images from a private container registry for configuration details. Download images in a predefined format without secrets: \u00b6 This example shows how you can define custom image links that can be defined in the CR using the simplified format docker.io/knative-images/${NAME}:{CUSTOM-TAG} . In the example below: the custom tag v0.13.0 is used for all images all image links are accessible without using secrets images are pushed as docker.io/knative-images/${NAME}:{CUSTOM-TAG} First, you need to make sure your images pushed to the following image tags: Container Docker Image activator docker.io/knative-images/activator:v0.13.0 autoscaler docker.io/knative-images/autoscaler:v0.13.0 controller docker.io/knative-images/controller:v0.13.0 webhook docker.io/knative-images/webhook:v0.13.0 autoscaler-hpa docker.io/knative-images/autoscaler-hpa:v0.13.0 networking-istio docker.io/knative-images/networking-istio:v0.13.0 queue-proxy docker.io/knative-images/queue-proxy:v0.13.0 Then, you need to define your operator CR with following content: apiVersion: operator.knative.dev/v1alpha1 kind: KnativeServing metadata: name: knative-serving namespace: knative-serving spec: registry: default: docker.io/knative-images/${NAME}:v0.13.0 Download images individually without secrets: \u00b6 If your custom image links are not defined in a uniform format by default, you will need to individually include each link in the CR. For example, to given the following images: Container Docker Image activator docker.io/knative-images-repo1/activator:v0.13.0 autoscaler docker.io/knative-images-repo2/autoscaler:v0.13.0 controller docker.io/knative-images-repo3/controller:v0.13.0 webhook docker.io/knative-images-repo4/webhook:v0.13.0 autoscaler-hpa docker.io/knative-images-repo5/autoscaler-hpa:v0.13.0 networking-istio docker.io/knative-images-repo6/prefix-networking-istio:v0.13.0 queue-proxy docker.io/knative-images-repo7/queue-proxy-suffix:v0.13.0 The operator CR should be modified to include the full list: apiVersion: operator.knative.dev/v1alpha1 kind: KnativeServing metadata: name: knative-serving namespace: knative-serving spec: registry: override: activator: docker.io/knative-images-repo1/activator:v0.13.0 autoscaler: docker.io/knative-images-repo2/autoscaler:v0.13.0 controller: docker.io/knative-images-repo3/controller:v0.13.0 webhook: docker.io/knative-images-repo4/webhook:v0.13.0 autoscaler-hpa: docker.io/knative-images-repo5/autoscaler-hpa:v0.13.0 networking-istio: docker.io/knative-images-repo6/prefix-networking-istio:v0.13.0 queue-proxy: docker.io/knative-images-repo7/queue-proxy-suffix:v0.13.0 Download images with secrets: \u00b6 If your image repository requires private secrets for access, include the imagePullSecrets attribute. This example uses a secret named regcred . You must create your own private secrets if these are required: From existing docker credentials From command line for docker credentials Create your own secret After you create this secret, edit your operator CR by appending the content below: apiVersion: operator.knative.dev/v1alpha1 kind: KnativeServing metadata: name: knative-serving namespace: knative-serving spec: registry: ... imagePullSecrets: - name: regcred The field imagePullSecrets expects a list of secrets. You can add multiple secrets to access the images as below: apiVersion: operator.knative.dev/v1alpha1 kind: KnativeServing metadata: name: knative-serving namespace: knative-serving spec: registry: ... imagePullSecrets: - name: regcred - name: regcred-2 ... SSL certificate for controller \u00b6 To enable tag to digest resolution , the Knative Serving controller needs to access the container registry. To allow the controller to trust a self-signed registry cert, you can use the Operator to specify the certificate using a ConfigMap or Secret. Specify the following fields in spec.controller-custom-certs to select a custom registry certificate: name : the name of the ConfigMap or Secret. type : either the string \"ConfigMap\" or \"Secret\". If you create a ConfigMap named testCert containing the certificate, change your CR: apiVersion: operator.knative.dev/v1alpha1 kind: KnativeServing metadata: name: knative-serving namespace: knative-serving spec: controller-custom-certs: name: testCert type: ConfigMap Configuration of Knative ingress gateway \u00b6 To set up custom ingress gateway, follow Step 1: Create Gateway Service and Deployment Instance . Step 2: Update the Knative gateway \u00b6 Update spec.ingress.istio.knative-ingress-gateway to select the labels of the new ingress gateway: apiVersion: operator.knative.dev/v1alpha1 kind: KnativeServing metadata: name: knative-serving namespace: knative-serving spec: ingress: istio: enabled: true knative-ingress-gateway: selector: custom: ingressgateway Step 3: Update Gateway ConfigMap \u00b6 Additionally, you will need to update the Istio ConfigMap: apiVersion: operator.knative.dev/v1alpha1 kind: KnativeServing metadata: name: knative-serving namespace: knative-serving spec: ingress: istio: enabled: true knative-ingress-gateway: selector: custom: ingressgateway config: istio: gateway.knative-serving.knative-ingress-gateway: \"custom-ingressgateway.istio-system.svc.cluster.local\" The key in spec.config.istio is in the format of gateway.. . Configuration of cluster local gateway \u00b6 Update spec.ingress.istio.knative-local-gateway to select the labels of the new cluster-local ingress gateway: Default local gateway name: \u00b6 Go through the guide here to use local cluster gateway, if you use the default gateway called knative-local-gateway . Non-default local gateway name: \u00b6 If you create custom local gateway with a name other than knative-local-gateway , update config.istio and the knative-local-gateway selector: This example shows a service and deployment knative-local-gateway in the namespace istio-system , with the label custom: custom-local-gw : apiVersion: operator.knative.dev/v1alpha1 kind: KnativeServing metadata: name: knative-serving namespace: knative-serving spec: ingress: istio: enabled: true knative-local-gateway: selector: custom: custom-local-gateway config: istio: local-gateway.knative-serving.knative-local-gateway: \"custom-local-gateway.istio-system.svc.cluster.local\" High availability \u00b6 By default, Knative Serving runs a single instance of each controller. The spec.high-availability field allows you to configure the number of replicas for the following leader-elected controllers: controller , autoscaler-hpa , networking-istio . This field also configures the HorizontalPodAutoscaler resources for the data plane ( activator ): The following configuration specifies a replica count of 3 for the controllers and a minimum of 3 activators (which may scale higher if needed): apiVersion: operator.knative.dev/v1alpha1 kind: KnativeServing metadata: name: knative-serving namespace: knative-serving spec: high-availability: replicas: 3 System Resource Settings \u00b6 The operator custom resource allows you to configure system resources for the Knative system containers. Requests and limits can be configured for the following containers: activator , autoscaler , controller , webhook , autoscaler-hpa , networking-istio and queue-proxy . To override resource settings for a specific container, create an entry in the spec.resources list with the container name and the Kubernetes resource settings . For example, the following KnativeServing resource configures the activator to request 0.3 CPU and 100MB of RAM, and sets hard limits of 1 CPU, 250MB RAM, and 4GB of local storage: apiVersion: operator.knative.dev/v1alpha1 kind: KnativeServing metadata: name: knative-serving namespace: knative-serving spec: resources: - container: activator requests: cpu: 300m memory: 100Mi limits: cpu: 1000m memory: 250Mi ephemeral-storage: 4Gi If you would like to add another container autoscaler with the same configuration, you need to change your CR as below: apiVersion: operator.knative.dev/v1alpha1 kind: KnativeServing metadata: name: knative-serving namespace: knative-serving spec: resources: - container: activator requests: cpu: 300m memory: 100Mi limits: cpu: 1000m memory: 250Mi ephemeral-storage: 4Gi - container: autoscaler requests: cpu: 300m memory: 100Mi limits: cpu: 1000m memory: 250Mi ephemeral-storage: 4Gi Override system deployments \u00b6 If you would like to override some configurations for a specific deployment, you can override the configuration by using spec.deployments in CR. Currently replicas , labels and annotations are supported. For example, the following KnativeServing resource overrides the webhook to have 3 replicass, mylabel: foo labels and myannotataions: bar annotations, while other system deployments have 2 replicas by spec.high-availability . apiVersion: operator.knative.dev/v1alpha1 kind: KnativeServing metadata: name: ks namespace: knative-serving spec: high-availability: replicas: 2 deployments: - name: webhook replicas: 3 labels: mylabel: foo annotations: myannotataions: bar NOTE: The labels and annotations settings override webhook's labels and annotations in deployment and pod both.","title":"Configuring the Serving Operator Custom Resource"},{"location":"install/operator/configuring-serving-cr/#configuring-the-serving-operator-custom-resource","text":"The Knative Serving operator can be configured with these options: Version Configuration Serving Configuration by ConfigMap Private repository and private secret SSL certificate for controller Knative ingress gateway Cluster local gateway High availability System Resource Settings Override system deployments","title":"Configuring the Serving Operator Custom Resource"},{"location":"install/operator/configuring-serving-cr/#version-configuration","text":"Cluster administrators can install a specific version of Knative Serving by using the spec.version field. For example, if you want to install Knative Serving 0.16.0, you can apply the following KnativeServing custom resource: apiVersion: operator.knative.dev/v1alpha1 kind: KnativeServing metadata: name: knative-serving namespace: knative-serving spec: version: 0.16.0 If spec.version is not specified, the Knative Operator will install the latest available version of Knative Serving. If users specify an invalid or unavailable version, the Knative Operator will do nothing. The Knative Operator always includes the latest 3 minor release versions. For example, if the current version of the Knative Operator is 0.16.x, the earliest version of Knative Serving available through the Operator is 0.14.0. If Knative Serving is already managed by the Operator, updating the spec.version field in the KnativeServing resource enables upgrading or downgrading the Knative Serving version, without needing to change the Operator. Note that the Knative Operator only permits upgrades or downgrades by one minor release version at a time. For example, if the current Knative Serving deployment is version 0.14.x, you must upgrade to 0.15.x before upgrading to 0.16.x.","title":"Version Configuration"},{"location":"install/operator/configuring-serving-cr/#serving-configuration-by-configmap","text":"The Operator manages the Knative Serving installation. It overwrites any updates to ConfigMaps which are used to configure Knative Serving. The KnativeServing custom resource (CR) allows you to set values for these ConfigMaps by using the Operator. Knative Serving has multiple ConfigMaps that are named with the prefix config- . The spec.config in the KnativeServing CR has one <name> entry for each ConfigMap, named config-<name> , with a value which will be used for the ConfigMap data . In the setup a custom domain example , you can see the content of the ConfigMap config-domain is: apiVersion: v1 kind: ConfigMap metadata: name: config-domain namespace: knative-serving data: example.org: | selector: app: prod example.com: \"\" Using the operator, specify the ConfigMap config-domain using the operator CR: apiVersion: operator.knative.dev/v1alpha1 kind: KnativeServing metadata: name: knative-serving namespace: knative-serving spec: config: domain: example.org: | selector: app: prod example.com: \"\" You can apply values to multiple ConfigMaps. This example sets stable-window to 60s in config-autoscaler as well as specifying config-domain : apiVersion: operator.knative.dev/v1alpha1 kind: KnativeServing metadata: name: knative-serving namespace: knative-serving spec: config: domain: example.org: | selector: app: prod example.com: \"\" autoscaler: stable-window: \"60s\" All the ConfigMaps are created in the same namespace as the operator CR. You can use the operator CR as the unique entry point to edit all of them.","title":"Serving Configuration by ConfigMap"},{"location":"install/operator/configuring-serving-cr/#private-repository-and-private-secrets","text":"You can use the spec.registry section of the operator CR to change the image references to point to a private registry or specify imagePullSecrets : default : this field defines a image reference template for all Knative images. The format is example-registry.io/custom/path/${NAME}:{CUSTOM-TAG} . If you use the same tag for all your images, the only difference is the image name. ${NAME} is a pre-defined variable in the operator corresponding to the container name. If you name the images in your private repo to align with the container names ( activator , autoscaler , controller , webhook , autoscaler-hpa , networking-istio , and queue-proxy ), the default argument should be sufficient. override : a map from container name to the full registry location. This section is only needed when the registry images do not match the common naming format. For containers whose name matches a key, the value is used in preference to the image name calculated by default . If a container's name does not match a key in override , the template in default is used. imagePullSecrets : a list of Secret names used when pulling Knative container images. The Secrets must be created in the same namespace as the Knative Serving Deployments. See deploying images from a private container registry for configuration details.","title":"Private repository and private secrets"},{"location":"install/operator/configuring-serving-cr/#download-images-in-a-predefined-format-without-secrets","text":"This example shows how you can define custom image links that can be defined in the CR using the simplified format docker.io/knative-images/${NAME}:{CUSTOM-TAG} . In the example below: the custom tag v0.13.0 is used for all images all image links are accessible without using secrets images are pushed as docker.io/knative-images/${NAME}:{CUSTOM-TAG} First, you need to make sure your images pushed to the following image tags: Container Docker Image activator docker.io/knative-images/activator:v0.13.0 autoscaler docker.io/knative-images/autoscaler:v0.13.0 controller docker.io/knative-images/controller:v0.13.0 webhook docker.io/knative-images/webhook:v0.13.0 autoscaler-hpa docker.io/knative-images/autoscaler-hpa:v0.13.0 networking-istio docker.io/knative-images/networking-istio:v0.13.0 queue-proxy docker.io/knative-images/queue-proxy:v0.13.0 Then, you need to define your operator CR with following content: apiVersion: operator.knative.dev/v1alpha1 kind: KnativeServing metadata: name: knative-serving namespace: knative-serving spec: registry: default: docker.io/knative-images/${NAME}:v0.13.0","title":"Download images in a predefined format without secrets:"},{"location":"install/operator/configuring-serving-cr/#download-images-individually-without-secrets","text":"If your custom image links are not defined in a uniform format by default, you will need to individually include each link in the CR. For example, to given the following images: Container Docker Image activator docker.io/knative-images-repo1/activator:v0.13.0 autoscaler docker.io/knative-images-repo2/autoscaler:v0.13.0 controller docker.io/knative-images-repo3/controller:v0.13.0 webhook docker.io/knative-images-repo4/webhook:v0.13.0 autoscaler-hpa docker.io/knative-images-repo5/autoscaler-hpa:v0.13.0 networking-istio docker.io/knative-images-repo6/prefix-networking-istio:v0.13.0 queue-proxy docker.io/knative-images-repo7/queue-proxy-suffix:v0.13.0 The operator CR should be modified to include the full list: apiVersion: operator.knative.dev/v1alpha1 kind: KnativeServing metadata: name: knative-serving namespace: knative-serving spec: registry: override: activator: docker.io/knative-images-repo1/activator:v0.13.0 autoscaler: docker.io/knative-images-repo2/autoscaler:v0.13.0 controller: docker.io/knative-images-repo3/controller:v0.13.0 webhook: docker.io/knative-images-repo4/webhook:v0.13.0 autoscaler-hpa: docker.io/knative-images-repo5/autoscaler-hpa:v0.13.0 networking-istio: docker.io/knative-images-repo6/prefix-networking-istio:v0.13.0 queue-proxy: docker.io/knative-images-repo7/queue-proxy-suffix:v0.13.0","title":"Download images individually without secrets:"},{"location":"install/operator/configuring-serving-cr/#download-images-with-secrets","text":"If your image repository requires private secrets for access, include the imagePullSecrets attribute. This example uses a secret named regcred . You must create your own private secrets if these are required: From existing docker credentials From command line for docker credentials Create your own secret After you create this secret, edit your operator CR by appending the content below: apiVersion: operator.knative.dev/v1alpha1 kind: KnativeServing metadata: name: knative-serving namespace: knative-serving spec: registry: ... imagePullSecrets: - name: regcred The field imagePullSecrets expects a list of secrets. You can add multiple secrets to access the images as below: apiVersion: operator.knative.dev/v1alpha1 kind: KnativeServing metadata: name: knative-serving namespace: knative-serving spec: registry: ... imagePullSecrets: - name: regcred - name: regcred-2 ...","title":"Download images with secrets:"},{"location":"install/operator/configuring-serving-cr/#ssl-certificate-for-controller","text":"To enable tag to digest resolution , the Knative Serving controller needs to access the container registry. To allow the controller to trust a self-signed registry cert, you can use the Operator to specify the certificate using a ConfigMap or Secret. Specify the following fields in spec.controller-custom-certs to select a custom registry certificate: name : the name of the ConfigMap or Secret. type : either the string \"ConfigMap\" or \"Secret\". If you create a ConfigMap named testCert containing the certificate, change your CR: apiVersion: operator.knative.dev/v1alpha1 kind: KnativeServing metadata: name: knative-serving namespace: knative-serving spec: controller-custom-certs: name: testCert type: ConfigMap","title":"SSL certificate for controller"},{"location":"install/operator/configuring-serving-cr/#configuration-of-knative-ingress-gateway","text":"To set up custom ingress gateway, follow Step 1: Create Gateway Service and Deployment Instance .","title":"Configuration of Knative ingress gateway"},{"location":"install/operator/configuring-serving-cr/#step-2-update-the-knative-gateway","text":"Update spec.ingress.istio.knative-ingress-gateway to select the labels of the new ingress gateway: apiVersion: operator.knative.dev/v1alpha1 kind: KnativeServing metadata: name: knative-serving namespace: knative-serving spec: ingress: istio: enabled: true knative-ingress-gateway: selector: custom: ingressgateway","title":"Step 2: Update the Knative gateway"},{"location":"install/operator/configuring-serving-cr/#step-3-update-gateway-configmap","text":"Additionally, you will need to update the Istio ConfigMap: apiVersion: operator.knative.dev/v1alpha1 kind: KnativeServing metadata: name: knative-serving namespace: knative-serving spec: ingress: istio: enabled: true knative-ingress-gateway: selector: custom: ingressgateway config: istio: gateway.knative-serving.knative-ingress-gateway: \"custom-ingressgateway.istio-system.svc.cluster.local\" The key in spec.config.istio is in the format of gateway.. .","title":"Step 3: Update Gateway ConfigMap"},{"location":"install/operator/configuring-serving-cr/#configuration-of-cluster-local-gateway","text":"Update spec.ingress.istio.knative-local-gateway to select the labels of the new cluster-local ingress gateway:","title":"Configuration of cluster local gateway"},{"location":"install/operator/configuring-serving-cr/#default-local-gateway-name","text":"Go through the guide here to use local cluster gateway, if you use the default gateway called knative-local-gateway .","title":"Default local gateway name:"},{"location":"install/operator/configuring-serving-cr/#non-default-local-gateway-name","text":"If you create custom local gateway with a name other than knative-local-gateway , update config.istio and the knative-local-gateway selector: This example shows a service and deployment knative-local-gateway in the namespace istio-system , with the label custom: custom-local-gw : apiVersion: operator.knative.dev/v1alpha1 kind: KnativeServing metadata: name: knative-serving namespace: knative-serving spec: ingress: istio: enabled: true knative-local-gateway: selector: custom: custom-local-gateway config: istio: local-gateway.knative-serving.knative-local-gateway: \"custom-local-gateway.istio-system.svc.cluster.local\"","title":"Non-default local gateway name:"},{"location":"install/operator/configuring-serving-cr/#high-availability","text":"By default, Knative Serving runs a single instance of each controller. The spec.high-availability field allows you to configure the number of replicas for the following leader-elected controllers: controller , autoscaler-hpa , networking-istio . This field also configures the HorizontalPodAutoscaler resources for the data plane ( activator ): The following configuration specifies a replica count of 3 for the controllers and a minimum of 3 activators (which may scale higher if needed): apiVersion: operator.knative.dev/v1alpha1 kind: KnativeServing metadata: name: knative-serving namespace: knative-serving spec: high-availability: replicas: 3","title":"High availability"},{"location":"install/operator/configuring-serving-cr/#system-resource-settings","text":"The operator custom resource allows you to configure system resources for the Knative system containers. Requests and limits can be configured for the following containers: activator , autoscaler , controller , webhook , autoscaler-hpa , networking-istio and queue-proxy . To override resource settings for a specific container, create an entry in the spec.resources list with the container name and the Kubernetes resource settings . For example, the following KnativeServing resource configures the activator to request 0.3 CPU and 100MB of RAM, and sets hard limits of 1 CPU, 250MB RAM, and 4GB of local storage: apiVersion: operator.knative.dev/v1alpha1 kind: KnativeServing metadata: name: knative-serving namespace: knative-serving spec: resources: - container: activator requests: cpu: 300m memory: 100Mi limits: cpu: 1000m memory: 250Mi ephemeral-storage: 4Gi If you would like to add another container autoscaler with the same configuration, you need to change your CR as below: apiVersion: operator.knative.dev/v1alpha1 kind: KnativeServing metadata: name: knative-serving namespace: knative-serving spec: resources: - container: activator requests: cpu: 300m memory: 100Mi limits: cpu: 1000m memory: 250Mi ephemeral-storage: 4Gi - container: autoscaler requests: cpu: 300m memory: 100Mi limits: cpu: 1000m memory: 250Mi ephemeral-storage: 4Gi","title":"System Resource Settings"},{"location":"install/operator/configuring-serving-cr/#override-system-deployments","text":"If you would like to override some configurations for a specific deployment, you can override the configuration by using spec.deployments in CR. Currently replicas , labels and annotations are supported. For example, the following KnativeServing resource overrides the webhook to have 3 replicass, mylabel: foo labels and myannotataions: bar annotations, while other system deployments have 2 replicas by spec.high-availability . apiVersion: operator.knative.dev/v1alpha1 kind: KnativeServing metadata: name: ks namespace: knative-serving spec: high-availability: replicas: 2 deployments: - name: webhook replicas: 3 labels: mylabel: foo annotations: myannotataions: bar NOTE: The labels and annotations settings override webhook's labels and annotations in deployment and pod both.","title":"Override system deployments"},{"location":"reference/_index/","text":"Knative Reference Documentation \u00b6 Knative reference documentation.","title":"Knative Reference Documentation"},{"location":"reference/_index/#knative-reference-documentation","text":"Knative reference documentation.","title":"Knative Reference Documentation"},{"location":"reference/api/","text":"View the latest release \u00b6 The reference documentation for the latest release of the Knative is available at www.knative.dev . Source files \u00b6 The API source files are located at: Serving API Eventing API Updating API Reference docs (for Knative maintainers) \u00b6 The Knative API reference documentation is manually generated using the gen-api-reference-docs.sh tool. If you need to generate a new version of the API docs for a recent update or for a new release, you can use the following steps. To learn more about the tool, see the gen-crd-api-reference-docs reference page. Before you begin \u00b6 You must meet the following requirements to run the gen-api-reference-docs.sh tool: You need the following software installed: git go version 1.11+ Clone knative/docs locally. For example: git clone git@github.com:knative/docs.git Generating the API \u00b6 To generate a version of the API: Ensure that your GOPATH is empty. The gen-api-reference-docs.sh script will result in the GOPATH should not be set error if your GOPATH is configured. You view the value by running the following command: echo $GOPATH If your GOPATH is already configured, temporarily clear the GOPATH value by running the following command: export GOPATH=\"\" Locate the commits or tags that correspond to the version of the API that you want to generate: Serving Eventing To run the gen-api-reference-docs.sh command from the hack directory, you specify the commits or tags for each of the corresponding Knative component variables ( KNATIVE_[component_name]_COMMIT ): cd hack KNATIVE_SERVING_COMMIT=[commit_or_tag] \\ KNATIVE_EVENTING_COMMIT=[commit_or_tag] \\ ./gen-api-reference-docs.sh where [commit_or_tag] is the commit or tag in the specific repo that represents the version of the API that you want to generate. Also see the example below. Result The gen-api-reference-docs.sh tool generates the API in a tmp folder. After a successful build, the tool automatically opens that folder in the tmp directory. If the script fails, there are a couple possible causes. If you get the F1116 15:21:23.549503 63473 main.go:129] no API packages found in ./pkg/apis error, check if a new version of the script is available: https://github.com/ahmetb/gen-crd-api-reference-docs/tags The script is kept up-to-date with changes that go into the Kubernetes API. As Knative adds support for those APIs, you might need to make sure the corresponding script gen-crd-api-reference-docs version is used. If you get the F0807 13:58:20.621526 168834 main.go:444] type invalid type has kind=Unsupported which is unhandled error, the import target might have moved. There might be other causes for that error but view #2054 (and the linked Issues) for details about how we handled that error in the past. Copy the generated API files into the docs/reference directory of your knative/docs clone. The linter now fails for content with trailing whitespaces. Use a tool of your choice to remove all trailing whitespace. For example, search for and remove: \\s+$ You can now perform the necessary steps to open a PR, complete a review, and merge the new API files into the appropriate branch of the knative/docs repo. See the contributor flow for details about requesting changes in the knative/docs repo. Example \u00b6 To build a set of Knative API docs for v0.18, you can use the v0.18.0 the tags from each of the Knative component repositories, like Serving v0.18.0 . If you want to use a commit for Serving v0.18.0, you would use 850b7c . Using tags from each repo, you would run the following command to generate the v0.18.0 API source files: KNATIVE_SERVING_COMMIT=v0.18.0 \\ KNATIVE_EVENTING_COMMIT=v0.18.0 \\ ./gen-api-reference-docs.sh","title":"Index"},{"location":"reference/api/#view-the-latest-release","text":"The reference documentation for the latest release of the Knative is available at www.knative.dev .","title":"View the latest release"},{"location":"reference/api/#source-files","text":"The API source files are located at: Serving API Eventing API","title":"Source files"},{"location":"reference/api/#updating-api-reference-docs-for-knative-maintainers","text":"The Knative API reference documentation is manually generated using the gen-api-reference-docs.sh tool. If you need to generate a new version of the API docs for a recent update or for a new release, you can use the following steps. To learn more about the tool, see the gen-crd-api-reference-docs reference page.","title":"Updating API Reference docs (for Knative maintainers)"},{"location":"reference/api/#before-you-begin","text":"You must meet the following requirements to run the gen-api-reference-docs.sh tool: You need the following software installed: git go version 1.11+ Clone knative/docs locally. For example: git clone git@github.com:knative/docs.git","title":"Before you begin"},{"location":"reference/api/#generating-the-api","text":"To generate a version of the API: Ensure that your GOPATH is empty. The gen-api-reference-docs.sh script will result in the GOPATH should not be set error if your GOPATH is configured. You view the value by running the following command: echo $GOPATH If your GOPATH is already configured, temporarily clear the GOPATH value by running the following command: export GOPATH=\"\" Locate the commits or tags that correspond to the version of the API that you want to generate: Serving Eventing To run the gen-api-reference-docs.sh command from the hack directory, you specify the commits or tags for each of the corresponding Knative component variables ( KNATIVE_[component_name]_COMMIT ): cd hack KNATIVE_SERVING_COMMIT=[commit_or_tag] \\ KNATIVE_EVENTING_COMMIT=[commit_or_tag] \\ ./gen-api-reference-docs.sh where [commit_or_tag] is the commit or tag in the specific repo that represents the version of the API that you want to generate. Also see the example below. Result The gen-api-reference-docs.sh tool generates the API in a tmp folder. After a successful build, the tool automatically opens that folder in the tmp directory. If the script fails, there are a couple possible causes. If you get the F1116 15:21:23.549503 63473 main.go:129] no API packages found in ./pkg/apis error, check if a new version of the script is available: https://github.com/ahmetb/gen-crd-api-reference-docs/tags The script is kept up-to-date with changes that go into the Kubernetes API. As Knative adds support for those APIs, you might need to make sure the corresponding script gen-crd-api-reference-docs version is used. If you get the F0807 13:58:20.621526 168834 main.go:444] type invalid type has kind=Unsupported which is unhandled error, the import target might have moved. There might be other causes for that error but view #2054 (and the linked Issues) for details about how we handled that error in the past. Copy the generated API files into the docs/reference directory of your knative/docs clone. The linter now fails for content with trailing whitespaces. Use a tool of your choice to remove all trailing whitespace. For example, search for and remove: \\s+$ You can now perform the necessary steps to open a PR, complete a review, and merge the new API files into the appropriate branch of the knative/docs repo. See the contributor flow for details about requesting changes in the knative/docs repo.","title":"Generating the API"},{"location":"reference/api/#example","text":"To build a set of Knative API docs for v0.18, you can use the v0.18.0 the tags from each of the Knative component repositories, like Serving v0.18.0 . If you want to use a commit for Serving v0.18.0, you would use 850b7c . Using tags from each repo, you would run the following command to generate the v0.18.0 API source files: KNATIVE_SERVING_COMMIT=v0.18.0 \\ KNATIVE_EVENTING_COMMIT=v0.18.0 \\ ./gen-api-reference-docs.sh","title":"Example"},{"location":"reference/api/_index/","text":"","title":"Knative API Reference Documentation"},{"location":"reference/api/serving-api/","text":"Macro Rendering Error \u00b6 TemplateSyntaxError : unexpected '<' Traceback (most recent call last): File \"/usr/local/lib/python3.9/site-packages/mkdocs_macros/plugin.py\", line 441, in render md_template = self.env.from_string(markdown) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 941, in from_string return cls.from_code(self, self.compile(source), globals, None) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 638, in compile self.handle_exception(source=source_hint) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 832, in handle_exception reraise(*rewrite_traceback_stack(source=source)) File \"/usr/local/lib/python3.9/site-packages/jinja2/_compat.py\", line 28, in reraise raise value.with_traceback(tb) File \"<unknown>\", line 3, in template jinja2.exceptions.TemplateSyntaxError: unexpected '<'","title":"Knative Serving Component"},{"location":"reference/api/serving-api/#macro-rendering-error","text":"TemplateSyntaxError : unexpected '<' Traceback (most recent call last): File \"/usr/local/lib/python3.9/site-packages/mkdocs_macros/plugin.py\", line 441, in render md_template = self.env.from_string(markdown) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 941, in from_string return cls.from_code(self, self.compile(source), globals, None) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 638, in compile self.handle_exception(source=source_hint) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 832, in handle_exception reraise(*rewrite_traceback_stack(source=source)) File \"/usr/local/lib/python3.9/site-packages/jinja2/_compat.py\", line 28, in reraise raise value.with_traceback(tb) File \"<unknown>\", line 3, in template jinja2.exceptions.TemplateSyntaxError: unexpected '<'","title":"Macro Rendering Error"},{"location":"reference/api/serving/","text":"Packages: autoscaling.internal.knative.dev/v1alpha1 serving.knative.dev/v1 serving.knative.dev/v1alpha1 autoscaling.internal.knative.dev/v1alpha1 Package v1alpha1 contains the Autoscaling v1alpha1 API types. Resource Types: PodAutoscaler PodAutoscaler PodAutoscaler is a Knative abstraction that encapsulates the interface by which Knative components instantiate autoscalers. This definition is an abstraction that may be backed by multiple definitions. For more information, see the Knative Pluggability presentation: https://docs.google.com/presentation/d/10KWynvAJYuOEWy69VBa6bHJVCqIsz1TNdEKosNvcpPY/edit Field Description apiVersion string autoscaling.internal.knative.dev/v1alpha1 kind string PodAutoscaler metadata Kubernetes meta/v1.ObjectMeta (Optional) Refer to the Kubernetes API documentation for the fields of the metadata field. spec PodAutoscalerSpec (Optional) Spec holds the desired state of the PodAutoscaler (from the client). containerConcurrency int64 (Optional) ContainerConcurrency specifies the maximum allowed in-flight (concurrent) requests per container of the Revision. Defaults to 0 which means unlimited concurrency. scaleTargetRef Kubernetes core/v1.ObjectReference ScaleTargetRef defines the /scale-able resource that this PodAutoscaler is responsible for quickly right-sizing. reachability ReachabilityType (Optional) Reachability specifies whether or not the ScaleTargetRef can be reached (ie. has a route). Defaults to ReachabilityUnknown protocolType knative.dev/networking/pkg/apis/networking.ProtocolType The application-layer protocol. Matches ProtocolType inferred from the revision spec. status PodAutoscalerStatus (Optional) Status communicates the observed state of the PodAutoscaler (from the controller). Metric Metric represents a resource to configure the metric collector with. Field Description metadata Kubernetes meta/v1.ObjectMeta (Optional) Refer to the Kubernetes API documentation for the fields of the metadata field. spec MetricSpec (Optional) Spec holds the desired state of the Metric (from the client). stableWindow time.Duration StableWindow is the aggregation window for metrics in a stable state. panicWindow time.Duration PanicWindow is the aggregation window for metrics where quick reactions are needed. scrapeTarget string ScrapeTarget is the K8s service that publishes the metric endpoint. status MetricStatus (Optional) Status communicates the observed state of the Metric (from the controller). MetricSpec ( Appears on: Metric ) MetricSpec contains all values a metric collector needs to operate. Field Description stableWindow time.Duration StableWindow is the aggregation window for metrics in a stable state. panicWindow time.Duration PanicWindow is the aggregation window for metrics where quick reactions are needed. scrapeTarget string ScrapeTarget is the K8s service that publishes the metric endpoint. MetricStatus ( Appears on: Metric ) MetricStatus reflects the status of metric collection for this specific entity. Field Description Status knative.dev/pkg/apis/duck/v1.Status (Members of Status are embedded into this type.) PodAutoscalerSpec ( Appears on: PodAutoscaler ) PodAutoscalerSpec holds the desired state of the PodAutoscaler (from the client). Field Description containerConcurrency int64 (Optional) ContainerConcurrency specifies the maximum allowed in-flight (concurrent) requests per container of the Revision. Defaults to 0 which means unlimited concurrency. scaleTargetRef Kubernetes core/v1.ObjectReference ScaleTargetRef defines the /scale-able resource that this PodAutoscaler is responsible for quickly right-sizing. reachability ReachabilityType (Optional) Reachability specifies whether or not the ScaleTargetRef can be reached (ie. has a route). Defaults to ReachabilityUnknown protocolType knative.dev/networking/pkg/apis/networking.ProtocolType The application-layer protocol. Matches ProtocolType inferred from the revision spec. PodAutoscalerStatus ( Appears on: PodAutoscaler ) PodAutoscalerStatus communicates the observed state of the PodAutoscaler (from the controller). Field Description Status knative.dev/pkg/apis/duck/v1.Status (Members of Status are embedded into this type.) serviceName string ServiceName is the K8s Service name that serves the revision, scaled by this PA. The service is created and owned by the ServerlessService object owned by this PA. metricsServiceName string MetricsServiceName is the K8s Service name that provides revision metrics. The service is managed by the PA object. desiredScale int32 DesiredScale shows the current desired number of replicas for the revision. actualScale int32 ActualScale shows the actual number of replicas for the revision. PodScalable PodScalable is a duck type that the resources referenced by the PodAutoscaler\u2019s ScaleTargetRef must implement. They must also implement the /scale sub-resource for use with /scale based implementations (e.g. HPA), but this further constrains the shape the referenced resources may take. Field Description metadata Kubernetes meta/v1.ObjectMeta Refer to the Kubernetes API documentation for the fields of the metadata field. spec PodScalableSpec replicas int32 selector Kubernetes meta/v1.LabelSelector template Kubernetes core/v1.PodTemplateSpec status PodScalableStatus PodScalableSpec ( Appears on: PodScalable ) PodScalableSpec is the specification for the desired state of a PodScalable (or at least our shared portion). Field Description replicas int32 selector Kubernetes meta/v1.LabelSelector template Kubernetes core/v1.PodTemplateSpec PodScalableStatus ( Appears on: PodScalable ) PodScalableStatus is the observed state of a PodScalable (or at least our shared portion). Field Description replicas int32 ReachabilityType ( string alias) ( Appears on: PodAutoscalerSpec ) ReachabilityType is the enumeration type for the different states of reachability to the ScaleTarget of a PodAutoscaler Value Description \"Reachable\" ReachabilityReachable means the ScaleTarget is reachable, ie. it has an active route. \"\" ReachabilityUnknown means the reachability of the ScaleTarget is unknown. Used when the reachability cannot be determined, eg. during activation. \"Unreachable\" ReachabilityUnreachable means the ScaleTarget is not reachable, ie. it does not have an active route. serving.knative.dev/v1 Package v1 contains the Serving v1 API types. Resource Types: Configuration Revision Route Service Configuration Configuration represents the \u201cfloating HEAD\u201d of a linear history of Revisions. Users create new Revisions by updating the Configuration\u2019s spec. The \u201clatest created\u201d revision\u2019s name is available under status, as is the \u201clatest ready\u201d revision\u2019s name. See also: https://github.com/knative/serving/blob/main/docs/spec/overview.md#configuration Field Description apiVersion string serving.knative.dev/v1 kind string Configuration metadata Kubernetes meta/v1.ObjectMeta (Optional) Refer to the Kubernetes API documentation for the fields of the metadata field. spec ConfigurationSpec (Optional) template RevisionTemplateSpec (Optional) Template holds the latest specification for the Revision to be stamped out. status ConfigurationStatus (Optional) Revision Revision is an immutable snapshot of code and configuration. A revision references a container image. Revisions are created by updates to a Configuration. See also: https://github.com/knative/serving/blob/main/docs/spec/overview.md#revision Field Description apiVersion string serving.knative.dev/v1 kind string Revision metadata Kubernetes meta/v1.ObjectMeta (Optional) Refer to the Kubernetes API documentation for the fields of the metadata field. spec RevisionSpec (Optional) PodSpec Kubernetes core/v1.PodSpec (Members of PodSpec are embedded into this type.) containerConcurrency int64 (Optional) ContainerConcurrency specifies the maximum allowed in-flight (concurrent) requests per container of the Revision. Defaults to 0 which means concurrency to the application is not limited, and the system decides the target concurrency for the autoscaler. timeoutSeconds int64 (Optional) TimeoutSeconds is the maximum duration in seconds that the request routing layer will wait for a request delivered to a container to begin replying (send network traffic). If unspecified, a system default will be provided. status RevisionStatus (Optional) Route Route is responsible for configuring ingress over a collection of Revisions. Some of the Revisions a Route distributes traffic over may be specified by referencing the Configuration responsible for creating them; in these cases the Route is additionally responsible for monitoring the Configuration for \u201clatest ready revision\u201d changes, and smoothly rolling out latest revisions. See also: https://github.com/knative/serving/blob/main/docs/spec/overview.md#route Field Description apiVersion string serving.knative.dev/v1 kind string Route metadata Kubernetes meta/v1.ObjectMeta (Optional) Refer to the Kubernetes API documentation for the fields of the metadata field. spec RouteSpec (Optional) Spec holds the desired state of the Route (from the client). traffic []TrafficTarget (Optional) Traffic specifies how to distribute traffic over a collection of revisions and configurations. status RouteStatus (Optional) Status communicates the observed state of the Route (from the controller). Service Service acts as a top-level container that manages a Route and Configuration which implement a network service. Service exists to provide a singular abstraction which can be access controlled, reasoned about, and which encapsulates software lifecycle decisions such as rollout policy and team resource ownership. Service acts only as an orchestrator of the underlying Routes and Configurations (much as a kubernetes Deployment orchestrates ReplicaSets), and its usage is optional but recommended. The Service\u2019s controller will track the statuses of its owned Configuration and Route, reflecting their statuses and conditions as its own. See also: https://github.com/knative/serving/blob/main/docs/spec/overview.md#service Field Description apiVersion string serving.knative.dev/v1 kind string Service metadata Kubernetes meta/v1.ObjectMeta (Optional) Refer to the Kubernetes API documentation for the fields of the metadata field. spec ServiceSpec (Optional) ConfigurationSpec ConfigurationSpec (Members of ConfigurationSpec are embedded into this type.) ServiceSpec inlines an unrestricted ConfigurationSpec. RouteSpec RouteSpec (Members of RouteSpec are embedded into this type.) ServiceSpec inlines RouteSpec and restricts/defaults its fields via webhook. In particular, this spec can only reference this Service\u2019s configuration and revisions (which also influences defaults). status ServiceStatus (Optional) ConfigurationSpec ( Appears on: Configuration , ServiceSpec ) ConfigurationSpec holds the desired state of the Configuration (from the client). Field Description template RevisionTemplateSpec (Optional) Template holds the latest specification for the Revision to be stamped out. ConfigurationStatus ( Appears on: Configuration ) ConfigurationStatus communicates the observed state of the Configuration (from the controller). Field Description Status knative.dev/pkg/apis/duck/v1.Status (Members of Status are embedded into this type.) ConfigurationStatusFields ConfigurationStatusFields (Members of ConfigurationStatusFields are embedded into this type.) ConfigurationStatusFields ( Appears on: ConfigurationStatus , ServiceStatus ) ConfigurationStatusFields holds the fields of Configuration\u2019s status that are not generally shared. This is defined separately and inlined so that other types can readily consume these fields via duck typing. Field Description latestReadyRevisionName string (Optional) LatestReadyRevisionName holds the name of the latest Revision stamped out from this Configuration that has had its \u201cReady\u201d condition become \u201cTrue\u201d. latestCreatedRevisionName string (Optional) LatestCreatedRevisionName is the last revision that was created from this Configuration. It might not be ready yet, for that use LatestReadyRevisionName. ContainerStatus ( Appears on: RevisionStatus ) ContainerStatus holds the information of container name and image digest value Field Description name string imageDigest string RevisionSpec ( Appears on: Revision , RevisionTemplateSpec ) RevisionSpec holds the desired state of the Revision (from the client). Field Description PodSpec Kubernetes core/v1.PodSpec (Members of PodSpec are embedded into this type.) containerConcurrency int64 (Optional) ContainerConcurrency specifies the maximum allowed in-flight (concurrent) requests per container of the Revision. Defaults to 0 which means concurrency to the application is not limited, and the system decides the target concurrency for the autoscaler. timeoutSeconds int64 (Optional) TimeoutSeconds is the maximum duration in seconds that the request routing layer will wait for a request delivered to a container to begin replying (send network traffic). If unspecified, a system default will be provided. RevisionStatus ( Appears on: Revision ) RevisionStatus communicates the observed state of the Revision (from the controller). Field Description Status knative.dev/pkg/apis/duck/v1.Status (Members of Status are embedded into this type.) serviceName string (Optional) ServiceName holds the name of a core Kubernetes Service resource that load balances over the pods backing this Revision. Deprecated: revision service name is effectively equal to the revision name, as per #10540. 0.23 \u2014 stop populating 0.25 \u2014 remove. logUrl string (Optional) LogURL specifies the generated logging url for this particular revision based on the revision url template specified in the controller\u2019s config. imageDigest string (Optional) DeprecatedImageDigest holds the resolved digest for the image specified within .Spec.Container.Image. The digest is resolved during the creation of Revision. This field holds the digest value regardless of whether a tag or digest was originally specified in the Container object. It may be empty if the image comes from a registry listed to skip resolution. If multiple containers specified then DeprecatedImageDigest holds the digest for serving container. DEPRECATED: Use ContainerStatuses instead. TODO(savitaashture) Remove deprecatedImageDigest. ref https://kubernetes.io/docs/reference/using-api/deprecation-policy for deprecation. containerStatuses []ContainerStatus (Optional) ContainerStatuses is a slice of images present in .Spec.Container[*].Image to their respective digests and their container name. The digests are resolved during the creation of Revision. ContainerStatuses holds the container name and image digests for both serving and non serving containers. ref: http://bit.ly/image-digests actualReplicas int32 (Optional) ActualReplicas reflects the amount of ready pods running this revision. desiredReplicas int32 (Optional) DesiredReplicas reflects the desired amount of pods running this revision. RevisionTemplateSpec ( Appears on: ConfigurationSpec ) RevisionTemplateSpec describes the data a revision should have when created from a template. Based on: https://github.com/kubernetes/api/blob/e771f807/core/v1/types.go#L3179-L3190 Field Description metadata Kubernetes meta/v1.ObjectMeta (Optional) Refer to the Kubernetes API documentation for the fields of the metadata field. spec RevisionSpec (Optional) PodSpec Kubernetes core/v1.PodSpec (Members of PodSpec are embedded into this type.) containerConcurrency int64 (Optional) ContainerConcurrency specifies the maximum allowed in-flight (concurrent) requests per container of the Revision. Defaults to 0 which means concurrency to the application is not limited, and the system decides the target concurrency for the autoscaler. timeoutSeconds int64 (Optional) TimeoutSeconds is the maximum duration in seconds that the request routing layer will wait for a request delivered to a container to begin replying (send network traffic). If unspecified, a system default will be provided. RouteSpec ( Appears on: Route , ServiceSpec ) RouteSpec holds the desired state of the Route (from the client). Field Description traffic []TrafficTarget (Optional) Traffic specifies how to distribute traffic over a collection of revisions and configurations. RouteStatus ( Appears on: Route ) RouteStatus communicates the observed state of the Route (from the controller). Field Description Status knative.dev/pkg/apis/duck/v1.Status (Members of Status are embedded into this type.) RouteStatusFields RouteStatusFields (Members of RouteStatusFields are embedded into this type.) RouteStatusFields ( Appears on: RouteStatus , ServiceStatus ) RouteStatusFields holds the fields of Route\u2019s status that are not generally shared. This is defined separately and inlined so that other types can readily consume these fields via duck typing. Field Description url knative.dev/pkg/apis.URL (Optional) URL holds the url that will distribute traffic over the provided traffic targets. It generally has the form http[s]://{route-name}.{route-namespace}.{cluster-level-suffix} address knative.dev/pkg/apis/duck/v1.Addressable (Optional) Address holds the information needed for a Route to be the target of an event. traffic []TrafficTarget (Optional) Traffic holds the configured traffic distribution. These entries will always contain RevisionName references. When ConfigurationName appears in the spec, this will hold the LatestReadyRevisionName that we last observed. RoutingState ( string alias) RoutingState represents states of a revision with regards to serving a route. Value Description \"active\" RoutingStateActive is a state for a revision which is actively referenced by a Route. \"pending\" RoutingStatePending is a state after a revision is created, but before its routing state has been determined. It is treated like active for the purposes of revision garbage collection. \"reserve\" RoutingStateReserve is a state for a revision which is no longer referenced by a Route, and is scaled down, but may be rapidly pinned to a route to be made active again. \"\" RoutingStateUnset is the empty value for routing state, this state is unexpected. ServiceSpec ( Appears on: Service ) ServiceSpec represents the configuration for the Service object. A Service\u2019s specification is the union of the specifications for a Route and Configuration. The Service restricts what can be expressed in these fields, e.g. the Route must reference the provided Configuration; however, these limitations also enable friendlier defaulting, e.g. Route never needs a Configuration name, and may be defaulted to the appropriate \u201crun latest\u201d spec. Field Description ConfigurationSpec ConfigurationSpec (Members of ConfigurationSpec are embedded into this type.) ServiceSpec inlines an unrestricted ConfigurationSpec. RouteSpec RouteSpec (Members of RouteSpec are embedded into this type.) ServiceSpec inlines RouteSpec and restricts/defaults its fields via webhook. In particular, this spec can only reference this Service\u2019s configuration and revisions (which also influences defaults). ServiceStatus ( Appears on: Service ) ServiceStatus represents the Status stanza of the Service resource. Field Description Status knative.dev/pkg/apis/duck/v1.Status (Members of Status are embedded into this type.) ConfigurationStatusFields ConfigurationStatusFields (Members of ConfigurationStatusFields are embedded into this type.) In addition to inlining ConfigurationSpec, we also inline the fields specific to ConfigurationStatus. RouteStatusFields RouteStatusFields (Members of RouteStatusFields are embedded into this type.) In addition to inlining RouteSpec, we also inline the fields specific to RouteStatus. TrafficTarget ( Appears on: RouteSpec , RouteStatusFields ) TrafficTarget holds a single entry of the routing table for a Route. Field Description tag string (Optional) Tag is optionally used to expose a dedicated url for referencing this target exclusively. revisionName string (Optional) RevisionName of a specific revision to which to send this portion of traffic. This is mutually exclusive with ConfigurationName. configurationName string (Optional) ConfigurationName of a configuration to whose latest revision we will send this portion of traffic. When the \u201cstatus.latestReadyRevisionName\u201d of the referenced configuration changes, we will automatically migrate traffic from the prior \u201clatest ready\u201d revision to the new one. This field is never set in Route\u2019s status, only its spec. This is mutually exclusive with RevisionName. latestRevision bool (Optional) LatestRevision may be optionally provided to indicate that the latest ready Revision of the Configuration should be used for this traffic target. When provided LatestRevision must be true if RevisionName is empty; it must be false when RevisionName is non-empty. percent int64 (Optional) Percent indicates that percentage based routing should be used and the value indicates the percent of traffic that is be routed to this Revision or Configuration. 0 (zero) mean no traffic, 100 means all traffic. When percentage based routing is being used the follow rules apply: - the sum of all percent values must equal 100 - when not specified, the implied value for percent is zero for that particular Revision or Configuration url knative.dev/pkg/apis.URL (Optional) URL displays the URL for accessing named traffic targets. URL is displayed in status, and is disallowed on spec. URL must contain a scheme (e.g. http://) and a hostname, but may not contain anything else (e.g. basic auth, url path, etc.) serving.knative.dev/v1alpha1 Package v1alpha1 contains the v1alpha1 versions of the serving apis. Api versions allow the api contract for a resource to be changed while keeping backward compatibility by support multiple concurrent versions of the same resource Resource Types: DomainMapping DomainMapping DomainMapping is a mapping from a custom hostname to an Addressable. Field Description apiVersion string serving.knative.dev/v1alpha1 kind string DomainMapping metadata Kubernetes meta/v1.ObjectMeta (Optional) Standard object\u2019s metadata. More info: https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api-conventions.md#metadata Refer to the Kubernetes API documentation for the fields of the metadata field. spec DomainMappingSpec (Optional) Spec is the desired state of the DomainMapping. More info: https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api-conventions.md#spec-and-status ref knative.dev/pkg/apis/duck/v1.KReference Ref specifies the target of the Domain Mapping. The object identified by the Ref must be an Addressable with a URL of the form {name}.{namespace}.{domain} where {domain} is the cluster domain, and {name} and {namespace} are the name and namespace of a Kubernetes Service. This contract is satisfied by Knative types such as Knative Services and Knative Routes, and by Kubernetes Services. status DomainMappingStatus (Optional) Status is the current state of the DomainMapping. More info: https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api-conventions.md#spec-and-status CannotConvertError CannotConvertError is returned when a field cannot be converted. Field Description Message string Field string DomainMappingSpec ( Appears on: DomainMapping ) DomainMappingSpec describes the DomainMapping the user wishes to exist. Field Description ref knative.dev/pkg/apis/duck/v1.KReference Ref specifies the target of the Domain Mapping. The object identified by the Ref must be an Addressable with a URL of the form {name}.{namespace}.{domain} where {domain} is the cluster domain, and {name} and {namespace} are the name and namespace of a Kubernetes Service. This contract is satisfied by Knative types such as Knative Services and Knative Routes, and by Kubernetes Services. DomainMappingStatus ( Appears on: DomainMapping ) DomainMappingStatus describes the current state of the DomainMapping. Field Description Status knative.dev/pkg/apis/duck/v1.Status (Members of Status are embedded into this type.) url knative.dev/pkg/apis.URL (Optional) URL is the URL of this DomainMapping. address knative.dev/pkg/apis/duck/v1.Addressable (Optional) Address holds the information needed for a DomainMapping to be the target of an event. Generated with gen-crd-api-reference-docs on git commit 813aa6596 .","title":"Serving"},{"location":"reference/api/eventing/_index/","text":"Macro Rendering Error \u00b6 TemplateSyntaxError : unexpected '<' Traceback (most recent call last): File \"/usr/local/lib/python3.9/site-packages/mkdocs_macros/plugin.py\", line 441, in render md_template = self.env.from_string(markdown) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 941, in from_string return cls.from_code(self, self.compile(source), globals, None) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 638, in compile self.handle_exception(source=source_hint) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 832, in handle_exception reraise(*rewrite_traceback_stack(source=source)) File \"/usr/local/lib/python3.9/site-packages/jinja2/_compat.py\", line 28, in reraise raise value.with_traceback(tb) File \"<unknown>\", line 3, in template jinja2.exceptions.TemplateSyntaxError: unexpected '<'","title":"Knative Eventing Component"},{"location":"reference/api/eventing/_index/#macro-rendering-error","text":"TemplateSyntaxError : unexpected '<' Traceback (most recent call last): File \"/usr/local/lib/python3.9/site-packages/mkdocs_macros/plugin.py\", line 441, in render md_template = self.env.from_string(markdown) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 941, in from_string return cls.from_code(self, self.compile(source), globals, None) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 638, in compile self.handle_exception(source=source_hint) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 832, in handle_exception reraise(*rewrite_traceback_stack(source=source)) File \"/usr/local/lib/python3.9/site-packages/jinja2/_compat.py\", line 28, in reraise raise value.with_traceback(tb) File \"<unknown>\", line 3, in template jinja2.exceptions.TemplateSyntaxError: unexpected '<'","title":"Macro Rendering Error"},{"location":"reference/api/eventing/eventing/","text":"Packages: duck.knative.dev/v1 duck.knative.dev/v1beta1 eventing.knative.dev/v1 eventing.knative.dev/v1beta1 flows.knative.dev/v1 flows.knative.dev/v1beta1 messaging.knative.dev/v1 messaging.knative.dev/v1beta1 sources.knative.dev/v1 sources.knative.dev/v1alpha1 sources.knative.dev/v1alpha2 sources.knative.dev/v1beta1 sources.knative.dev/v1beta2 duck.knative.dev/v1 Package v1 is the v1 version of the API. Resource Types: BackoffPolicyType ( string alias) ( Appears on: DeliverySpec ) BackoffPolicyType is the type for backoff policies Value Description \"exponential\" Exponential backoff policy \"linear\" Linear backoff policy Channelable Channelable is a skeleton type wrapping Subscribable and Addressable in the manner we expect resource writers defining compatible resources to embed it. We will typically use this type to deserialize Channelable ObjectReferences and access their subscription and address data. This is not a real resource. Field Description metadata Kubernetes meta/v1.ObjectMeta Refer to the Kubernetes API documentation for the fields of the metadata field. spec ChannelableSpec Spec is the part where the Channelable fulfills the Subscribable contract. SubscribableSpec SubscribableSpec (Members of SubscribableSpec are embedded into this type.) delivery DeliverySpec (Optional) DeliverySpec contains the default delivery spec for each subscription to this Channelable. Each subscription delivery spec, if any, overrides this global delivery spec. status ChannelableStatus ChannelableSpec ( Appears on: Channelable , ChannelSpec , InMemoryChannelSpec ) ChannelableSpec contains Spec of the Channelable object Field Description SubscribableSpec SubscribableSpec (Members of SubscribableSpec are embedded into this type.) delivery DeliverySpec (Optional) DeliverySpec contains the default delivery spec for each subscription to this Channelable. Each subscription delivery spec, if any, overrides this global delivery spec. ChannelableStatus ( Appears on: Channelable , ChannelStatus , InMemoryChannelStatus ) ChannelableStatus contains the Status of a Channelable object. Field Description Status knative.dev/pkg/apis/duck/v1.Status (Members of Status are embedded into this type.) inherits duck/v1 Status, which currently provides: * ObservedGeneration - the \u2018Generation\u2019 of the Service that was last processed by the controller. * Conditions - the latest available observations of a resource\u2019s current state. AddressStatus knative.dev/pkg/apis/duck/v1.AddressStatus (Members of AddressStatus are embedded into this type.) AddressStatus is the part where the Channelable fulfills the Addressable contract. SubscribableStatus SubscribableStatus (Members of SubscribableStatus are embedded into this type.) Subscribers is populated with the statuses of each of the Channelable\u2019s subscribers. deadLetterChannel knative.dev/pkg/apis/duck/v1.KReference (Optional) DeadLetterChannel is a KReference and is set by the channel when it supports native error handling via a channel Failed messages are delivered here. DeliverySpec ( Appears on: ChannelableSpec , SubscriberSpec , BrokerSpec , TriggerSpec , TriggerSpec , ParallelBranch , SequenceStep , SubscriptionSpec ) DeliverySpec contains the delivery options for event senders, such as channelable and source. Field Description deadLetterSink knative.dev/pkg/apis/duck/v1.Destination (Optional) DeadLetterSink is the sink receiving event that could not be sent to a destination. retry int32 (Optional) Retry is the minimum number of retries the sender should attempt when sending an event before moving it to the dead letter sink. backoffPolicy BackoffPolicyType (Optional) BackoffPolicy is the retry backoff policy (linear, exponential). backoffDelay string (Optional) BackoffDelay is the delay before retrying. More information on Duration format: - https://www.iso.org/iso-8601-date-and-time-format.html - https://en.wikipedia.org/wiki/ISO_8601 For linear policy, backoff delay is backoffDelay* . For exponential policy, backoff delay is backoffDelay*2^ . DeliveryStatus DeliveryStatus contains the Status of an object supporting delivery options. Field Description deadLetterChannel knative.dev/pkg/apis/duck/v1.KReference (Optional) DeadLetterChannel is a KReference that is the reference to the native, platform specific channel where failed events are sent to. Subscribable Subscribable is a skeleton type wrapping Subscribable in the manner we expect resource writers defining compatible resources to embed it. We will typically use this type to deserialize SubscribableType ObjectReferences and access the Subscription data. This is not a real resource. Field Description metadata Kubernetes meta/v1.ObjectMeta Refer to the Kubernetes API documentation for the fields of the metadata field. spec SubscribableSpec SubscribableSpec is the part where Subscribable object is configured as to be compatible with Subscribable contract. subscribers []SubscriberSpec This is the list of subscriptions for this subscribable. status SubscribableStatus SubscribableStatus is the part where SubscribableStatus object is configured as to be compatible with Subscribable contract. SubscribableSpec ( Appears on: ChannelableSpec , Subscribable ) SubscribableSpec shows how we expect folks to embed Subscribable in their Spec field. Field Description subscribers []SubscriberSpec This is the list of subscriptions for this subscribable. SubscribableStatus ( Appears on: ChannelableStatus , Subscribable ) SubscribableStatus is the schema for the subscribable\u2019s status portion of the status section of the resource. Field Description subscribers []SubscriberStatus This is the list of subscription\u2019s statuses for this channel. SubscriberSpec ( Appears on: SubscribableSpec ) SubscriberSpec defines a single subscriber to a Subscribable. At least one of SubscriberURI and ReplyURI must be present Field Description uid k8s.io/apimachinery/pkg/types.UID (Optional) UID is used to understand the origin of the subscriber. generation int64 (Optional) Generation of the origin of the subscriber with uid:UID. subscriberUri knative.dev/pkg/apis.URL (Optional) SubscriberURI is the endpoint for the subscriber replyUri knative.dev/pkg/apis.URL (Optional) ReplyURI is the endpoint for the reply delivery DeliverySpec (Optional) DeliverySpec contains options controlling the event delivery SubscriberStatus ( Appears on: SubscribableStatus ) SubscriberStatus defines the status of a single subscriber to a Channel. Field Description uid k8s.io/apimachinery/pkg/types.UID (Optional) UID is used to understand the origin of the subscriber. observedGeneration int64 (Optional) Generation of the origin of the subscriber with uid:UID. ready Kubernetes core/v1.ConditionStatus Status of the subscriber. message string (Optional) A human readable message indicating details of Ready status. duck.knative.dev/v1beta1 Package v1beta1 is the v1beta1 version of the API. Resource Types: BackoffPolicyType ( string alias) ( Appears on: DeliverySpec ) BackoffPolicyType is the type for backoff policies Value Description \"exponential\" Exponential backoff policy \"linear\" Linear backoff policy Channelable Channelable is a skeleton type wrapping Subscribable and Addressable in the manner we expect resource writers defining compatible resources to embed it. We will typically use this type to deserialize Channelable ObjectReferences and access their subscription and address data. This is not a real resource. Field Description metadata Kubernetes meta/v1.ObjectMeta Refer to the Kubernetes API documentation for the fields of the metadata field. spec ChannelableSpec Spec is the part where the Channelable fulfills the Subscribable contract. SubscribableSpec SubscribableSpec (Members of SubscribableSpec are embedded into this type.) delivery DeliverySpec (Optional) DeliverySpec contains options controlling the event delivery status ChannelableStatus ChannelableSpec ( Appears on: Channelable , ChannelSpec , InMemoryChannelSpec ) ChannelableSpec contains Spec of the Channelable object Field Description SubscribableSpec SubscribableSpec (Members of SubscribableSpec are embedded into this type.) delivery DeliverySpec (Optional) DeliverySpec contains options controlling the event delivery ChannelableStatus ( Appears on: Channelable , ChannelStatus , InMemoryChannelStatus ) ChannelableStatus contains the Status of a Channelable object. Field Description Status knative.dev/pkg/apis/duck/v1.Status (Members of Status are embedded into this type.) inherits duck/v1 Status, which currently provides: * ObservedGeneration - the \u2018Generation\u2019 of the Service that was last processed by the controller. * Conditions - the latest available observations of a resource\u2019s current state. AddressStatus knative.dev/pkg/apis/duck/v1.AddressStatus (Members of AddressStatus are embedded into this type.) AddressStatus is the part where the Channelable fulfills the Addressable contract. SubscribableStatus SubscribableStatus (Members of SubscribableStatus are embedded into this type.) Subscribers is populated with the statuses of each of the Channelable\u2019s subscribers. deadLetterChannel knative.dev/pkg/apis/duck/v1.KReference (Optional) DeadLetterChannel is a KReference and is set by the channel when it supports native error handling via a channel Failed messages are delivered here. DeliverySpec ( Appears on: ChannelableSpec , SubscriberSpec , BrokerSpec , ParallelBranch , SequenceStep , SubscriptionSpec ) DeliverySpec contains the delivery options for event senders, such as channelable and source. Field Description deadLetterSink knative.dev/pkg/apis/duck/v1.Destination (Optional) DeadLetterSink is the sink receiving event that could not be sent to a destination. retry int32 (Optional) Retry is the minimum number of retries the sender should attempt when sending an event before moving it to the dead letter sink. backoffPolicy BackoffPolicyType (Optional) BackoffPolicy is the retry backoff policy (linear, exponential). backoffDelay string (Optional) BackoffDelay is the delay before retrying. More information on Duration format: - https://www.iso.org/iso-8601-date-and-time-format.html - https://en.wikipedia.org/wiki/ISO_8601 For linear policy, backoff delay is backoffDelay* . For exponential policy, backoff delay is backoffDelay*2^ . DeliveryStatus DeliveryStatus contains the Status of an object supporting delivery options. Field Description deadLetterChannel knative.dev/pkg/apis/duck/v1.KReference (Optional) DeadLetterChannel is a KReference that is the reference to the native, platform specific channel where failed events are sent to. Subscribable Subscribable is a skeleton type wrapping Subscribable in the manner we expect resource writers defining compatible resources to embed it. We will typically use this type to deserialize SubscribableType ObjectReferences and access the Subscription data. This is not a real resource. Field Description metadata Kubernetes meta/v1.ObjectMeta Refer to the Kubernetes API documentation for the fields of the metadata field. spec SubscribableSpec SubscribableSpec is the part where Subscribable object is configured as to be compatible with Subscribable contract. subscribers []SubscriberSpec This is the list of subscriptions for this subscribable. status SubscribableStatus SubscribableStatus is the part where SubscribableStatus object is configured as to be compatible with Subscribable contract. SubscribableSpec ( Appears on: ChannelableSpec , Subscribable ) SubscribableSpec shows how we expect folks to embed Subscribable in their Spec field. Field Description subscribers []SubscriberSpec This is the list of subscriptions for this subscribable. SubscribableStatus ( Appears on: ChannelableStatus , Subscribable ) SubscribableStatus is the schema for the subscribable\u2019s status portion of the status section of the resource. Field Description subscribers []SubscriberStatus This is the list of subscription\u2019s statuses for this channel. SubscriberSpec ( Appears on: SubscribableSpec ) SubscriberSpec defines a single subscriber to a Subscribable. At least one of SubscriberURI and ReplyURI must be present Field Description uid k8s.io/apimachinery/pkg/types.UID (Optional) UID is used to understand the origin of the subscriber. generation int64 (Optional) Generation of the origin of the subscriber with uid:UID. subscriberUri knative.dev/pkg/apis.URL (Optional) SubscriberURI is the endpoint for the subscriber replyUri knative.dev/pkg/apis.URL (Optional) ReplyURI is the endpoint for the reply delivery DeliverySpec (Optional) DeliverySpec contains options controlling the event delivery SubscriberStatus ( Appears on: SubscribableStatus ) SubscriberStatus defines the status of a single subscriber to a Channel. Field Description uid k8s.io/apimachinery/pkg/types.UID (Optional) UID is used to understand the origin of the subscriber. observedGeneration int64 (Optional) Generation of the origin of the subscriber with uid:UID. ready Kubernetes core/v1.ConditionStatus Status of the subscriber. message string (Optional) A human readable message indicating details of Ready status. eventing.knative.dev/v1 Package v1 is the v1 version of the API. Resource Types: Broker Trigger Broker Broker collects a pool of events that are consumable using Triggers. Brokers provide a well-known endpoint for event delivery that senders can use with minimal knowledge of the event routing strategy. Subscribers use Triggers to request delivery of events from a Broker\u2019s pool to a specific URL or Addressable endpoint. Field Description apiVersion string eventing.knative.dev/v1 kind string Broker metadata Kubernetes meta/v1.ObjectMeta (Optional) Refer to the Kubernetes API documentation for the fields of the metadata field. spec BrokerSpec Spec defines the desired state of the Broker. config knative.dev/pkg/apis/duck/v1.KReference (Optional) Config is a KReference to the configuration that specifies configuration options for this Broker. For example, this could be a pointer to a ConfigMap. delivery DeliverySpec (Optional) Delivery contains the delivery spec for each trigger to this Broker. Each trigger delivery spec, if any, overrides this global delivery spec. status BrokerStatus (Optional) Status represents the current state of the Broker. This data may be out of date. Trigger Trigger represents a request to have events delivered to a subscriber from a Broker\u2019s event pool. Field Description apiVersion string eventing.knative.dev/v1 kind string Trigger metadata Kubernetes meta/v1.ObjectMeta (Optional) Refer to the Kubernetes API documentation for the fields of the metadata field. spec TriggerSpec Spec defines the desired state of the Trigger. broker string Broker is the broker that this trigger receives events from. filter TriggerFilter (Optional) Filter is the filter to apply against all events from the Broker. Only events that pass this filter will be sent to the Subscriber. If not specified, will default to allowing all events. subscriber knative.dev/pkg/apis/duck/v1.Destination Subscriber is the addressable that receives events from the Broker that pass the Filter. It is required. delivery DeliverySpec (Optional) Delivery contains the delivery spec for this specific trigger. status TriggerStatus (Optional) Status represents the current state of the Trigger. This data may be out of date. BrokerSpec ( Appears on: Broker ) Field Description config knative.dev/pkg/apis/duck/v1.KReference (Optional) Config is a KReference to the configuration that specifies configuration options for this Broker. For example, this could be a pointer to a ConfigMap. delivery DeliverySpec (Optional) Delivery contains the delivery spec for each trigger to this Broker. Each trigger delivery spec, if any, overrides this global delivery spec. BrokerStatus ( Appears on: Broker ) BrokerStatus represents the current state of a Broker. Field Description Status knative.dev/pkg/apis/duck/v1.Status (Members of Status are embedded into this type.) inherits duck/v1 Status, which currently provides: * ObservedGeneration - the \u2018Generation\u2019 of the Broker that was last processed by the controller. * Conditions - the latest available observations of a resource\u2019s current state. address knative.dev/pkg/apis/duck/v1.Addressable Broker is Addressable. It exposes the endpoint as an URI to get events delivered into the Broker mesh. TriggerFilter ( Appears on: TriggerSpec ) Field Description attributes TriggerFilterAttributes (Optional) Attributes filters events by exact match on event context attributes. Each key in the map is compared with the equivalent key in the event context. An event passes the filter if all values are equal to the specified values. Nested context attributes are not supported as keys. Only string values are supported. TriggerFilterAttributes ( map[string]string alias) ( Appears on: TriggerFilter ) TriggerFilterAttributes is a map of context attribute names to values for filtering by equality. Only exact matches will pass the filter. You can use the value \u201c to indicate all strings match. TriggerSpec ( Appears on: Trigger ) Field Description broker string Broker is the broker that this trigger receives events from. filter TriggerFilter (Optional) Filter is the filter to apply against all events from the Broker. Only events that pass this filter will be sent to the Subscriber. If not specified, will default to allowing all events. subscriber knative.dev/pkg/apis/duck/v1.Destination Subscriber is the addressable that receives events from the Broker that pass the Filter. It is required. delivery DeliverySpec (Optional) Delivery contains the delivery spec for this specific trigger. TriggerStatus ( Appears on: Trigger ) TriggerStatus represents the current state of a Trigger. Field Description Status knative.dev/pkg/apis/duck/v1.Status (Members of Status are embedded into this type.) inherits duck/v1 Status, which currently provides: * ObservedGeneration - the \u2018Generation\u2019 of the Trigger that was last processed by the controller. * Conditions - the latest available observations of a resource\u2019s current state. subscriberUri knative.dev/pkg/apis.URL (Optional) SubscriberURI is the resolved URI of the receiver for this Trigger. eventing.knative.dev/v1beta1 Package v1beta1 is the v1beta1 version of the API. Resource Types: Broker EventType Trigger Broker Broker collects a pool of events that are consumable using Triggers. Brokers provide a well-known endpoint for event delivery that senders can use with minimal knowledge of the event routing strategy. Receivers use Triggers to request delivery of events from a Broker\u2019s pool to a specific URL or Addressable endpoint. Field Description apiVersion string eventing.knative.dev/v1beta1 kind string Broker metadata Kubernetes meta/v1.ObjectMeta (Optional) Refer to the Kubernetes API documentation for the fields of the metadata field. spec BrokerSpec Spec defines the desired state of the Broker. config knative.dev/pkg/apis/duck/v1.KReference (Optional) Config is a KReference to the configuration that specifies configuration options for this Broker. For example, this could be a pointer to a ConfigMap. delivery DeliverySpec (Optional) Delivery is the delivery specification for Events within the Broker mesh. This includes things like retries, DLQ, etc. status BrokerStatus (Optional) Status represents the current state of the Broker. This data may be out of date. EventType EventType represents a type of event that can be consumed from a Broker. Field Description apiVersion string eventing.knative.dev/v1beta1 kind string EventType metadata Kubernetes meta/v1.ObjectMeta (Optional) Refer to the Kubernetes API documentation for the fields of the metadata field. spec EventTypeSpec Spec defines the desired state of the EventType. type string Type represents the CloudEvents type. It is authoritative. source knative.dev/pkg/apis.URL (Optional) Source is a URI, it represents the CloudEvents source. schema knative.dev/pkg/apis.URL (Optional) Schema is a URI, it represents the CloudEvents schemaurl extension attribute. It may be a JSON schema, a protobuf schema, etc. It is optional. schemaData string (Optional) SchemaData allows the CloudEvents schema to be stored directly in the EventType. Content is dependent on the encoding. Optional attribute. The contents are not validated or manipulated by the system. broker string (Optional) TODO remove https://github.com/knative/eventing/issues/2750 Broker refers to the Broker that can provide the EventType. description string (Optional) Description is an optional field used to describe the EventType, in any meaningful way. status EventTypeStatus (Optional) Status represents the current state of the EventType. This data may be out of date. TODO might be removed https://github.com/knative/eventing/issues/2750 Trigger Trigger represents a request to have events delivered to a consumer from a Broker\u2019s event pool. Field Description apiVersion string eventing.knative.dev/v1beta1 kind string Trigger metadata Kubernetes meta/v1.ObjectMeta (Optional) Refer to the Kubernetes API documentation for the fields of the metadata field. spec TriggerSpec Spec defines the desired state of the Trigger. broker string Broker is the broker that this trigger receives events from. If not specified, will default to \u2018default\u2019. filter TriggerFilter (Optional) Filter is the filter to apply against all events from the Broker. Only events that pass this filter will be sent to the Subscriber. If not specified, will default to allowing all events. subscriber knative.dev/pkg/apis/duck/v1.Destination Subscriber is the addressable that receives events from the Broker that pass the Filter. It is required. delivery DeliverySpec (Optional) Delivery contains the delivery spec for this specific trigger. status TriggerStatus (Optional) Status represents the current state of the Trigger. This data may be out of date. BrokerSpec ( Appears on: Broker ) Field Description config knative.dev/pkg/apis/duck/v1.KReference (Optional) Config is a KReference to the configuration that specifies configuration options for this Broker. For example, this could be a pointer to a ConfigMap. delivery DeliverySpec (Optional) Delivery is the delivery specification for Events within the Broker mesh. This includes things like retries, DLQ, etc. BrokerStatus ( Appears on: Broker ) BrokerStatus represents the current state of a Broker. Field Description Status knative.dev/pkg/apis/duck/v1.Status (Members of Status are embedded into this type.) inherits duck/v1 Status, which currently provides: * ObservedGeneration - the \u2018Generation\u2019 of the Service that was last processed by the controller. * Conditions - the latest available observations of a resource\u2019s current state. address knative.dev/pkg/apis/duck/v1.Addressable Broker is Addressable. It exposes the endpoint as an URI to get events delivered into the Broker mesh. EventTypeSpec ( Appears on: EventType ) Field Description type string Type represents the CloudEvents type. It is authoritative. source knative.dev/pkg/apis.URL (Optional) Source is a URI, it represents the CloudEvents source. schema knative.dev/pkg/apis.URL (Optional) Schema is a URI, it represents the CloudEvents schemaurl extension attribute. It may be a JSON schema, a protobuf schema, etc. It is optional. schemaData string (Optional) SchemaData allows the CloudEvents schema to be stored directly in the EventType. Content is dependent on the encoding. Optional attribute. The contents are not validated or manipulated by the system. broker string (Optional) TODO remove https://github.com/knative/eventing/issues/2750 Broker refers to the Broker that can provide the EventType. description string (Optional) Description is an optional field used to describe the EventType, in any meaningful way. EventTypeStatus ( Appears on: EventType ) EventTypeStatus represents the current state of a EventType. Field Description Status knative.dev/pkg/apis/duck/v1.Status (Members of Status are embedded into this type.) inherits duck/v1 Status, which currently provides: * ObservedGeneration - the \u2018Generation\u2019 of the Service that was last processed by the controller. * Conditions - the latest available observations of a resource\u2019s current state. TriggerFilter ( Appears on: TriggerSpec ) Field Description attributes TriggerFilterAttributes (Optional) Attributes filters events by exact match on event context attributes. Each key in the map is compared with the equivalent key in the event context. An event passes the filter if all values are equal to the specified values. Nested context attributes are not supported as keys. Only string values are supported. TriggerFilterAttributes ( map[string]string alias) ( Appears on: TriggerFilter ) TriggerFilterAttributes is a map of context attribute names to values for filtering by equality. Only exact matches will pass the filter. You can use the value \u201c to indicate all strings match. TriggerSpec ( Appears on: Trigger ) Field Description broker string Broker is the broker that this trigger receives events from. If not specified, will default to \u2018default\u2019. filter TriggerFilter (Optional) Filter is the filter to apply against all events from the Broker. Only events that pass this filter will be sent to the Subscriber. If not specified, will default to allowing all events. subscriber knative.dev/pkg/apis/duck/v1.Destination Subscriber is the addressable that receives events from the Broker that pass the Filter. It is required. delivery DeliverySpec (Optional) Delivery contains the delivery spec for this specific trigger. TriggerStatus ( Appears on: Trigger ) TriggerStatus represents the current state of a Trigger. Field Description Status knative.dev/pkg/apis/duck/v1.Status (Members of Status are embedded into this type.) inherits duck/v1 Status, which currently provides: * ObservedGeneration - the \u2018Generation\u2019 of the Service that was last processed by the controller. * Conditions - the latest available observations of a resource\u2019s current state. subscriberUri knative.dev/pkg/apis.URL (Optional) SubscriberURI is the resolved URI of the receiver for this Trigger. flows.knative.dev/v1 Package v1 is the v1 version of the API. Resource Types: Parallel Parallel defines conditional branches that will be wired in series through Channels and Subscriptions. Field Description metadata Kubernetes meta/v1.ObjectMeta (Optional) Refer to the Kubernetes API documentation for the fields of the metadata field. spec ParallelSpec Spec defines the desired state of the Parallel. branches []ParallelBranch Branches is the list of Filter/Subscribers pairs. channelTemplate ChannelTemplateSpec (Optional) ChannelTemplate specifies which Channel CRD to use. If left unspecified, it is set to the default Channel CRD for the namespace (or cluster, in case there are no defaults for the namespace). reply knative.dev/pkg/apis/duck/v1.Destination (Optional) Reply is a Reference to where the result of a case Subscriber gets sent to when the case does not have a Reply status ParallelStatus (Optional) Status represents the current state of the Parallel. This data may be out of date. ParallelBranch ( Appears on: ParallelSpec ) Field Description filter knative.dev/pkg/apis/duck/v1.Destination (Optional) Filter is the expression guarding the branch subscriber knative.dev/pkg/apis/duck/v1.Destination Subscriber receiving the event when the filter passes reply knative.dev/pkg/apis/duck/v1.Destination (Optional) Reply is a Reference to where the result of Subscriber of this case gets sent to. If not specified, sent the result to the Parallel Reply delivery DeliverySpec (Optional) Delivery is the delivery specification for events to the subscriber This includes things like retries, DLQ, etc. ParallelBranchStatus ( Appears on: ParallelStatus ) ParallelBranchStatus represents the current state of a Parallel branch Field Description filterSubscriptionStatus ParallelSubscriptionStatus FilterSubscriptionStatus corresponds to the filter subscription status. filterChannelStatus ParallelChannelStatus FilterChannelStatus corresponds to the filter channel status. subscriberSubscriptionStatus ParallelSubscriptionStatus SubscriptionStatus corresponds to the subscriber subscription status. ParallelChannelStatus ( Appears on: ParallelBranchStatus , ParallelStatus ) Field Description channel Kubernetes core/v1.ObjectReference Channel is the reference to the underlying channel. ready knative.dev/pkg/apis.Condition ReadyCondition indicates whether the Channel is ready or not. ParallelSpec ( Appears on: Parallel ) Field Description branches []ParallelBranch Branches is the list of Filter/Subscribers pairs. channelTemplate ChannelTemplateSpec (Optional) ChannelTemplate specifies which Channel CRD to use. If left unspecified, it is set to the default Channel CRD for the namespace (or cluster, in case there are no defaults for the namespace). reply knative.dev/pkg/apis/duck/v1.Destination (Optional) Reply is a Reference to where the result of a case Subscriber gets sent to when the case does not have a Reply ParallelStatus ( Appears on: Parallel ) ParallelStatus represents the current state of a Parallel. Field Description Status knative.dev/pkg/apis/duck/v1.Status (Members of Status are embedded into this type.) inherits duck/v1 Status, which currently provides: * ObservedGeneration - the \u2018Generation\u2019 of the Service that was last processed by the controller. * Conditions - the latest available observations of a resource\u2019s current state. ingressChannelStatus ParallelChannelStatus IngressChannelStatus corresponds to the ingress channel status. branchStatuses []ParallelBranchStatus BranchStatuses is an array of corresponding to branch statuses. Matches the Spec.Branches array in the order. AddressStatus knative.dev/pkg/apis/duck/v1.AddressStatus (Members of AddressStatus are embedded into this type.) AddressStatus is the starting point to this Parallel. Sending to this will target the first subscriber. It generally has the form {channel}.{namespace}.svc.{cluster domain name} ParallelSubscriptionStatus ( Appears on: ParallelBranchStatus ) Field Description subscription Kubernetes core/v1.ObjectReference Subscription is the reference to the underlying Subscription. ready knative.dev/pkg/apis.Condition ReadyCondition indicates whether the Subscription is ready or not. Sequence Sequence defines a sequence of Subscribers that will be wired in series through Channels and Subscriptions. Field Description metadata Kubernetes meta/v1.ObjectMeta (Optional) Refer to the Kubernetes API documentation for the fields of the metadata field. spec SequenceSpec Spec defines the desired state of the Sequence. steps []SequenceStep Steps is the list of Destinations (processors / functions) that will be called in the order provided. Each step has its own delivery options channelTemplate ChannelTemplateSpec (Optional) ChannelTemplate specifies which Channel CRD to use. If left unspecified, it is set to the default Channel CRD for the namespace (or cluster, in case there are no defaults for the namespace). reply knative.dev/pkg/apis/duck/v1.Destination (Optional) Reply is a Reference to where the result of the last Subscriber gets sent to. status SequenceStatus (Optional) Status represents the current state of the Sequence. This data may be out of date. SequenceChannelStatus ( Appears on: SequenceStatus ) Field Description channel Kubernetes core/v1.ObjectReference Channel is the reference to the underlying channel. ready knative.dev/pkg/apis.Condition ReadyCondition indicates whether the Channel is ready or not. SequenceSpec ( Appears on: Sequence ) Field Description steps []SequenceStep Steps is the list of Destinations (processors / functions) that will be called in the order provided. Each step has its own delivery options channelTemplate ChannelTemplateSpec (Optional) ChannelTemplate specifies which Channel CRD to use. If left unspecified, it is set to the default Channel CRD for the namespace (or cluster, in case there are no defaults for the namespace). reply knative.dev/pkg/apis/duck/v1.Destination (Optional) Reply is a Reference to where the result of the last Subscriber gets sent to. SequenceStatus ( Appears on: Sequence ) SequenceStatus represents the current state of a Sequence. Field Description Status knative.dev/pkg/apis/duck/v1.Status (Members of Status are embedded into this type.) inherits duck/v1 Status, which currently provides: * ObservedGeneration - the \u2018Generation\u2019 of the Service that was last processed by the controller. * Conditions - the latest available observations of a resource\u2019s current state. subscriptionStatuses []SequenceSubscriptionStatus SubscriptionStatuses is an array of corresponding Subscription statuses. Matches the Spec.Steps array in the order. channelStatuses []SequenceChannelStatus ChannelStatuses is an array of corresponding Channel statuses. Matches the Spec.Steps array in the order. AddressStatus knative.dev/pkg/apis/duck/v1.AddressStatus (Members of AddressStatus are embedded into this type.) AddressStatus is the starting point to this Sequence. Sending to this will target the first subscriber. It generally has the form {channel}.{namespace}.svc.{cluster domain name} SequenceStep ( Appears on: SequenceSpec ) Field Description Destination knative.dev/pkg/apis/duck/v1.Destination (Members of Destination are embedded into this type.) Subscriber receiving the step event delivery DeliverySpec (Optional) Delivery is the delivery specification for events to the subscriber This includes things like retries, DLQ, etc. SequenceSubscriptionStatus ( Appears on: SequenceStatus ) Field Description subscription Kubernetes core/v1.ObjectReference Subscription is the reference to the underlying Subscription. ready knative.dev/pkg/apis.Condition ReadyCondition indicates whether the Subscription is ready or not. flows.knative.dev/v1beta1 Package v1beta1 is the v1beta1 version of the API. Resource Types: Parallel Parallel defines conditional branches that will be wired in series through Channels and Subscriptions. Field Description metadata Kubernetes meta/v1.ObjectMeta (Optional) Refer to the Kubernetes API documentation for the fields of the metadata field. spec ParallelSpec Spec defines the desired state of the Parallel. branches []ParallelBranch Branches is the list of Filter/Subscribers pairs. channelTemplate ChannelTemplateSpec (Optional) ChannelTemplate specifies which Channel CRD to use. If left unspecified, it is set to the default Channel CRD for the namespace (or cluster, in case there are no defaults for the namespace). reply knative.dev/pkg/apis/duck/v1.Destination (Optional) Reply is a Reference to where the result of a case Subscriber gets sent to when the case does not have a Reply status ParallelStatus (Optional) Status represents the current state of the Parallel. This data may be out of date. ParallelBranch ( Appears on: ParallelSpec ) Field Description filter knative.dev/pkg/apis/duck/v1.Destination (Optional) Filter is the expression guarding the branch subscriber knative.dev/pkg/apis/duck/v1.Destination Subscriber receiving the event when the filter passes reply knative.dev/pkg/apis/duck/v1.Destination (Optional) Reply is a Reference to where the result of Subscriber of this case gets sent to. If not specified, sent the result to the Parallel Reply delivery DeliverySpec (Optional) Delivery is the delivery specification for events to the subscriber This includes things like retries, DLQ, etc. Needed for Roundtripping v1alpha1 <-> v1beta1. ParallelBranchStatus ( Appears on: ParallelStatus ) ParallelBranchStatus represents the current state of a Parallel branch Field Description filterSubscriptionStatus ParallelSubscriptionStatus FilterSubscriptionStatus corresponds to the filter subscription status. filterChannelStatus ParallelChannelStatus FilterChannelStatus corresponds to the filter channel status. subscriberSubscriptionStatus ParallelSubscriptionStatus SubscriptionStatus corresponds to the subscriber subscription status. ParallelChannelStatus ( Appears on: ParallelBranchStatus , ParallelStatus ) Field Description channel Kubernetes core/v1.ObjectReference Channel is the reference to the underlying channel. ready knative.dev/pkg/apis.Condition ReadyCondition indicates whether the Channel is ready or not. ParallelSpec ( Appears on: Parallel ) Field Description branches []ParallelBranch Branches is the list of Filter/Subscribers pairs. channelTemplate ChannelTemplateSpec (Optional) ChannelTemplate specifies which Channel CRD to use. If left unspecified, it is set to the default Channel CRD for the namespace (or cluster, in case there are no defaults for the namespace). reply knative.dev/pkg/apis/duck/v1.Destination (Optional) Reply is a Reference to where the result of a case Subscriber gets sent to when the case does not have a Reply ParallelStatus ( Appears on: Parallel ) ParallelStatus represents the current state of a Parallel. Field Description Status knative.dev/pkg/apis/duck/v1.Status (Members of Status are embedded into this type.) inherits duck/v1 Status, which currently provides: * ObservedGeneration - the \u2018Generation\u2019 of the Service that was last processed by the controller. * Conditions - the latest available observations of a resource\u2019s current state. ingressChannelStatus ParallelChannelStatus IngressChannelStatus corresponds to the ingress channel status. branchStatuses []ParallelBranchStatus BranchStatuses is an array of corresponding to branch statuses. Matches the Spec.Branches array in the order. AddressStatus knative.dev/pkg/apis/duck/v1.AddressStatus (Members of AddressStatus are embedded into this type.) AddressStatus is the starting point to this Parallel. Sending to this will target the first subscriber. It generally has the form {channel}.{namespace}.svc.{cluster domain name} ParallelSubscriptionStatus ( Appears on: ParallelBranchStatus ) Field Description subscription Kubernetes core/v1.ObjectReference Subscription is the reference to the underlying Subscription. ready knative.dev/pkg/apis.Condition ReadyCondition indicates whether the Subscription is ready or not. Sequence Sequence defines a sequence of Subscribers that will be wired in series through Channels and Subscriptions. Field Description metadata Kubernetes meta/v1.ObjectMeta (Optional) Refer to the Kubernetes API documentation for the fields of the metadata field. spec SequenceSpec Spec defines the desired state of the Sequence. steps []SequenceStep Steps is the list of Destinations (processors / functions) that will be called in the order provided. Each step has its own delivery options channelTemplate ChannelTemplateSpec (Optional) ChannelTemplate specifies which Channel CRD to use. If left unspecified, it is set to the default Channel CRD for the namespace (or cluster, in case there are no defaults for the namespace). reply knative.dev/pkg/apis/duck/v1.Destination (Optional) Reply is a Reference to where the result of the last Subscriber gets sent to. status SequenceStatus (Optional) Status represents the current state of the Sequence. This data may be out of date. SequenceChannelStatus ( Appears on: SequenceStatus ) Field Description channel Kubernetes core/v1.ObjectReference Channel is the reference to the underlying channel. ready knative.dev/pkg/apis.Condition ReadyCondition indicates whether the Channel is ready or not. SequenceSpec ( Appears on: Sequence ) Field Description steps []SequenceStep Steps is the list of Destinations (processors / functions) that will be called in the order provided. Each step has its own delivery options channelTemplate ChannelTemplateSpec (Optional) ChannelTemplate specifies which Channel CRD to use. If left unspecified, it is set to the default Channel CRD for the namespace (or cluster, in case there are no defaults for the namespace). reply knative.dev/pkg/apis/duck/v1.Destination (Optional) Reply is a Reference to where the result of the last Subscriber gets sent to. SequenceStatus ( Appears on: Sequence ) SequenceStatus represents the current state of a Sequence. Field Description Status knative.dev/pkg/apis/duck/v1.Status (Members of Status are embedded into this type.) inherits duck/v1 Status, which currently provides: * ObservedGeneration - the \u2018Generation\u2019 of the Service that was last processed by the controller. * Conditions - the latest available observations of a resource\u2019s current state. subscriptionStatuses []SequenceSubscriptionStatus SubscriptionStatuses is an array of corresponding Subscription statuses. Matches the Spec.Steps array in the order. channelStatuses []SequenceChannelStatus ChannelStatuses is an array of corresponding Channel statuses. Matches the Spec.Steps array in the order. AddressStatus knative.dev/pkg/apis/duck/v1.AddressStatus (Members of AddressStatus are embedded into this type.) AddressStatus is the starting point to this Sequence. Sending to this will target the first subscriber. It generally has the form {channel}.{namespace}.svc.{cluster domain name} SequenceStep ( Appears on: SequenceSpec ) Field Description Destination knative.dev/pkg/apis/duck/v1.Destination (Members of Destination are embedded into this type.) Subscriber receiving the step event delivery DeliverySpec (Optional) Delivery is the delivery specification for events to the subscriber This includes things like retries, DLQ, etc. SequenceSubscriptionStatus ( Appears on: SequenceStatus ) Field Description subscription Kubernetes core/v1.ObjectReference Subscription is the reference to the underlying Subscription. ready knative.dev/pkg/apis.Condition ReadyCondition indicates whether the Subscription is ready or not. messaging.knative.dev/v1 Package v1 is the v1 version of the API. Resource Types: Channel InMemoryChannel Subscription Channel Channel represents a generic Channel. It is normally used when we want a Channel, but don\u2019t need a specific Channel implementation. Field Description apiVersion string messaging.knative.dev/v1 kind string Channel metadata Kubernetes meta/v1.ObjectMeta (Optional) Refer to the Kubernetes API documentation for the fields of the metadata field. spec ChannelSpec Spec defines the desired state of the Channel. channelTemplate ChannelTemplateSpec ChannelTemplate specifies which Channel CRD to use to create the CRD Channel backing this Channel. This is immutable after creation. Normally this is set by the Channel defaulter, not directly by the user. ChannelableSpec ChannelableSpec (Members of ChannelableSpec are embedded into this type.) Channel conforms to ChannelableSpec status ChannelStatus (Optional) Status represents the current state of the Channel. This data may be out of date. InMemoryChannel InMemoryChannel is a resource representing an in memory channel Field Description apiVersion string messaging.knative.dev/v1 kind string InMemoryChannel metadata Kubernetes meta/v1.ObjectMeta (Optional) Refer to the Kubernetes API documentation for the fields of the metadata field. spec InMemoryChannelSpec Spec defines the desired state of the Channel. ChannelableSpec ChannelableSpec (Members of ChannelableSpec are embedded into this type.) Channel conforms to Duck type Channelable. status InMemoryChannelStatus (Optional) Status represents the current state of the Channel. This data may be out of date. Subscription Subscription routes events received on a Channel to a DNS name and corresponds to the subscriptions.channels.knative.dev CRD. Field Description apiVersion string messaging.knative.dev/v1 kind string Subscription metadata Kubernetes meta/v1.ObjectMeta Refer to the Kubernetes API documentation for the fields of the metadata field. spec SubscriptionSpec channel Kubernetes core/v1.ObjectReference Reference to a channel that will be used to create the subscription You can specify only the following fields of the ObjectReference: - Kind - APIVersion - Name The resource pointed by this ObjectReference must meet the contract to the ChannelableSpec duck type. If the resource does not meet this contract it will be reflected in the Subscription\u2019s status. This field is immutable. We have no good answer on what happens to the events that are currently in the channel being consumed from and what the semantics there should be. For now, you can always delete the Subscription and recreate it to point to a different channel, giving the user more control over what semantics should be used (drain the channel first, possibly have events dropped, etc.) subscriber knative.dev/pkg/apis/duck/v1.Destination (Optional) Subscriber is reference to (optional) function for processing events. Events from the Channel will be delivered here and replies are sent to a Destination as specified by the Reply. reply knative.dev/pkg/apis/duck/v1.Destination (Optional) Reply specifies (optionally) how to handle events returned from the Subscriber target. delivery DeliverySpec (Optional) Delivery configuration status SubscriptionStatus ChannelDefaulter ChannelDefaulter sets the default Channel CRD and Arguments on Channels that do not specify any implementation. ChannelSpec ( Appears on: Channel ) ChannelSpec defines which subscribers have expressed interest in receiving events from this Channel. It also defines the ChannelTemplate to use in order to create the CRD Channel backing this Channel. Field Description channelTemplate ChannelTemplateSpec ChannelTemplate specifies which Channel CRD to use to create the CRD Channel backing this Channel. This is immutable after creation. Normally this is set by the Channel defaulter, not directly by the user. ChannelableSpec ChannelableSpec (Members of ChannelableSpec are embedded into this type.) Channel conforms to ChannelableSpec ChannelStatus ( Appears on: Channel ) ChannelStatus represents the current state of a Channel. Field Description ChannelableStatus ChannelableStatus (Members of ChannelableStatus are embedded into this type.) Channel conforms to ChannelableStatus channel knative.dev/pkg/apis/duck/v1.KReference Channel is an KReference to the Channel CRD backing this Channel. ChannelTemplateSpec ( Appears on: ParallelSpec , SequenceSpec , ChannelSpec ) Field Description spec k8s.io/apimachinery/pkg/runtime.RawExtension (Optional) Spec defines the Spec to use for each channel created. Passed in verbatim to the Channel CRD as Spec section. InMemoryChannelSpec ( Appears on: InMemoryChannel ) InMemoryChannelSpec defines which subscribers have expressed interest in receiving events from this InMemoryChannel. arguments for a Channel. Field Description ChannelableSpec ChannelableSpec (Members of ChannelableSpec are embedded into this type.) Channel conforms to Duck type Channelable. InMemoryChannelStatus ( Appears on: InMemoryChannel ) ChannelStatus represents the current state of a Channel. Field Description ChannelableStatus ChannelableStatus (Members of ChannelableStatus are embedded into this type.) Channel conforms to Duck type Channelable. SubscriptionSpec ( Appears on: Subscription ) SubscriptionSpec specifies the Channel for incoming events, a Subscriber target for processing those events and where to put the result of the processing. Only From (where the events are coming from) is always required. You can optionally only Process the events (results in no output events) by leaving out the Result. You can also perform an identity transformation on the incoming events by leaving out the Subscriber and only specifying Result. The following are all valid specifications: channel \u2013[subscriber]\u2013> reply Sink, no outgoing events: channel \u2013 subscriber no-op function (identity transformation): channel \u2013> reply Field Description channel Kubernetes core/v1.ObjectReference Reference to a channel that will be used to create the subscription You can specify only the following fields of the ObjectReference: - Kind - APIVersion - Name The resource pointed by this ObjectReference must meet the contract to the ChannelableSpec duck type. If the resource does not meet this contract it will be reflected in the Subscription\u2019s status. This field is immutable. We have no good answer on what happens to the events that are currently in the channel being consumed from and what the semantics there should be. For now, you can always delete the Subscription and recreate it to point to a different channel, giving the user more control over what semantics should be used (drain the channel first, possibly have events dropped, etc.) subscriber knative.dev/pkg/apis/duck/v1.Destination (Optional) Subscriber is reference to (optional) function for processing events. Events from the Channel will be delivered here and replies are sent to a Destination as specified by the Reply. reply knative.dev/pkg/apis/duck/v1.Destination (Optional) Reply specifies (optionally) how to handle events returned from the Subscriber target. delivery DeliverySpec (Optional) Delivery configuration SubscriptionStatus ( Appears on: Subscription ) SubscriptionStatus (computed) for a subscription Field Description Status knative.dev/pkg/apis/duck/v1.Status (Members of Status are embedded into this type.) inherits duck/v1 Status, which currently provides: * ObservedGeneration - the \u2018Generation\u2019 of the Service that was last processed by the controller. * Conditions - the latest available observations of a resource\u2019s current state. physicalSubscription SubscriptionStatusPhysicalSubscription PhysicalSubscription is the fully resolved values that this Subscription represents. SubscriptionStatusPhysicalSubscription ( Appears on: SubscriptionStatus ) SubscriptionStatusPhysicalSubscription represents the fully resolved values for this Subscription. Field Description subscriberUri knative.dev/pkg/apis.URL SubscriberURI is the fully resolved URI for spec.subscriber. replyUri knative.dev/pkg/apis.URL ReplyURI is the fully resolved URI for the spec.reply. deadLetterSinkUri knative.dev/pkg/apis.URL ReplyURI is the fully resolved URI for the spec.delivery.deadLetterSink. messaging.knative.dev/v1beta1 Package v1beta1 is the v1beta1 version of the API. Resource Types: Channel InMemoryChannel Subscription Channel Channel represents a generic Channel. It is normally used when we want a Channel, but don\u2019t need a specific Channel implementation. Field Description apiVersion string messaging.knative.dev/v1beta1 kind string Channel metadata Kubernetes meta/v1.ObjectMeta (Optional) Refer to the Kubernetes API documentation for the fields of the metadata field. spec ChannelSpec Spec defines the desired state of the Channel. channelTemplate ChannelTemplateSpec ChannelTemplate specifies which Channel CRD to use to create the CRD Channel backing this Channel. This is immutable after creation. Normally this is set by the Channel defaulter, not directly by the user. ChannelableSpec ChannelableSpec (Members of ChannelableSpec are embedded into this type.) Channel conforms to ChannelableSpec status ChannelStatus (Optional) Status represents the current state of the Channel. This data may be out of date. InMemoryChannel InMemoryChannel is a resource representing an in memory channel Field Description apiVersion string messaging.knative.dev/v1beta1 kind string InMemoryChannel metadata Kubernetes meta/v1.ObjectMeta (Optional) Refer to the Kubernetes API documentation for the fields of the metadata field. spec InMemoryChannelSpec Spec defines the desired state of the Channel. ChannelableSpec ChannelableSpec (Members of ChannelableSpec are embedded into this type.) Channel conforms to Duck type Channelable. status InMemoryChannelStatus (Optional) Status represents the current state of the Channel. This data may be out of date. Subscription Subscription routes events received on a Channel to a DNS name and corresponds to the subscriptions.channels.knative.dev CRD. Field Description apiVersion string messaging.knative.dev/v1beta1 kind string Subscription metadata Kubernetes meta/v1.ObjectMeta Refer to the Kubernetes API documentation for the fields of the metadata field. spec SubscriptionSpec channel Kubernetes core/v1.ObjectReference Reference to a channel that will be used to create the subscription You can specify only the following fields of the ObjectReference: - Kind - APIVersion - Name The resource pointed by this ObjectReference must meet the contract to the ChannelableSpec duck type. If the resource does not meet this contract it will be reflected in the Subscription\u2019s status. This field is immutable. We have no good answer on what happens to the events that are currently in the channel being consumed from and what the semantics there should be. For now, you can always delete the Subscription and recreate it to point to a different channel, giving the user more control over what semantics should be used (drain the channel first, possibly have events dropped, etc.) subscriber knative.dev/pkg/apis/duck/v1.Destination (Optional) Subscriber is reference to (optional) function for processing events. Events from the Channel will be delivered here and replies are sent to a Destination as specified by the Reply. reply knative.dev/pkg/apis/duck/v1.Destination (Optional) Reply specifies (optionally) how to handle events returned from the Subscriber target. delivery DeliverySpec (Optional) Delivery configuration status SubscriptionStatus ChannelDefaulter ChannelDefaulter sets the default Channel CRD and Arguments on Channels that do not specify any implementation. ChannelSpec ( Appears on: Channel ) ChannelSpec defines which subscribers have expressed interest in receiving events from this Channel. It also defines the ChannelTemplate to use in order to create the CRD Channel backing this Channel. Field Description channelTemplate ChannelTemplateSpec ChannelTemplate specifies which Channel CRD to use to create the CRD Channel backing this Channel. This is immutable after creation. Normally this is set by the Channel defaulter, not directly by the user. ChannelableSpec ChannelableSpec (Members of ChannelableSpec are embedded into this type.) Channel conforms to ChannelableSpec ChannelStatus ( Appears on: Channel ) ChannelStatus represents the current state of a Channel. Field Description ChannelableStatus ChannelableStatus (Members of ChannelableStatus are embedded into this type.) Channel conforms to ChannelableStatus channel knative.dev/pkg/apis/duck/v1.KReference Channel is an KReference to the Channel CRD backing this Channel. ChannelTemplateSpec ( Appears on: ParallelSpec , SequenceSpec , ChannelSpec ) Field Description spec k8s.io/apimachinery/pkg/runtime.RawExtension (Optional) Spec defines the Spec to use for each channel created. Passed in verbatim to the Channel CRD as Spec section. InMemoryChannelSpec ( Appears on: InMemoryChannel ) InMemoryChannelSpec defines which subscribers have expressed interest in receiving events from this InMemoryChannel. arguments for a Channel. Field Description ChannelableSpec ChannelableSpec (Members of ChannelableSpec are embedded into this type.) Channel conforms to Duck type Channelable. InMemoryChannelStatus ( Appears on: InMemoryChannel ) ChannelStatus represents the current state of a Channel. Field Description ChannelableStatus ChannelableStatus (Members of ChannelableStatus are embedded into this type.) Channel conforms to Duck type Channelable. SubscriptionSpec ( Appears on: Subscription ) SubscriptionSpec specifies the Channel for incoming events, a Subscriber target for processing those events and where to put the result of the processing. Only From (where the events are coming from) is always required. You can optionally only Process the events (results in no output events) by leaving out the Result. You can also perform an identity transformation on the incoming events by leaving out the Subscriber and only specifying Result. The following are all valid specifications: channel \u2013[subscriber]\u2013> reply Sink, no outgoing events: channel \u2013 subscriber no-op function (identity transformation): channel \u2013> reply Field Description channel Kubernetes core/v1.ObjectReference Reference to a channel that will be used to create the subscription You can specify only the following fields of the ObjectReference: - Kind - APIVersion - Name The resource pointed by this ObjectReference must meet the contract to the ChannelableSpec duck type. If the resource does not meet this contract it will be reflected in the Subscription\u2019s status. This field is immutable. We have no good answer on what happens to the events that are currently in the channel being consumed from and what the semantics there should be. For now, you can always delete the Subscription and recreate it to point to a different channel, giving the user more control over what semantics should be used (drain the channel first, possibly have events dropped, etc.) subscriber knative.dev/pkg/apis/duck/v1.Destination (Optional) Subscriber is reference to (optional) function for processing events. Events from the Channel will be delivered here and replies are sent to a Destination as specified by the Reply. reply knative.dev/pkg/apis/duck/v1.Destination (Optional) Reply specifies (optionally) how to handle events returned from the Subscriber target. delivery DeliverySpec (Optional) Delivery configuration SubscriptionStatus ( Appears on: Subscription ) SubscriptionStatus (computed) for a subscription Field Description Status knative.dev/pkg/apis/duck/v1.Status (Members of Status are embedded into this type.) inherits duck/v1 Status, which currently provides: * ObservedGeneration - the \u2018Generation\u2019 of the Service that was last processed by the controller. * Conditions - the latest available observations of a resource\u2019s current state. physicalSubscription SubscriptionStatusPhysicalSubscription PhysicalSubscription is the fully resolved values that this Subscription represents. SubscriptionStatusPhysicalSubscription ( Appears on: SubscriptionStatus ) SubscriptionStatusPhysicalSubscription represents the fully resolved values for this Subscription. Field Description subscriberUri knative.dev/pkg/apis.URL SubscriberURI is the fully resolved URI for spec.subscriber. replyUri knative.dev/pkg/apis.URL ReplyURI is the fully resolved URI for the spec.reply. deadLetterSinkUri knative.dev/pkg/apis.URL ReplyURI is the fully resolved URI for the spec.delivery.deadLetterSink. sources.knative.dev/v1 Package v1 contains API Schema definitions for the sources v1 API group. Resource Types: ApiServerSource ContainerSource SinkBinding ApiServerSource ApiServerSource is the Schema for the apiserversources API Field Description apiVersion string sources.knative.dev/v1 kind string ApiServerSource metadata Kubernetes meta/v1.ObjectMeta Refer to the Kubernetes API documentation for the fields of the metadata field. spec ApiServerSourceSpec SourceSpec knative.dev/pkg/apis/duck/v1.SourceSpec (Members of SourceSpec are embedded into this type.) inherits duck/v1 SourceSpec, which currently provides: * Sink - a reference to an object that will resolve to a domain name or a URI directly to use as the sink. * CloudEventOverrides - defines overrides to control the output format and modifications of the event sent to the sink. resources []APIVersionKindSelector Resource are the resources this source will track and send related lifecycle events from the Kubernetes ApiServer, with an optional label selector to help filter. owner APIVersionKind (Optional) ResourceOwner is an additional filter to only track resources that are owned by a specific resource type. If ResourceOwner matches Resources[n] then Resources[n] is allowed to pass the ResourceOwner filter. mode string (Optional) EventMode controls the format of the event. Reference sends a dataref event type for the resource under watch. Resource send the full resource lifecycle event. Defaults to Reference serviceAccountName string (Optional) ServiceAccountName is the name of the ServiceAccount to use to run this source. Defaults to default if not set. status ApiServerSourceStatus ContainerSource ContainerSource is the Schema for the containersources API Field Description apiVersion string sources.knative.dev/v1 kind string ContainerSource metadata Kubernetes meta/v1.ObjectMeta Refer to the Kubernetes API documentation for the fields of the metadata field. spec ContainerSourceSpec SourceSpec knative.dev/pkg/apis/duck/v1.SourceSpec (Members of SourceSpec are embedded into this type.) inherits duck/v1 SourceSpec, which currently provides: * Sink - a reference to an object that will resolve to a domain name or a URI directly to use as the sink. * CloudEventOverrides - defines overrides to control the output format and modifications of the event sent to the sink. template Kubernetes core/v1.PodTemplateSpec Template describes the pods that will be created status ContainerSourceStatus SinkBinding SinkBinding describes a Binding that is also a Source. The sink (from the Source duck) is resolved to a URL and then projected into the subject by augmenting the runtime contract of the referenced containers to have a K_SINK environment variable holding the endpoint to which to send cloud events. Field Description apiVersion string sources.knative.dev/v1 kind string SinkBinding metadata Kubernetes meta/v1.ObjectMeta Refer to the Kubernetes API documentation for the fields of the metadata field. spec SinkBindingSpec SourceSpec knative.dev/pkg/apis/duck/v1.SourceSpec (Members of SourceSpec are embedded into this type.) inherits duck/v1 SourceSpec, which currently provides: * Sink - a reference to an object that will resolve to a domain name or a URI directly to use as the sink. * CloudEventOverrides - defines overrides to control the output format and modifications of the event sent to the sink. BindingSpec knative.dev/pkg/apis/duck/v1.BindingSpec (Members of BindingSpec are embedded into this type.) inherits duck/v1 BindingSpec, which currently provides: * Subject - Subject references the resource(s) whose \u201cruntime contract\u201d should be augmented by Binding implementations. status SinkBindingStatus APIVersionKind ( Appears on: ApiServerSourceSpec ) APIVersionKind is an APIVersion and Kind tuple. Field Description apiVersion string APIVersion - the API version of the resource to watch. kind string Kind of the resource to watch. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds APIVersionKindSelector ( Appears on: ApiServerSourceSpec ) APIVersionKindSelector is an APIVersion Kind tuple with a LabelSelector. Field Description apiVersion string APIVersion - the API version of the resource to watch. kind string Kind of the resource to watch. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds selector Kubernetes meta/v1.LabelSelector (Optional) LabelSelector filters this source to objects to those resources pass the label selector. More info: http://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#label-selectors ApiServerSourceSpec ( Appears on: ApiServerSource ) ApiServerSourceSpec defines the desired state of ApiServerSource Field Description SourceSpec knative.dev/pkg/apis/duck/v1.SourceSpec (Members of SourceSpec are embedded into this type.) inherits duck/v1 SourceSpec, which currently provides: * Sink - a reference to an object that will resolve to a domain name or a URI directly to use as the sink. * CloudEventOverrides - defines overrides to control the output format and modifications of the event sent to the sink. resources []APIVersionKindSelector Resource are the resources this source will track and send related lifecycle events from the Kubernetes ApiServer, with an optional label selector to help filter. owner APIVersionKind (Optional) ResourceOwner is an additional filter to only track resources that are owned by a specific resource type. If ResourceOwner matches Resources[n] then Resources[n] is allowed to pass the ResourceOwner filter. mode string (Optional) EventMode controls the format of the event. Reference sends a dataref event type for the resource under watch. Resource send the full resource lifecycle event. Defaults to Reference serviceAccountName string (Optional) ServiceAccountName is the name of the ServiceAccount to use to run this source. Defaults to default if not set. ApiServerSourceStatus ( Appears on: ApiServerSource ) ApiServerSourceStatus defines the observed state of ApiServerSource Field Description SourceStatus knative.dev/pkg/apis/duck/v1.SourceStatus (Members of SourceStatus are embedded into this type.) inherits duck/v1 SourceStatus, which currently provides: * ObservedGeneration - the \u2018Generation\u2019 of the Service that was last processed by the controller. * Conditions - the latest available observations of a resource\u2019s current state. * SinkURI - the current active sink URI that has been configured for the Source. ContainerSourceSpec ( Appears on: ContainerSource ) ContainerSourceSpec defines the desired state of ContainerSource Field Description SourceSpec knative.dev/pkg/apis/duck/v1.SourceSpec (Members of SourceSpec are embedded into this type.) inherits duck/v1 SourceSpec, which currently provides: * Sink - a reference to an object that will resolve to a domain name or a URI directly to use as the sink. * CloudEventOverrides - defines overrides to control the output format and modifications of the event sent to the sink. template Kubernetes core/v1.PodTemplateSpec Template describes the pods that will be created ContainerSourceStatus ( Appears on: ContainerSource ) ContainerSourceStatus defines the observed state of ContainerSource Field Description SourceStatus knative.dev/pkg/apis/duck/v1.SourceStatus (Members of SourceStatus are embedded into this type.) inherits duck/v1 SourceStatus, which currently provides: * ObservedGeneration - the \u2018Generation\u2019 of the Service that was last processed by the controller. * Conditions - the latest available observations of a resource\u2019s current state. * SinkURI - the current active sink URI that has been configured for the Source. SinkBindingSpec ( Appears on: SinkBinding ) SinkBindingSpec holds the desired state of the SinkBinding (from the client). Field Description SourceSpec knative.dev/pkg/apis/duck/v1.SourceSpec (Members of SourceSpec are embedded into this type.) inherits duck/v1 SourceSpec, which currently provides: * Sink - a reference to an object that will resolve to a domain name or a URI directly to use as the sink. * CloudEventOverrides - defines overrides to control the output format and modifications of the event sent to the sink. BindingSpec knative.dev/pkg/apis/duck/v1.BindingSpec (Members of BindingSpec are embedded into this type.) inherits duck/v1 BindingSpec, which currently provides: * Subject - Subject references the resource(s) whose \u201cruntime contract\u201d should be augmented by Binding implementations. SinkBindingStatus ( Appears on: SinkBinding ) SinkBindingStatus communicates the observed state of the SinkBinding (from the controller). Field Description SourceStatus knative.dev/pkg/apis/duck/v1.SourceStatus (Members of SourceStatus are embedded into this type.) inherits duck/v1 SourceStatus, which currently provides: * ObservedGeneration - the \u2018Generation\u2019 of the Service that was last processed by the controller. * Conditions - the latest available observations of a resource\u2019s current state. * SinkURI - the current active sink URI that has been configured for the Source. sources.knative.dev/v1alpha1 Package v1alpha1 contains API Schema definitions for the sources v1alpha1 API group Resource Types: ApiServerSource SinkBinding ApiServerSource ApiServerSource is the Schema for the apiserversources API Field Description apiVersion string sources.knative.dev/v1alpha1 kind string ApiServerSource metadata Kubernetes meta/v1.ObjectMeta Refer to the Kubernetes API documentation for the fields of the metadata field. spec ApiServerSourceSpec resources []ApiServerResource Resources is the list of resources to watch serviceAccountName string (Optional) ServiceAccountName is the name of the ServiceAccount to use to run this source. sink knative.dev/pkg/apis/duck/v1beta1.Destination (Optional) Sink is a reference to an object that will resolve to a domain name to use as the sink. ceOverrides knative.dev/pkg/apis/duck/v1.CloudEventOverrides (Optional) CloudEventOverrides defines overrides to control the output format and modifications of the event sent to the sink. owner APIVersionKind (Optional) ResourceOwner is an additional filter to only track resources that are owned by a specific resource type. If ResourceOwner matches Resources[n] then Resources[n] is allowed to pass the ResourceOwner filter. mode string Mode is the mode the receive adapter controller runs under: Ref or Resource. Ref sends only the reference to the resource. Resource send the full resource. status ApiServerSourceStatus SinkBinding SinkBinding describes a Binding that is also a Source. The sink (from the Source duck) is resolved to a URL and then projected into the subject by augmenting the runtime contract of the referenced containers to have a K_SINK environment variable holding the endpoint to which to send cloud events. Field Description apiVersion string sources.knative.dev/v1alpha1 kind string SinkBinding metadata Kubernetes meta/v1.ObjectMeta Refer to the Kubernetes API documentation for the fields of the metadata field. spec SinkBindingSpec SourceSpec knative.dev/pkg/apis/duck/v1.SourceSpec (Members of SourceSpec are embedded into this type.) BindingSpec knative.dev/pkg/apis/duck/v1alpha1.BindingSpec (Members of BindingSpec are embedded into this type.) status SinkBindingStatus ApiServerResource ( Appears on: ApiServerSourceSpec ) ApiServerResource defines the resource to watch Field Description apiVersion string API version of the resource to watch. kind string Kind of the resource to watch. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds labelSelector Kubernetes meta/v1.LabelSelector LabelSelector restricts this source to objects with the selected labels More info: http://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#label-selectors controllerSelector Kubernetes meta/v1.OwnerReference ControllerSelector restricts this source to objects with a controlling owner reference of the specified kind. Only apiVersion and kind are used. Both are optional. Deprecated: Per-resource owner refs will no longer be supported in v1alpha2, please use Spec.Owner as a GKV. controller bool If true, send an event referencing the object controlling the resource Deprecated: Per-resource controller flag will no longer be supported in v1alpha2, please use Spec.Owner as a GKV. ApiServerSourceSpec ( Appears on: ApiServerSource ) ApiServerSourceSpec defines the desired state of ApiServerSource Field Description resources []ApiServerResource Resources is the list of resources to watch serviceAccountName string (Optional) ServiceAccountName is the name of the ServiceAccount to use to run this source. sink knative.dev/pkg/apis/duck/v1beta1.Destination (Optional) Sink is a reference to an object that will resolve to a domain name to use as the sink. ceOverrides knative.dev/pkg/apis/duck/v1.CloudEventOverrides (Optional) CloudEventOverrides defines overrides to control the output format and modifications of the event sent to the sink. owner APIVersionKind (Optional) ResourceOwner is an additional filter to only track resources that are owned by a specific resource type. If ResourceOwner matches Resources[n] then Resources[n] is allowed to pass the ResourceOwner filter. mode string Mode is the mode the receive adapter controller runs under: Ref or Resource. Ref sends only the reference to the resource. Resource send the full resource. ApiServerSourceStatus ( Appears on: ApiServerSource ) ApiServerSourceStatus defines the observed state of ApiServerSource Field Description SourceStatus knative.dev/pkg/apis/duck/v1.SourceStatus (Members of SourceStatus are embedded into this type.) inherits duck/v1 SourceStatus, which currently provides: * ObservedGeneration - the \u2018Generation\u2019 of the Service that was last processed by the controller. * Conditions - the latest available observations of a resource\u2019s current state. * SinkURI - the current active sink URI that has been configured for the Source. SinkBindingSpec ( Appears on: SinkBinding ) SinkBindingSpec holds the desired state of the SinkBinding (from the client). Field Description SourceSpec knative.dev/pkg/apis/duck/v1.SourceSpec (Members of SourceSpec are embedded into this type.) BindingSpec knative.dev/pkg/apis/duck/v1alpha1.BindingSpec (Members of BindingSpec are embedded into this type.) SinkBindingStatus ( Appears on: SinkBinding ) SinkBindingStatus communicates the observed state of the SinkBinding (from the controller). Field Description SourceStatus knative.dev/pkg/apis/duck/v1.SourceStatus (Members of SourceStatus are embedded into this type.) sources.knative.dev/v1alpha2 Package v1alpha2 contains API Schema definitions for the sources v1beta1 API group Resource Types: ApiServerSource ContainerSource PingSource SinkBinding ApiServerSource ApiServerSource is the Schema for the apiserversources API Field Description apiVersion string sources.knative.dev/v1alpha2 kind string ApiServerSource metadata Kubernetes meta/v1.ObjectMeta Refer to the Kubernetes API documentation for the fields of the metadata field. spec ApiServerSourceSpec SourceSpec knative.dev/pkg/apis/duck/v1.SourceSpec (Members of SourceSpec are embedded into this type.) inherits duck/v1 SourceSpec, which currently provides: * Sink - a reference to an object that will resolve to a domain name or a URI directly to use as the sink. * CloudEventOverrides - defines overrides to control the output format and modifications of the event sent to the sink. resources []APIVersionKindSelector Resource are the resources this source will track and send related lifecycle events from the Kubernetes ApiServer, with an optional label selector to help filter. owner APIVersionKind (Optional) ResourceOwner is an additional filter to only track resources that are owned by a specific resource type. If ResourceOwner matches Resources[n] then Resources[n] is allowed to pass the ResourceOwner filter. mode string (Optional) EventMode controls the format of the event. Reference sends a dataref event type for the resource under watch. Resource send the full resource lifecycle event. Defaults to Reference serviceAccountName string (Optional) ServiceAccountName is the name of the ServiceAccount to use to run this source. Defaults to default if not set. status ApiServerSourceStatus ContainerSource ContainerSource is the Schema for the containersources API Field Description apiVersion string sources.knative.dev/v1alpha2 kind string ContainerSource metadata Kubernetes meta/v1.ObjectMeta Refer to the Kubernetes API documentation for the fields of the metadata field. spec ContainerSourceSpec SourceSpec knative.dev/pkg/apis/duck/v1.SourceSpec (Members of SourceSpec are embedded into this type.) inherits duck/v1 SourceSpec, which currently provides: * Sink - a reference to an object that will resolve to a domain name or a URI directly to use as the sink. * CloudEventOverrides - defines overrides to control the output format and modifications of the event sent to the sink. template Kubernetes core/v1.PodTemplateSpec Template describes the pods that will be created status ContainerSourceStatus PingSource PingSource is the Schema for the PingSources API. Field Description apiVersion string sources.knative.dev/v1alpha2 kind string PingSource metadata Kubernetes meta/v1.ObjectMeta Refer to the Kubernetes API documentation for the fields of the metadata field. spec PingSourceSpec SourceSpec knative.dev/pkg/apis/duck/v1.SourceSpec (Members of SourceSpec are embedded into this type.) inherits duck/v1 SourceSpec, which currently provides: * Sink - a reference to an object that will resolve to a domain name or a URI directly to use as the sink. * CloudEventOverrides - defines overrides to control the output format and modifications of the event sent to the sink. schedule string (Optional) Schedule is the cronjob schedule. Defaults to * * * * * . jsonData string (Optional) JsonData is json encoded data used as the body of the event posted to the sink. Default is empty. If set, datacontenttype will also be set to \u201capplication/json\u201d. status PingSourceStatus SinkBinding SinkBinding describes a Binding that is also a Source. The sink (from the Source duck) is resolved to a URL and then projected into the subject by augmenting the runtime contract of the referenced containers to have a K_SINK environment variable holding the endpoint to which to send cloud events. Field Description apiVersion string sources.knative.dev/v1alpha2 kind string SinkBinding metadata Kubernetes meta/v1.ObjectMeta Refer to the Kubernetes API documentation for the fields of the metadata field. spec SinkBindingSpec SourceSpec knative.dev/pkg/apis/duck/v1.SourceSpec (Members of SourceSpec are embedded into this type.) inherits duck/v1 SourceSpec, which currently provides: * Sink - a reference to an object that will resolve to a domain name or a URI directly to use as the sink. * CloudEventOverrides - defines overrides to control the output format and modifications of the event sent to the sink. BindingSpec knative.dev/pkg/apis/duck/v1alpha1.BindingSpec (Members of BindingSpec are embedded into this type.) inherits duck/v1alpha1 BindingSpec, which currently provides: * Subject - Subject references the resource(s) whose \u201cruntime contract\u201d should be augmented by Binding implementations. status SinkBindingStatus APIVersionKind ( Appears on: ApiServerSourceSpec , ApiServerSourceSpec ) APIVersionKind is an APIVersion and Kind tuple. Field Description apiVersion string APIVersion - the API version of the resource to watch. kind string Kind of the resource to watch. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds APIVersionKindSelector ( Appears on: ApiServerSourceSpec ) APIVersionKindSelector is an APIVersion Kind tuple with a LabelSelector. Field Description apiVersion string APIVersion - the API version of the resource to watch. kind string Kind of the resource to watch. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds selector Kubernetes meta/v1.LabelSelector (Optional) LabelSelector filters this source to objects to those resources pass the label selector. More info: http://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#label-selectors ApiServerSourceSpec ( Appears on: ApiServerSource ) ApiServerSourceSpec defines the desired state of ApiServerSource Field Description SourceSpec knative.dev/pkg/apis/duck/v1.SourceSpec (Members of SourceSpec are embedded into this type.) inherits duck/v1 SourceSpec, which currently provides: * Sink - a reference to an object that will resolve to a domain name or a URI directly to use as the sink. * CloudEventOverrides - defines overrides to control the output format and modifications of the event sent to the sink. resources []APIVersionKindSelector Resource are the resources this source will track and send related lifecycle events from the Kubernetes ApiServer, with an optional label selector to help filter. owner APIVersionKind (Optional) ResourceOwner is an additional filter to only track resources that are owned by a specific resource type. If ResourceOwner matches Resources[n] then Resources[n] is allowed to pass the ResourceOwner filter. mode string (Optional) EventMode controls the format of the event. Reference sends a dataref event type for the resource under watch. Resource send the full resource lifecycle event. Defaults to Reference serviceAccountName string (Optional) ServiceAccountName is the name of the ServiceAccount to use to run this source. Defaults to default if not set. ApiServerSourceStatus ( Appears on: ApiServerSource ) ApiServerSourceStatus defines the observed state of ApiServerSource Field Description SourceStatus knative.dev/pkg/apis/duck/v1.SourceStatus (Members of SourceStatus are embedded into this type.) inherits duck/v1 SourceStatus, which currently provides: * ObservedGeneration - the \u2018Generation\u2019 of the Service that was last processed by the controller. * Conditions - the latest available observations of a resource\u2019s current state. * SinkURI - the current active sink URI that has been configured for the Source. ContainerSourceSpec ( Appears on: ContainerSource ) ContainerSourceSpec defines the desired state of ContainerSource Field Description SourceSpec knative.dev/pkg/apis/duck/v1.SourceSpec (Members of SourceSpec are embedded into this type.) inherits duck/v1 SourceSpec, which currently provides: * Sink - a reference to an object that will resolve to a domain name or a URI directly to use as the sink. * CloudEventOverrides - defines overrides to control the output format and modifications of the event sent to the sink. template Kubernetes core/v1.PodTemplateSpec Template describes the pods that will be created ContainerSourceStatus ( Appears on: ContainerSource ) ContainerSourceStatus defines the observed state of ContainerSource Field Description SourceStatus knative.dev/pkg/apis/duck/v1.SourceStatus (Members of SourceStatus are embedded into this type.) inherits duck/v1 SourceStatus, which currently provides: * ObservedGeneration - the \u2018Generation\u2019 of the Service that was last processed by the controller. * Conditions - the latest available observations of a resource\u2019s current state. * SinkURI - the current active sink URI that has been configured for the Source. PingSourceSpec ( Appears on: PingSource ) PingSourceSpec defines the desired state of the PingSource. Field Description SourceSpec knative.dev/pkg/apis/duck/v1.SourceSpec (Members of SourceSpec are embedded into this type.) inherits duck/v1 SourceSpec, which currently provides: * Sink - a reference to an object that will resolve to a domain name or a URI directly to use as the sink. * CloudEventOverrides - defines overrides to control the output format and modifications of the event sent to the sink. schedule string (Optional) Schedule is the cronjob schedule. Defaults to * * * * * . jsonData string (Optional) JsonData is json encoded data used as the body of the event posted to the sink. Default is empty. If set, datacontenttype will also be set to \u201capplication/json\u201d. PingSourceStatus ( Appears on: PingSource ) PingSourceStatus defines the observed state of PingSource. Field Description SourceStatus knative.dev/pkg/apis/duck/v1.SourceStatus (Members of SourceStatus are embedded into this type.) inherits duck/v1 SourceStatus, which currently provides: * ObservedGeneration - the \u2018Generation\u2019 of the Service that was last processed by the controller. * Conditions - the latest available observations of a resource\u2019s current state. * SinkURI - the current active sink URI that has been configured for the Source. SinkBindingSpec ( Appears on: SinkBinding ) SinkBindingSpec holds the desired state of the SinkBinding (from the client). Field Description SourceSpec knative.dev/pkg/apis/duck/v1.SourceSpec (Members of SourceSpec are embedded into this type.) inherits duck/v1 SourceSpec, which currently provides: * Sink - a reference to an object that will resolve to a domain name or a URI directly to use as the sink. * CloudEventOverrides - defines overrides to control the output format and modifications of the event sent to the sink. BindingSpec knative.dev/pkg/apis/duck/v1alpha1.BindingSpec (Members of BindingSpec are embedded into this type.) inherits duck/v1alpha1 BindingSpec, which currently provides: * Subject - Subject references the resource(s) whose \u201cruntime contract\u201d should be augmented by Binding implementations. SinkBindingStatus ( Appears on: SinkBinding ) SinkBindingStatus communicates the observed state of the SinkBinding (from the controller). Field Description SourceStatus knative.dev/pkg/apis/duck/v1.SourceStatus (Members of SourceStatus are embedded into this type.) inherits duck/v1 SourceStatus, which currently provides: * ObservedGeneration - the \u2018Generation\u2019 of the Service that was last processed by the controller. * Conditions - the latest available observations of a resource\u2019s current state. * SinkURI - the current active sink URI that has been configured for the Source. sources.knative.dev/v1beta1 Package v1beta1 contains API Schema definitions for the sources v1beta1 API group. Resource Types: ApiServerSource ContainerSource PingSource SinkBinding ApiServerSource ApiServerSource is the Schema for the apiserversources API Field Description apiVersion string sources.knative.dev/v1beta1 kind string ApiServerSource metadata Kubernetes meta/v1.ObjectMeta Refer to the Kubernetes API documentation for the fields of the metadata field. spec ApiServerSourceSpec SourceSpec knative.dev/pkg/apis/duck/v1.SourceSpec (Members of SourceSpec are embedded into this type.) inherits duck/v1 SourceSpec, which currently provides: * Sink - a reference to an object that will resolve to a domain name or a URI directly to use as the sink. * CloudEventOverrides - defines overrides to control the output format and modifications of the event sent to the sink. resources []APIVersionKindSelector Resource are the resources this source will track and send related lifecycle events from the Kubernetes ApiServer, with an optional label selector to help filter. owner APIVersionKind (Optional) ResourceOwner is an additional filter to only track resources that are owned by a specific resource type. If ResourceOwner matches Resources[n] then Resources[n] is allowed to pass the ResourceOwner filter. mode string (Optional) EventMode controls the format of the event. Reference sends a dataref event type for the resource under watch. Resource send the full resource lifecycle event. Defaults to Reference serviceAccountName string (Optional) ServiceAccountName is the name of the ServiceAccount to use to run this source. Defaults to default if not set. status ApiServerSourceStatus ContainerSource ContainerSource is the Schema for the containersources API Field Description apiVersion string sources.knative.dev/v1beta1 kind string ContainerSource metadata Kubernetes meta/v1.ObjectMeta Refer to the Kubernetes API documentation for the fields of the metadata field. spec ContainerSourceSpec SourceSpec knative.dev/pkg/apis/duck/v1.SourceSpec (Members of SourceSpec are embedded into this type.) inherits duck/v1 SourceSpec, which currently provides: * Sink - a reference to an object that will resolve to a domain name or a URI directly to use as the sink. * CloudEventOverrides - defines overrides to control the output format and modifications of the event sent to the sink. template Kubernetes core/v1.PodTemplateSpec Template describes the pods that will be created status ContainerSourceStatus PingSource PingSource is the Schema for the PingSources API. Field Description apiVersion string sources.knative.dev/v1beta1 kind string PingSource metadata Kubernetes meta/v1.ObjectMeta Refer to the Kubernetes API documentation for the fields of the metadata field. spec PingSourceSpec SourceSpec knative.dev/pkg/apis/duck/v1.SourceSpec (Members of SourceSpec are embedded into this type.) inherits duck/v1 SourceSpec, which currently provides: * Sink - a reference to an object that will resolve to a domain name or a URI directly to use as the sink. * CloudEventOverrides - defines overrides to control the output format and modifications of the event sent to the sink. schedule string (Optional) Schedule is the cronjob schedule. Defaults to * * * * * . timezone string Timezone modifies the actual time relative to the specified timezone. Defaults to the system time zone. More general information about time zones: https://www.iana.org/time-zones List of valid timezone values: https://en.wikipedia.org/wiki/List_of_tz_database_time_zones jsonData string (Optional) JsonData is json encoded data used as the body of the event posted to the sink. Default is empty. If set, datacontenttype will also be set to \u201capplication/json\u201d. status PingSourceStatus SinkBinding SinkBinding describes a Binding that is also a Source. The sink (from the Source duck) is resolved to a URL and then projected into the subject by augmenting the runtime contract of the referenced containers to have a K_SINK environment variable holding the endpoint to which to send cloud events. Field Description apiVersion string sources.knative.dev/v1beta1 kind string SinkBinding metadata Kubernetes meta/v1.ObjectMeta Refer to the Kubernetes API documentation for the fields of the metadata field. spec SinkBindingSpec SourceSpec knative.dev/pkg/apis/duck/v1.SourceSpec (Members of SourceSpec are embedded into this type.) inherits duck/v1 SourceSpec, which currently provides: * Sink - a reference to an object that will resolve to a domain name or a URI directly to use as the sink. * CloudEventOverrides - defines overrides to control the output format and modifications of the event sent to the sink. BindingSpec knative.dev/pkg/apis/duck/v1beta1.BindingSpec (Members of BindingSpec are embedded into this type.) inherits duck/v1beta1 BindingSpec, which currently provides: * Subject - Subject references the resource(s) whose \u201cruntime contract\u201d should be augmented by Binding implementations. status SinkBindingStatus APIVersionKind ( Appears on: ApiServerSourceSpec ) APIVersionKind is an APIVersion and Kind tuple. Field Description apiVersion string APIVersion - the API version of the resource to watch. kind string Kind of the resource to watch. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds APIVersionKindSelector ( Appears on: ApiServerSourceSpec ) APIVersionKindSelector is an APIVersion Kind tuple with a LabelSelector. Field Description apiVersion string APIVersion - the API version of the resource to watch. kind string Kind of the resource to watch. More info: https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds selector Kubernetes meta/v1.LabelSelector (Optional) LabelSelector filters this source to objects to those resources pass the label selector. More info: http://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#label-selectors ApiServerSourceSpec ( Appears on: ApiServerSource ) ApiServerSourceSpec defines the desired state of ApiServerSource Field Description SourceSpec knative.dev/pkg/apis/duck/v1.SourceSpec (Members of SourceSpec are embedded into this type.) inherits duck/v1 SourceSpec, which currently provides: * Sink - a reference to an object that will resolve to a domain name or a URI directly to use as the sink. * CloudEventOverrides - defines overrides to control the output format and modifications of the event sent to the sink. resources []APIVersionKindSelector Resource are the resources this source will track and send related lifecycle events from the Kubernetes ApiServer, with an optional label selector to help filter. owner APIVersionKind (Optional) ResourceOwner is an additional filter to only track resources that are owned by a specific resource type. If ResourceOwner matches Resources[n] then Resources[n] is allowed to pass the ResourceOwner filter. mode string (Optional) EventMode controls the format of the event. Reference sends a dataref event type for the resource under watch. Resource send the full resource lifecycle event. Defaults to Reference serviceAccountName string (Optional) ServiceAccountName is the name of the ServiceAccount to use to run this source. Defaults to default if not set. ApiServerSourceStatus ( Appears on: ApiServerSource ) ApiServerSourceStatus defines the observed state of ApiServerSource Field Description SourceStatus knative.dev/pkg/apis/duck/v1.SourceStatus (Members of SourceStatus are embedded into this type.) inherits duck/v1 SourceStatus, which currently provides: * ObservedGeneration - the \u2018Generation\u2019 of the Service that was last processed by the controller. * Conditions - the latest available observations of a resource\u2019s current state. * SinkURI - the current active sink URI that has been configured for the Source. ContainerSourceSpec ( Appears on: ContainerSource ) ContainerSourceSpec defines the desired state of ContainerSource Field Description SourceSpec knative.dev/pkg/apis/duck/v1.SourceSpec (Members of SourceSpec are embedded into this type.) inherits duck/v1 SourceSpec, which currently provides: * Sink - a reference to an object that will resolve to a domain name or a URI directly to use as the sink. * CloudEventOverrides - defines overrides to control the output format and modifications of the event sent to the sink. template Kubernetes core/v1.PodTemplateSpec Template describes the pods that will be created ContainerSourceStatus ( Appears on: ContainerSource ) ContainerSourceStatus defines the observed state of ContainerSource Field Description SourceStatus knative.dev/pkg/apis/duck/v1.SourceStatus (Members of SourceStatus are embedded into this type.) inherits duck/v1 SourceStatus, which currently provides: * ObservedGeneration - the \u2018Generation\u2019 of the Service that was last processed by the controller. * Conditions - the latest available observations of a resource\u2019s current state. * SinkURI - the current active sink URI that has been configured for the Source. PingSourceSpec ( Appears on: PingSource ) PingSourceSpec defines the desired state of the PingSource. Field Description SourceSpec knative.dev/pkg/apis/duck/v1.SourceSpec (Members of SourceSpec are embedded into this type.) inherits duck/v1 SourceSpec, which currently provides: * Sink - a reference to an object that will resolve to a domain name or a URI directly to use as the sink. * CloudEventOverrides - defines overrides to control the output format and modifications of the event sent to the sink. schedule string (Optional) Schedule is the cronjob schedule. Defaults to * * * * * . timezone string Timezone modifies the actual time relative to the specified timezone. Defaults to the system time zone. More general information about time zones: https://www.iana.org/time-zones List of valid timezone values: https://en.wikipedia.org/wiki/List_of_tz_database_time_zones jsonData string (Optional) JsonData is json encoded data used as the body of the event posted to the sink. Default is empty. If set, datacontenttype will also be set to \u201capplication/json\u201d. PingSourceStatus ( Appears on: PingSource ) PingSourceStatus defines the observed state of PingSource. Field Description SourceStatus knative.dev/pkg/apis/duck/v1.SourceStatus (Members of SourceStatus are embedded into this type.) inherits duck/v1 SourceStatus, which currently provides: * ObservedGeneration - the \u2018Generation\u2019 of the Service that was last processed by the controller. * Conditions - the latest available observations of a resource\u2019s current state. * SinkURI - the current active sink URI that has been configured for the Source. SinkBindingSpec ( Appears on: SinkBinding ) SinkBindingSpec holds the desired state of the SinkBinding (from the client). Field Description SourceSpec knative.dev/pkg/apis/duck/v1.SourceSpec (Members of SourceSpec are embedded into this type.) inherits duck/v1 SourceSpec, which currently provides: * Sink - a reference to an object that will resolve to a domain name or a URI directly to use as the sink. * CloudEventOverrides - defines overrides to control the output format and modifications of the event sent to the sink. BindingSpec knative.dev/pkg/apis/duck/v1beta1.BindingSpec (Members of BindingSpec are embedded into this type.) inherits duck/v1beta1 BindingSpec, which currently provides: * Subject - Subject references the resource(s) whose \u201cruntime contract\u201d should be augmented by Binding implementations. SinkBindingStatus ( Appears on: SinkBinding ) SinkBindingStatus communicates the observed state of the SinkBinding (from the controller). Field Description SourceStatus knative.dev/pkg/apis/duck/v1.SourceStatus (Members of SourceStatus are embedded into this type.) inherits duck/v1 SourceStatus, which currently provides: * ObservedGeneration - the \u2018Generation\u2019 of the Service that was last processed by the controller. * Conditions - the latest available observations of a resource\u2019s current state. * SinkURI - the current active sink URI that has been configured for the Source. sources.knative.dev/v1beta2 Package v1beta2 contains API Schema definitions for the sources v1beta2 API group. Resource Types: PingSource PingSource PingSource is the Schema for the PingSources API. Field Description apiVersion string sources.knative.dev/v1beta2 kind string PingSource metadata Kubernetes meta/v1.ObjectMeta Refer to the Kubernetes API documentation for the fields of the metadata field. spec PingSourceSpec SourceSpec knative.dev/pkg/apis/duck/v1.SourceSpec (Members of SourceSpec are embedded into this type.) inherits duck/v1 SourceSpec, which currently provides: * Sink - a reference to an object that will resolve to a domain name or a URI directly to use as the sink. * CloudEventOverrides - defines overrides to control the output format and modifications of the event sent to the sink. schedule string (Optional) Schedule is the cron schedule. Defaults to * * * * * . timezone string Timezone modifies the actual time relative to the specified timezone. Defaults to the system time zone. More general information about time zones: https://www.iana.org/time-zones List of valid timezone values: https://en.wikipedia.org/wiki/List_of_tz_database_time_zones contentType string (Optional) ContentType is the media type of Data or DataBase64. Default is empty. data string (Optional) Data is data used as the body of the event posted to the sink. Default is empty. Mutually exclusive with DataBase64. dataBase64 string (Optional) DataBase64 is the base64-encoded string of the actual event\u2019s body posted to the sink. Default is empty. Mutually exclusive with Data. status PingSourceStatus PingSourceSpec ( Appears on: PingSource ) PingSourceSpec defines the desired state of the PingSource. Field Description SourceSpec knative.dev/pkg/apis/duck/v1.SourceSpec (Members of SourceSpec are embedded into this type.) inherits duck/v1 SourceSpec, which currently provides: * Sink - a reference to an object that will resolve to a domain name or a URI directly to use as the sink. * CloudEventOverrides - defines overrides to control the output format and modifications of the event sent to the sink. schedule string (Optional) Schedule is the cron schedule. Defaults to * * * * * . timezone string Timezone modifies the actual time relative to the specified timezone. Defaults to the system time zone. More general information about time zones: https://www.iana.org/time-zones List of valid timezone values: https://en.wikipedia.org/wiki/List_of_tz_database_time_zones contentType string (Optional) ContentType is the media type of Data or DataBase64. Default is empty. data string (Optional) Data is data used as the body of the event posted to the sink. Default is empty. Mutually exclusive with DataBase64. dataBase64 string (Optional) DataBase64 is the base64-encoded string of the actual event\u2019s body posted to the sink. Default is empty. Mutually exclusive with Data. PingSourceStatus ( Appears on: PingSource ) PingSourceStatus defines the observed state of PingSource. Field Description SourceStatus knative.dev/pkg/apis/duck/v1.SourceStatus (Members of SourceStatus are embedded into this type.) inherits duck/v1 SourceStatus, which currently provides: * ObservedGeneration - the \u2018Generation\u2019 of the Service that was last processed by the controller. * Conditions - the latest available observations of a resource\u2019s current state. * SinkURI - the current active sink URI that has been configured for the Source. Generated with gen-crd-api-reference-docs on git commit 8f35d4254 .","title":"Eventing"},{"location":"reference/relnotes/_index/","text":"Knative Release Notes \u00b6 For details about the Knative releases, see the following pages: Knative Serving releases Knative Serving Operator releases Knative CLI releases Knative Eventing releases","title":"Knative Release Notes"},{"location":"reference/relnotes/_index/#knative-release-notes","text":"For details about the Knative releases, see the following pages: Knative Serving releases Knative Serving Operator releases Knative CLI releases Knative Eventing releases","title":"Knative Release Notes"},{"location":"serving/","text":"Knative Serving \u00b6 Knative Serving builds on Kubernetes and Istio to support deploying and serving of serverless applications and functions. Serving is easy to get started with and scales to support advanced scenarios. The Knative Serving project provides middleware primitives that enable: Rapid deployment of serverless containers Automatic scaling up and down to zero Routing and network programming for Istio components Point-in-time snapshots of deployed code and configurations Serving resources \u00b6 Knative Serving defines a set of objects as Kubernetes Custom Resource Definitions (CRDs). These objects are used to define and control how your serverless workload behaves on the cluster: Service : The service.serving.knative.dev resource automatically manages the whole lifecycle of your workload. It controls the creation of other objects to ensure that your app has a route, a configuration, and a new revision for each update of the service. Service can be defined to always route traffic to the latest revision or to a pinned revision. Route : The route.serving.knative.dev resource maps a network endpoint to one or more revisions. You can manage the traffic in several ways, including fractional traffic and named routes. Configuration : The configuration.serving.knative.dev resource maintains the desired state for your deployment. It provides a clean separation between code and configuration and follows the Twelve-Factor App methodology. Modifying a configuration creates a new revision. Revision : The revision.serving.knative.dev resource is a point-in-time snapshot of the code and configuration for each modification made to the workload. Revisions are immutable objects and can be retained for as long as useful. Knative Serving Revisions can be automatically scaled up and down according to incoming traffic. See Configuring the Autoscaler for more information. Getting Started \u00b6 To get started with Serving, check out one of the hello world sample projects. These projects use the Service resource, which manages all of the details for you. With the Service resource, a deployed service will automatically have a matching route and configuration created. Each time the Service is updated, a new revision is created. For more information on the resources and their interactions, see the Resource Types Overview in the Knative Serving repository. More samples and demos \u00b6 Knative Serving code samples Debugging Knative Serving issues \u00b6 Debugging Application Issues Configuration and Networking \u00b6 Configuring cluster local routes Using a custom domain Assigning a static IP address for Knative on Google Kubernetes Engine Using subroutes Known Issues \u00b6 See the Knative Serving Issues page for a full list of known issues.","title":"Overview"},{"location":"serving/#knative-serving","text":"Knative Serving builds on Kubernetes and Istio to support deploying and serving of serverless applications and functions. Serving is easy to get started with and scales to support advanced scenarios. The Knative Serving project provides middleware primitives that enable: Rapid deployment of serverless containers Automatic scaling up and down to zero Routing and network programming for Istio components Point-in-time snapshots of deployed code and configurations","title":"Knative Serving"},{"location":"serving/#serving-resources","text":"Knative Serving defines a set of objects as Kubernetes Custom Resource Definitions (CRDs). These objects are used to define and control how your serverless workload behaves on the cluster: Service : The service.serving.knative.dev resource automatically manages the whole lifecycle of your workload. It controls the creation of other objects to ensure that your app has a route, a configuration, and a new revision for each update of the service. Service can be defined to always route traffic to the latest revision or to a pinned revision. Route : The route.serving.knative.dev resource maps a network endpoint to one or more revisions. You can manage the traffic in several ways, including fractional traffic and named routes. Configuration : The configuration.serving.knative.dev resource maintains the desired state for your deployment. It provides a clean separation between code and configuration and follows the Twelve-Factor App methodology. Modifying a configuration creates a new revision. Revision : The revision.serving.knative.dev resource is a point-in-time snapshot of the code and configuration for each modification made to the workload. Revisions are immutable objects and can be retained for as long as useful. Knative Serving Revisions can be automatically scaled up and down according to incoming traffic. See Configuring the Autoscaler for more information.","title":"Serving resources"},{"location":"serving/#getting-started","text":"To get started with Serving, check out one of the hello world sample projects. These projects use the Service resource, which manages all of the details for you. With the Service resource, a deployed service will automatically have a matching route and configuration created. Each time the Service is updated, a new revision is created. For more information on the resources and their interactions, see the Resource Types Overview in the Knative Serving repository.","title":"Getting Started"},{"location":"serving/#more-samples-and-demos","text":"Knative Serving code samples","title":"More samples and demos"},{"location":"serving/#debugging-knative-serving-issues","text":"Debugging Application Issues","title":"Debugging Knative Serving issues"},{"location":"serving/#configuration-and-networking","text":"Configuring cluster local routes Using a custom domain Assigning a static IP address for Knative on Google Kubernetes Engine Using subroutes","title":"Configuration and Networking"},{"location":"serving/#known-issues","text":"See the Knative Serving Issues page for a full list of known issues.","title":"Known Issues"},{"location":"serving/accessing-traces/","text":"Accessing request traces \u00b6 Depending on the request tracing tool that you have installed on your Knative Serving cluster, see the corresponding section for details about how to visualize and trace your requests. Configuring Traces \u00b6 You can update the configuration file for tracing in config-tracing.yaml . Follow the instructions in the file to set your configuration options. This file includes options such as sample rate (to determine what percentage of requests to trace), debug mode, and backend selection (zipkin or stackdriver). You can quickly explore and update the ConfigMap object with the following command: kubectl -n knative-serving edit configmap config-tracing Zipkin \u00b6 In order to access request traces, you use the Zipkin visualization tool. To open the Zipkin UI, enter the following command: kubectl proxy This command starts a local proxy of Zipkin on port 8001. For security reasons, the Zipkin UI is exposed only within the cluster. Navigate to the Zipkin UI . Click \"Find Traces\" to see the latest traces. You can search for a trace ID or look at traces of a specific application. Click on a trace to see a detailed view of a specific call. Jaeger \u00b6 In order to access request traces, you use the Jaeger visualization tool. To open the Jaeger UI, enter the following command: kubectl proxy This command starts a local proxy of Jaeger on port 8001. For security reasons, the Jaeger UI is exposed only within the cluster. Navigate to the Jaeger UI . Select the service of interest and click \"Find Traces\" to see the latest traces. Click on a trace to see a detailed view of a specific call.","title":"Accessing Request Traces"},{"location":"serving/accessing-traces/#accessing-request-traces","text":"Depending on the request tracing tool that you have installed on your Knative Serving cluster, see the corresponding section for details about how to visualize and trace your requests.","title":"Accessing request traces"},{"location":"serving/accessing-traces/#configuring-traces","text":"You can update the configuration file for tracing in config-tracing.yaml . Follow the instructions in the file to set your configuration options. This file includes options such as sample rate (to determine what percentage of requests to trace), debug mode, and backend selection (zipkin or stackdriver). You can quickly explore and update the ConfigMap object with the following command: kubectl -n knative-serving edit configmap config-tracing","title":"Configuring Traces"},{"location":"serving/accessing-traces/#zipkin","text":"In order to access request traces, you use the Zipkin visualization tool. To open the Zipkin UI, enter the following command: kubectl proxy This command starts a local proxy of Zipkin on port 8001. For security reasons, the Zipkin UI is exposed only within the cluster. Navigate to the Zipkin UI . Click \"Find Traces\" to see the latest traces. You can search for a trace ID or look at traces of a specific application. Click on a trace to see a detailed view of a specific call.","title":"Zipkin"},{"location":"serving/accessing-traces/#jaeger","text":"In order to access request traces, you use the Jaeger visualization tool. To open the Jaeger UI, enter the following command: kubectl proxy This command starts a local proxy of Jaeger on port 8001. For security reasons, the Jaeger UI is exposed only within the cluster. Navigate to the Jaeger UI . Select the service of interest and click \"Find Traces\" to see the latest traces. Click on a trace to see a detailed view of a specific call.","title":"Jaeger"},{"location":"serving/cluster-local-route/","text":"Creating a private cluster-local service \u00b6 By default services deployed through Knative are published to an external IP address, making them public services on a public IP address and with a public URL . While this is useful for services that need to be accessible from outside of the cluster, frequently you may be building a backend service which should not be available off-cluster. Knative provides two ways to enable private services which are only available inside the cluster: To make all services only cluster-local, change the default domain to svc.cluster.local by editing the config-domain config map . This will change all services deployed through Knative to only be published to the cluster, none will be available off-cluster. To make an individual service cluster-local, the service or route can be labeled in such a way to prevent it from getting published to the external gateway. Label a service to be cluster-local \u00b6 To configure a Knative service to only be available on the cluster-local network (and not on the public Internet), you can apply the networking.knative.dev/visibility=cluster-local label to the Knative service, route or Kubernetes service object. To label the Knative service: kubectl label kservice ${ KSVC_NAME } networking.knative.dev/visibility = cluster-local To label a route when the route is used directly without a Knative service: kubectl label route ${ ROUTE_NAME } networking.knative.dev/visibility = cluster-local To label a Kubernetes service: kubectl label service ${ SERVICE_NAME } networking.knative.dev/visibility = cluster-local By labeling the Kubernetes service it allows you to restrict visibility in a more fine-grained way. See subroutes for information about tagged routes. For example, you can deploy the Hello World sample and then convert it to be an cluster-local service by labeling the service: kubectl label kservice helloworld-go networking.knative.dev/visibility = cluster-local You can then verify that the change has been made by verifying the URL for the helloworld-go service: kubectl get kservice helloworld-go NAME URL LATESTCREATED LATESTREADY READY REASON helloworld-go http://helloworld-go.default.svc.cluster.local helloworld-go-2bz5l helloworld-go-2bz5l True The service returns the a URL with the svc.cluster.local domain, indicating the service is only available in the cluster local network.","title":"Configuing Cluster-Local Services"},{"location":"serving/cluster-local-route/#creating-a-private-cluster-local-service","text":"By default services deployed through Knative are published to an external IP address, making them public services on a public IP address and with a public URL . While this is useful for services that need to be accessible from outside of the cluster, frequently you may be building a backend service which should not be available off-cluster. Knative provides two ways to enable private services which are only available inside the cluster: To make all services only cluster-local, change the default domain to svc.cluster.local by editing the config-domain config map . This will change all services deployed through Knative to only be published to the cluster, none will be available off-cluster. To make an individual service cluster-local, the service or route can be labeled in such a way to prevent it from getting published to the external gateway.","title":"Creating a private cluster-local service"},{"location":"serving/cluster-local-route/#label-a-service-to-be-cluster-local","text":"To configure a Knative service to only be available on the cluster-local network (and not on the public Internet), you can apply the networking.knative.dev/visibility=cluster-local label to the Knative service, route or Kubernetes service object. To label the Knative service: kubectl label kservice ${ KSVC_NAME } networking.knative.dev/visibility = cluster-local To label a route when the route is used directly without a Knative service: kubectl label route ${ ROUTE_NAME } networking.knative.dev/visibility = cluster-local To label a Kubernetes service: kubectl label service ${ SERVICE_NAME } networking.knative.dev/visibility = cluster-local By labeling the Kubernetes service it allows you to restrict visibility in a more fine-grained way. See subroutes for information about tagged routes. For example, you can deploy the Hello World sample and then convert it to be an cluster-local service by labeling the service: kubectl label kservice helloworld-go networking.knative.dev/visibility = cluster-local You can then verify that the change has been made by verifying the URL for the helloworld-go service: kubectl get kservice helloworld-go NAME URL LATESTCREATED LATESTREADY READY REASON helloworld-go http://helloworld-go.default.svc.cluster.local helloworld-go-2bz5l helloworld-go-2bz5l True The service returns the a URL with the svc.cluster.local domain, indicating the service is only available in the cluster local network.","title":"Label a service to be cluster-local"},{"location":"serving/config-ha/","text":"Configuring high-availability components \u00b6 Active/passive high availability (HA) is a standard feature of Kubernetes APIs that helps to ensure that APIs stay operational if a disruption occurs. In an HA deployment, if an active controller crashes or is deleted, another controller is available to take over processing of the APIs that were being serviced by the controller that is now unavailable. When using a leader election HA pattern, instances of controllers are already scheduled and running inside the cluster before they are required. These controller instances compete to use a shared resource, known as the leader election lock. The instance of the controller that has access to the leader election lock resource at any given time is referred to as the leader. Leader election is enabled by default for all Knative Serving components. HA functionality is disabled by default for all Knative Serving components, which are configured with only one replica. Disabling leader election \u00b6 For components leveraging leader election to achieve HA, this capability can be disabled by passing the flag: --disable-ha . This option will go away when HA graduates to \"stable\". Scaling the control plane \u00b6 With the exception of the activator component you can scale up any deployment running in knative-serving (or kourier-system ) with a command like: $ kubectl -n knative-serving scale deployment <deployment-name> --replicas=2 Setting --replicas to a value of 2 enables HA. You can use a higher value if you have a use case that requires more replicas of a deployment. For example, if you require a minimum of 3 controller deployments, set --replicas=3 . Setting --replicas=1 disables HA. NOTE: If you scale down the autoscaler component, you may observe inaccurate autoscaling results for some revisions for a period of time up to the stable-window value. This is because when an autoscaler pod is terminating, ownership of the revisions belonging to that pod is passed to other autoscaler pods that are on stand by. The autoscaler pods that take over ownership of those revisions use the stable-window time to build the scaling metrics state for those revisions. Scaling the data plane \u00b6 The scale of the activator component is governed by the Kubernetes HPA component. You can see the current HPA scale limits and the current scale by running: $ kubectl get hpa activator -n knative-serving The possible output will be something like: NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE activator Deployment/activator 2%/100% 5 15 11 346d By default minReplicas and maxReplicas are set to 1 and 20 , correspondingly. If those values are not desirable for some reason, then, for example, you can change those values to minScale=9 and maxScale=19 using the following command: $ kubectl patch hpa activator -n knative-serving -p '{\"spec\":{\"minReplicas\":9,\"maxReplicas\":19}}' To set the activator scale to a particular value, just set minScale and maxScale to the same desired value. It is recommended for production deployments to run at least 3 activator instances for redundancy and avoiding single point of failure if a Knative service needs to be scaled from 0.","title":"Configuring high-availability components"},{"location":"serving/config-ha/#configuring-high-availability-components","text":"Active/passive high availability (HA) is a standard feature of Kubernetes APIs that helps to ensure that APIs stay operational if a disruption occurs. In an HA deployment, if an active controller crashes or is deleted, another controller is available to take over processing of the APIs that were being serviced by the controller that is now unavailable. When using a leader election HA pattern, instances of controllers are already scheduled and running inside the cluster before they are required. These controller instances compete to use a shared resource, known as the leader election lock. The instance of the controller that has access to the leader election lock resource at any given time is referred to as the leader. Leader election is enabled by default for all Knative Serving components. HA functionality is disabled by default for all Knative Serving components, which are configured with only one replica.","title":"Configuring high-availability components"},{"location":"serving/config-ha/#disabling-leader-election","text":"For components leveraging leader election to achieve HA, this capability can be disabled by passing the flag: --disable-ha . This option will go away when HA graduates to \"stable\".","title":"Disabling leader election"},{"location":"serving/config-ha/#scaling-the-control-plane","text":"With the exception of the activator component you can scale up any deployment running in knative-serving (or kourier-system ) with a command like: $ kubectl -n knative-serving scale deployment <deployment-name> --replicas=2 Setting --replicas to a value of 2 enables HA. You can use a higher value if you have a use case that requires more replicas of a deployment. For example, if you require a minimum of 3 controller deployments, set --replicas=3 . Setting --replicas=1 disables HA. NOTE: If you scale down the autoscaler component, you may observe inaccurate autoscaling results for some revisions for a period of time up to the stable-window value. This is because when an autoscaler pod is terminating, ownership of the revisions belonging to that pod is passed to other autoscaler pods that are on stand by. The autoscaler pods that take over ownership of those revisions use the stable-window time to build the scaling metrics state for those revisions.","title":"Scaling the control plane"},{"location":"serving/config-ha/#scaling-the-data-plane","text":"The scale of the activator component is governed by the Kubernetes HPA component. You can see the current HPA scale limits and the current scale by running: $ kubectl get hpa activator -n knative-serving The possible output will be something like: NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE activator Deployment/activator 2%/100% 5 15 11 346d By default minReplicas and maxReplicas are set to 1 and 20 , correspondingly. If those values are not desirable for some reason, then, for example, you can change those values to minScale=9 and maxScale=19 using the following command: $ kubectl patch hpa activator -n knative-serving -p '{\"spec\":{\"minReplicas\":9,\"maxReplicas\":19}}' To set the activator scale to a particular value, just set minScale and maxScale to the same desired value. It is recommended for production deployments to run at least 3 activator instances for redundancy and avoiding single point of failure if a Knative service needs to be scaled from 0.","title":"Scaling the data plane"},{"location":"serving/creating-domain-mappings/","text":"Creating a Mapping between a Custom Domain Name and a Knative Service (Alpha) \u00b6 Knative Services are automatically given a default domain name based on the cluster configuration, e.g. \"mysvc.mynamespace.mydomain\". You can also map a single custom domain name that you own to a specific Knative Service using the Domain Mapping feature, if enabled. For example, if you own the \"example.org\" domain name, and configure its DNS to reference your Knative cluster, you can use the DomainMapping feature to have this domain be served by a Knative Service. Before you begin \u00b6 You need to enable the DomainMapping feature (and a supported Knative Ingress implementation) to use it. See Install optional Serving extensions . To map a custom domain to a Knative Service, you must first create a Knative Service . You will need a Domain Name to map, and the ability to change its DNS to point to your Knative Cluster. The details of this step are dependant on your domain registrar. Creating a Domain Mapping \u00b6 To create a mapping from a custom domain name that you control to a Knative Service, you need to create a YAML file that defines a Domain Mapping. This YAML file specifies the domain name to map and the Knative Service to use to service requests. You will also need to point the domain name at your Knative cluster using the tools provided by your domain registrar. Domain Mappings map a single, non-wildcard domain to a specific Knative Service. For example in the example yaml below, the \"example.org\" Domain Mapping maps only \"example.org\" and not \"www.example.org\". You can create multiple Domain Mappings to map multiple domains and subdomains. Procedure \u00b6 Create a new file named domainmapping.yaml containing the following information. apiVersion : serving.knative.dev/v1alpha1 kind : DomainMapping metadata : name : example.org namespace : default spec : ref : name : helloworld-go kind : Service apiVersion : serving.knative.dev/v1 name (metadata): The domain name you wish to map to the Knative Service. namespace : The namespace that both the DomainMapping and Knative Service use. name (ref): The Knative Service which should be used to service requests for the custom domain name. You can also map to other targets as long as they conform to the Addressable contract and their resolved URL is of the form {name}.{namespace}.{clusterdomain} where {name} and {namespace} are the name and namespace of a Kubernetes service, and {clusterdomain} is the cluster domain. Objects conforming to this contract include Knative Services and Routes, and Kubernetes Services. From the directory where the new domainmapping.yaml file was created, deploy the domain mapping by applying the domainmapping.yaml file. kubectl apply --filename domainmapping.yaml You will also need to point the \"example.org\" domain name at the IP address of your Knative cluster. Details of this step differ depending on your domain registrar.","title":"Creating Domain Mappings"},{"location":"serving/creating-domain-mappings/#creating-a-mapping-between-a-custom-domain-name-and-a-knative-service-alpha","text":"Knative Services are automatically given a default domain name based on the cluster configuration, e.g. \"mysvc.mynamespace.mydomain\". You can also map a single custom domain name that you own to a specific Knative Service using the Domain Mapping feature, if enabled. For example, if you own the \"example.org\" domain name, and configure its DNS to reference your Knative cluster, you can use the DomainMapping feature to have this domain be served by a Knative Service.","title":"Creating a Mapping between a Custom Domain Name and a Knative Service (Alpha)"},{"location":"serving/creating-domain-mappings/#before-you-begin","text":"You need to enable the DomainMapping feature (and a supported Knative Ingress implementation) to use it. See Install optional Serving extensions . To map a custom domain to a Knative Service, you must first create a Knative Service . You will need a Domain Name to map, and the ability to change its DNS to point to your Knative Cluster. The details of this step are dependant on your domain registrar.","title":"Before you begin"},{"location":"serving/creating-domain-mappings/#creating-a-domain-mapping","text":"To create a mapping from a custom domain name that you control to a Knative Service, you need to create a YAML file that defines a Domain Mapping. This YAML file specifies the domain name to map and the Knative Service to use to service requests. You will also need to point the domain name at your Knative cluster using the tools provided by your domain registrar. Domain Mappings map a single, non-wildcard domain to a specific Knative Service. For example in the example yaml below, the \"example.org\" Domain Mapping maps only \"example.org\" and not \"www.example.org\". You can create multiple Domain Mappings to map multiple domains and subdomains.","title":"Creating a Domain Mapping"},{"location":"serving/creating-domain-mappings/#procedure","text":"Create a new file named domainmapping.yaml containing the following information. apiVersion : serving.knative.dev/v1alpha1 kind : DomainMapping metadata : name : example.org namespace : default spec : ref : name : helloworld-go kind : Service apiVersion : serving.knative.dev/v1 name (metadata): The domain name you wish to map to the Knative Service. namespace : The namespace that both the DomainMapping and Knative Service use. name (ref): The Knative Service which should be used to service requests for the custom domain name. You can also map to other targets as long as they conform to the Addressable contract and their resolved URL is of the form {name}.{namespace}.{clusterdomain} where {name} and {namespace} are the name and namespace of a Kubernetes service, and {clusterdomain} is the cluster domain. Objects conforming to this contract include Knative Services and Routes, and Kubernetes Services. From the directory where the new domainmapping.yaml file was created, deploy the domain mapping by applying the domainmapping.yaml file. kubectl apply --filename domainmapping.yaml You will also need to point the \"example.org\" domain name at the IP address of your Knative cluster. Details of this step differ depending on your domain registrar.","title":"Procedure"},{"location":"serving/debugging-application-issues/","text":"Debugging issues with your application \u00b6 You deployed your app to Knative Serving, but it isn't working as expected. Go through this step-by-step guide to understand what failed. Check command-line output \u00b6 Check your deploy command output to see whether it succeeded or not. If your deployment process was terminated, you should see an error message in the output that describes the reason why the deployment failed. This kind of failure is most likely due to either a misconfigured manifest or wrong command. For example, the following output says that you must configure route traffic percent to sum to 100: Error from server (InternalError): error when applying patch: {\"metadata\":{\"annotations\":{\"kubectl.kubernetes.io/last-applied-configuration\":\"{\\\"apiVersion\\\":\\\"serving.knative.dev/v1\\\",\\\"kind\\\":\\\"Route\\\",\\\"metadata\\\":{\\\"annotations\\\":{},\\\"name\\\":\\\"route-example\\\",\\\"namespace\\\":\\\"default\\\"},\\\"spec\\\":{\\\"traffic\\\":[{\\\"configurationName\\\":\\\"configuration-example\\\",\\\"percent\\\":50}]}}\\n\"}},\"spec\":{\"traffic\":[{\"configurationName\":\"configuration-example\",\"percent\":50}]}} to: &{0xc421d98240 0xc421e77490 default route-example STDIN 0xc421db0488 264682 false} for: \"STDIN\": Internal error occurred: admission webhook \"webhook.knative.dev\" denied the request: mutation failed: The route must have traffic percent sum equal to 100. ERROR: Non-zero return code '1' from command: Process exited with status 1 Check Route status \u00b6 Run the following command to get the status of the Route object with which you deployed your application: kubectl get route <route-name> --output yaml The conditions in status provide the reason if there is any failure. For details, see Knative Error Conditions and Reporting . Check Ingress/Istio routing \u00b6 To list all Ingress resources and their corresponding labels, run the following command: kubectl get ingresses.networking.internal.knative.dev -o = custom-columns = 'NAME:.metadata.name,LABELS:.metadata.labels' NAME LABELS helloworld-go map [ serving.knative.dev/route:helloworld-go serving.knative.dev/routeNamespace:default serving.knative.dev/service:helloworld-go ] The labels serving.knative.dev/route and serving.knative.dev/routeNamespace indicate the Route in which the Ingress resource resides. Your Route and Ingress should be listed. If your Ingress does not exist, the route controller believes that the Revisions targeted by your Route/Service isn't ready. Please proceed to later sections to diagnose Revision readiness status. Otherwise, run the following command to look at the ClusterIngress created for your Route kubectl get ingresses.networking.internal.knative.dev <INGRESS_NAME> --output yaml particularly, look at the status: section. If the Ingress is working correctly, we should see the condition with type=Ready to have status=True . Otherwise, there will be error messages. Now, if Ingress shows status Ready , there must be a corresponding VirtualService. Run the following command: kubectl get virtualservice -l networking.internal.knative.dev/ingress = <INGRESS_NAME> -n <INGRESS_NAMESPACE> --output yaml the network configuration in VirtualService must match that of Ingress and Route. VirtualService currently doesn't expose a Status field, so if one exists and have matching configurations with Ingress and Route, you may want to wait a little bit for those settings to propagate. If you are familar with Istio and istioctl , you may try using istioctl to look deeper using Istio guide . Check Ingress status \u00b6 Knative uses a LoadBalancer service called istio-ingressgateway Service. To check the IP address of your Ingress, use kubectl get svc -n istio-system istio-ingressgateway If there is no external IP address, use kubectl describe svc istio-ingressgateway -n istio-system to see a reason why IP addresses weren't provisioned. Most likely it is due to a quota issue. Check Revision status \u00b6 If you configure your Route with Configuration , run the following command to get the name of the Revision created for you deployment (look up the configuration name in the Route .yaml file): kubectl get configuration <configuration-name> --output jsonpath = \"{.status.latestCreatedRevisionName}\" If you configure your Route with Revision directly, look up the revision name in the Route yaml file. Then run the following command: kubectl get revision <revision-name> --output yaml A ready Revision should have the following condition in status : conditions : - reason : ServiceReady status : \"True\" type : Ready If you see this condition, check the following to continue debugging: Check Pod status Check Istio routing If you see other conditions, look up the meaning of the conditions in Knative Error Conditions and Reporting . Note: some of them are not implemented yet. An alternative is to check Pod status . Check Pod status \u00b6 To get the Pod s for all your deployments: kubectl get pods This command should list all Pod s with brief status. For example: NAME READY STATUS RESTARTS AGE configuration-example-00001-deployment-659747ff99-9bvr4 2/2 Running 0 3h configuration-example-00002-deployment-5f475b7849-gxcht 1/2 CrashLoopBackOff 2 36s Choose one and use the following command to see detailed information for its status . Some useful fields are conditions and containerStatuses : kubectl get pod <pod-name> --output yaml","title":"Debugging Issues with your Application"},{"location":"serving/debugging-application-issues/#debugging-issues-with-your-application","text":"You deployed your app to Knative Serving, but it isn't working as expected. Go through this step-by-step guide to understand what failed.","title":"Debugging issues with your application"},{"location":"serving/debugging-application-issues/#check-command-line-output","text":"Check your deploy command output to see whether it succeeded or not. If your deployment process was terminated, you should see an error message in the output that describes the reason why the deployment failed. This kind of failure is most likely due to either a misconfigured manifest or wrong command. For example, the following output says that you must configure route traffic percent to sum to 100: Error from server (InternalError): error when applying patch: {\"metadata\":{\"annotations\":{\"kubectl.kubernetes.io/last-applied-configuration\":\"{\\\"apiVersion\\\":\\\"serving.knative.dev/v1\\\",\\\"kind\\\":\\\"Route\\\",\\\"metadata\\\":{\\\"annotations\\\":{},\\\"name\\\":\\\"route-example\\\",\\\"namespace\\\":\\\"default\\\"},\\\"spec\\\":{\\\"traffic\\\":[{\\\"configurationName\\\":\\\"configuration-example\\\",\\\"percent\\\":50}]}}\\n\"}},\"spec\":{\"traffic\":[{\"configurationName\":\"configuration-example\",\"percent\":50}]}} to: &{0xc421d98240 0xc421e77490 default route-example STDIN 0xc421db0488 264682 false} for: \"STDIN\": Internal error occurred: admission webhook \"webhook.knative.dev\" denied the request: mutation failed: The route must have traffic percent sum equal to 100. ERROR: Non-zero return code '1' from command: Process exited with status 1","title":"Check command-line output"},{"location":"serving/debugging-application-issues/#check-route-status","text":"Run the following command to get the status of the Route object with which you deployed your application: kubectl get route <route-name> --output yaml The conditions in status provide the reason if there is any failure. For details, see Knative Error Conditions and Reporting .","title":"Check Route status"},{"location":"serving/debugging-application-issues/#check-ingressistio-routing","text":"To list all Ingress resources and their corresponding labels, run the following command: kubectl get ingresses.networking.internal.knative.dev -o = custom-columns = 'NAME:.metadata.name,LABELS:.metadata.labels' NAME LABELS helloworld-go map [ serving.knative.dev/route:helloworld-go serving.knative.dev/routeNamespace:default serving.knative.dev/service:helloworld-go ] The labels serving.knative.dev/route and serving.knative.dev/routeNamespace indicate the Route in which the Ingress resource resides. Your Route and Ingress should be listed. If your Ingress does not exist, the route controller believes that the Revisions targeted by your Route/Service isn't ready. Please proceed to later sections to diagnose Revision readiness status. Otherwise, run the following command to look at the ClusterIngress created for your Route kubectl get ingresses.networking.internal.knative.dev <INGRESS_NAME> --output yaml particularly, look at the status: section. If the Ingress is working correctly, we should see the condition with type=Ready to have status=True . Otherwise, there will be error messages. Now, if Ingress shows status Ready , there must be a corresponding VirtualService. Run the following command: kubectl get virtualservice -l networking.internal.knative.dev/ingress = <INGRESS_NAME> -n <INGRESS_NAMESPACE> --output yaml the network configuration in VirtualService must match that of Ingress and Route. VirtualService currently doesn't expose a Status field, so if one exists and have matching configurations with Ingress and Route, you may want to wait a little bit for those settings to propagate. If you are familar with Istio and istioctl , you may try using istioctl to look deeper using Istio guide .","title":"Check Ingress/Istio routing"},{"location":"serving/debugging-application-issues/#check-ingress-status","text":"Knative uses a LoadBalancer service called istio-ingressgateway Service. To check the IP address of your Ingress, use kubectl get svc -n istio-system istio-ingressgateway If there is no external IP address, use kubectl describe svc istio-ingressgateway -n istio-system to see a reason why IP addresses weren't provisioned. Most likely it is due to a quota issue.","title":"Check Ingress status"},{"location":"serving/debugging-application-issues/#check-revision-status","text":"If you configure your Route with Configuration , run the following command to get the name of the Revision created for you deployment (look up the configuration name in the Route .yaml file): kubectl get configuration <configuration-name> --output jsonpath = \"{.status.latestCreatedRevisionName}\" If you configure your Route with Revision directly, look up the revision name in the Route yaml file. Then run the following command: kubectl get revision <revision-name> --output yaml A ready Revision should have the following condition in status : conditions : - reason : ServiceReady status : \"True\" type : Ready If you see this condition, check the following to continue debugging: Check Pod status Check Istio routing If you see other conditions, look up the meaning of the conditions in Knative Error Conditions and Reporting . Note: some of them are not implemented yet. An alternative is to check Pod status .","title":"Check Revision status"},{"location":"serving/debugging-application-issues/#check-pod-status","text":"To get the Pod s for all your deployments: kubectl get pods This command should list all Pod s with brief status. For example: NAME READY STATUS RESTARTS AGE configuration-example-00001-deployment-659747ff99-9bvr4 2/2 Running 0 3h configuration-example-00002-deployment-5f475b7849-gxcht 1/2 CrashLoopBackOff 2 36s Choose one and use the following command to see detailed information for its status . Some useful fields are conditions and containerStatuses : kubectl get pod <pod-name> --output yaml","title":"Check Pod status"},{"location":"serving/feature-flags/","text":"Feature Flags \u00b6 Knative is deliberate about the concepts it incorporates into its core API. The API aims to be portable and abstracts away the specificities of each users' implementation. That being said, the Knative API should empower users to surface extra features and extensions possible within their platform of choice. This document introduces two concepts: * Feature: a way to stage the introduction of features to the Knative API. * Extension: a way to extend Knative beyond the portable concepts of the Knative API. Control \u00b6 Features and extensions are controlled by flags defined in the config-features ConfigMap in the knative-serving namespace. Flags can have the following values: Enabled: the feature is enabled. Allowed: the feature may be enabled (e.g. using an annotation or looser validation). Disabled: the feature cannot be enabled. These three states don't make sense for all features. Let's consider two types of features: multi-container and kubernetes.podspec-dryrun . multi-container allows the user to specify more than one container in the Knative Service spec. In this case, Enabled and Allowed are equivalent because using this feature requires to actually use it in the Knative Service spec. If a single container is specified, whether the feature is enabled or not doesn't change anything. kubernetes.podspec-dryrun changes the behavior of the Kubernetes implementation of the Knative API, but it has nothing to do with the Knative API itself. In this case, Enabled means the feature will be enabled unconditionally, Allowed means that the feature will be enabled only when specified with an annotation, and Disabled means that the feature cannot be used at all. Lifecyle \u00b6 Features and extensions go through 3 similar phases (Alpha, Beta, GA) but with important differences. Alpha means: Might be buggy. Enabling the feature may expose bugs. Support for feature may be dropped at any time without notice. The API may change in incompatible ways in a later software release without notice. Recommended for use only in short-lived testing clusters, due to increased risk of bugs and lack of long-term support. Beta means: The feature is well tested. Enabling the feature is considered safe. Support for the overall feature will not be dropped, though details may change. The schema and/or semantics of objects may change in incompatible ways in a subsequent beta or stable release. When this happens, we will provide instructions for migrating to the next version. This may require deleting, editing, or re-creating API objects. The editing process may require some thought. This may require downtime for applications that rely on the feature. Recommended for only non-business-critical uses because of potential for incompatible changes in subsequent releases. If you have multiple clusters that can be upgraded independently, you may be able to relax this restriction. General Availability (GA) means: Stable versions of features/extensions will appear in released software for many subsequent versions. Feature \u00b6 Features use flags to safely introduce new changes to the Knative API. Eventually, each feature will graduate to become fully part of the Knative API, and the flag guard will be removed. Alpha \u00b6 Disabled by default. Beta \u00b6 Enabled by default. GA \u00b6 The feature is always enabled; you cannot disable it. The corresponding feature flag is no longer needed. Extension \u00b6 An extension may surface details of a specific Knative implementation or features of the underlying environment. It is never intended for inclusion in the core Knative API due to its lack of portability. Each extension will always be controlled by a flag and never enabled by default. Alpha \u00b6 Disabled by default. Beta \u00b6 Allowed by default. GA \u00b6 Allowed by default. Available Flags \u00b6 Multi Containers \u00b6 Type : feature ConfigMap key: multi-container This flag allows specifying multiple \"user containers\" in a Knative Service spec. Only one container can handle the requests, and therefore exactly one container must have a port specified. apiVersion : serving.knative.dev/v1 kind : Service ... spec : template : spec : containers : - name : first-container image : gcr.io/knative-samples/helloworld-go ports : - containerPort : 8080 - name : second-container image : gcr.io/knative-samples/helloworld-java Kubernetes Node Affinity \u00b6 Type : extension ConfigMap key: kubernetes.podspec-affinity This extension controls whether node affinity can be specified. apiVersion : serving.knative.dev/v1 kind : Service ... spec : template : spec : affinity : nodeAffinity : requiredDuringSchedulingIgnoredDuringExecution : nodeSelectorTerms : - matchExpressions : - key : kubernetes.io/e2e-az-name operator : In values : - e2e-az1 - e2e-az2 Kubernetes Host Aliases \u00b6 Type : extension ConfigMap key: kubernetes.podspec-hostaliases This flag controls whether host aliases can be specified. apiVersion : serving.knative.dev/v1 kind : Service ... spec : template : spec : hostAliases : - ip : \"127.0.0.1\" hostnames : - \"foo.local\" - \"bar.local\" Kubernetes Node Selector \u00b6 Type : extension ConfigMap key: kubernetes.podspec-nodeselector This flag controls whether node selector can be specified. apiVersion : serving.knative.dev/v1 kind : Service ... spec : template : spec : nodeSelector : labelName : labelValue Kubernetes Toleration \u00b6 Type : extension ConfigMap key: kubernetes.podspec-tolerations This flag controls whether tolerations can be specified. apiVersion : serving.knative.dev/v1 kind : Service ... spec : template : spec : tolerations : - key : \"example-key\" operator : \"Exists\" effect : \"NoSchedule\" Kubernetes FieldRef \u00b6 Type : extension ConfigMap key: kubernetes.podspec-fieldref This flag controls whether the Downward API (env based) can be specified. apiVersion : serving.knative.dev/v1 kind : Service ... spec : template : spec : containers : - name : user-container image : gcr.io/knative-samples/helloworld-go env : - name : MY_NODE_NAME valueFrom : fieldRef : fieldPath : spec.nodeName Kubernetes Dry Run \u00b6 Type : extension ConfigMap key: kubernetes.podspec-dryrun This flag controls whether Knative will try to validate the Pod spec derived from the Knative Service spec using the Kubernetes API server before accepting the object. When \"enabled\", the server will always run the extra validation. When \"allowed\", the server will not run the dry-run validation by default. However, clients may enable the behavior on an individual Service by attaching the following metadata annotation: \"features.knative.dev/podspec-dryrun\":\"enabled\". apiVersion : serving.knative.dev/v1 kind : Service metadata : annotations : features.knative.dev/podspec-dryrun\":\"enabled ... spec : template : spec : ... Kubernetes Runtime Class \u00b6 Type : extension ConfigMap key: kubernetes.podspec-runtimeclass This flag controls whether the runtime class can be used or not. apiVersion : serving.knative.dev/v1 kind : Service ... spec : template : spec : runtimeClassName : myclass ... Kubernetes Security Context \u00b6 Type : extension ConfigMap key: kubernetes.podspec-securitycontext This flag controls whether a subset of the security context can be used. When set to \"enabled\" or \"allowed\" it allows the following PodSecurityContext properties: - FSGroup - RunAsGroup - RunAsNonRoot - SupplementalGroups - RunAsUser When set to \"enabled\" or \"allowed\" it allows the following Container SecurityContext properties: - RunAsNonRoot - RunAsGroup - RunAsUser (already allowed without this flag) This flag should be used with caution as the PodSecurityContext properties may have a side-effect on non-user sidecar containers that come from Knative or your service mesh apiVersion : serving.knative.dev/v1 kind : Service ... spec : template : spec : securityContext : runAsUser : 1000 ... Responsive Revision Garbage Collector \u00b6 Type : extension ConfigMap key: responsive-revision-gc This flag controls whether new responsive garbage collection is enabled. This feature labels revisions in real-time as they become referenced and dereferenced by Routes. This allows us to reap revisions shortly after they are no longer active. Tag Header Based Routing \u00b6 Type : extension ConfigMap key: tag-header-based-routing This flags controls whether tag header based routing is enabled.","title":"Feature Flags"},{"location":"serving/feature-flags/#feature-flags","text":"Knative is deliberate about the concepts it incorporates into its core API. The API aims to be portable and abstracts away the specificities of each users' implementation. That being said, the Knative API should empower users to surface extra features and extensions possible within their platform of choice. This document introduces two concepts: * Feature: a way to stage the introduction of features to the Knative API. * Extension: a way to extend Knative beyond the portable concepts of the Knative API.","title":"Feature Flags"},{"location":"serving/feature-flags/#control","text":"Features and extensions are controlled by flags defined in the config-features ConfigMap in the knative-serving namespace. Flags can have the following values: Enabled: the feature is enabled. Allowed: the feature may be enabled (e.g. using an annotation or looser validation). Disabled: the feature cannot be enabled. These three states don't make sense for all features. Let's consider two types of features: multi-container and kubernetes.podspec-dryrun . multi-container allows the user to specify more than one container in the Knative Service spec. In this case, Enabled and Allowed are equivalent because using this feature requires to actually use it in the Knative Service spec. If a single container is specified, whether the feature is enabled or not doesn't change anything. kubernetes.podspec-dryrun changes the behavior of the Kubernetes implementation of the Knative API, but it has nothing to do with the Knative API itself. In this case, Enabled means the feature will be enabled unconditionally, Allowed means that the feature will be enabled only when specified with an annotation, and Disabled means that the feature cannot be used at all.","title":"Control"},{"location":"serving/feature-flags/#lifecyle","text":"Features and extensions go through 3 similar phases (Alpha, Beta, GA) but with important differences. Alpha means: Might be buggy. Enabling the feature may expose bugs. Support for feature may be dropped at any time without notice. The API may change in incompatible ways in a later software release without notice. Recommended for use only in short-lived testing clusters, due to increased risk of bugs and lack of long-term support. Beta means: The feature is well tested. Enabling the feature is considered safe. Support for the overall feature will not be dropped, though details may change. The schema and/or semantics of objects may change in incompatible ways in a subsequent beta or stable release. When this happens, we will provide instructions for migrating to the next version. This may require deleting, editing, or re-creating API objects. The editing process may require some thought. This may require downtime for applications that rely on the feature. Recommended for only non-business-critical uses because of potential for incompatible changes in subsequent releases. If you have multiple clusters that can be upgraded independently, you may be able to relax this restriction. General Availability (GA) means: Stable versions of features/extensions will appear in released software for many subsequent versions.","title":"Lifecyle"},{"location":"serving/feature-flags/#feature","text":"Features use flags to safely introduce new changes to the Knative API. Eventually, each feature will graduate to become fully part of the Knative API, and the flag guard will be removed.","title":"Feature"},{"location":"serving/feature-flags/#alpha","text":"Disabled by default.","title":"Alpha"},{"location":"serving/feature-flags/#beta","text":"Enabled by default.","title":"Beta"},{"location":"serving/feature-flags/#ga","text":"The feature is always enabled; you cannot disable it. The corresponding feature flag is no longer needed.","title":"GA"},{"location":"serving/feature-flags/#extension","text":"An extension may surface details of a specific Knative implementation or features of the underlying environment. It is never intended for inclusion in the core Knative API due to its lack of portability. Each extension will always be controlled by a flag and never enabled by default.","title":"Extension"},{"location":"serving/feature-flags/#alpha_1","text":"Disabled by default.","title":"Alpha"},{"location":"serving/feature-flags/#beta_1","text":"Allowed by default.","title":"Beta"},{"location":"serving/feature-flags/#ga_1","text":"Allowed by default.","title":"GA"},{"location":"serving/feature-flags/#available-flags","text":"","title":"Available Flags"},{"location":"serving/feature-flags/#multi-containers","text":"Type : feature ConfigMap key: multi-container This flag allows specifying multiple \"user containers\" in a Knative Service spec. Only one container can handle the requests, and therefore exactly one container must have a port specified. apiVersion : serving.knative.dev/v1 kind : Service ... spec : template : spec : containers : - name : first-container image : gcr.io/knative-samples/helloworld-go ports : - containerPort : 8080 - name : second-container image : gcr.io/knative-samples/helloworld-java","title":"Multi Containers"},{"location":"serving/feature-flags/#kubernetes-node-affinity","text":"Type : extension ConfigMap key: kubernetes.podspec-affinity This extension controls whether node affinity can be specified. apiVersion : serving.knative.dev/v1 kind : Service ... spec : template : spec : affinity : nodeAffinity : requiredDuringSchedulingIgnoredDuringExecution : nodeSelectorTerms : - matchExpressions : - key : kubernetes.io/e2e-az-name operator : In values : - e2e-az1 - e2e-az2","title":"Kubernetes Node Affinity"},{"location":"serving/feature-flags/#kubernetes-host-aliases","text":"Type : extension ConfigMap key: kubernetes.podspec-hostaliases This flag controls whether host aliases can be specified. apiVersion : serving.knative.dev/v1 kind : Service ... spec : template : spec : hostAliases : - ip : \"127.0.0.1\" hostnames : - \"foo.local\" - \"bar.local\"","title":"Kubernetes Host Aliases"},{"location":"serving/feature-flags/#kubernetes-node-selector","text":"Type : extension ConfigMap key: kubernetes.podspec-nodeselector This flag controls whether node selector can be specified. apiVersion : serving.knative.dev/v1 kind : Service ... spec : template : spec : nodeSelector : labelName : labelValue","title":"Kubernetes Node Selector"},{"location":"serving/feature-flags/#kubernetes-toleration","text":"Type : extension ConfigMap key: kubernetes.podspec-tolerations This flag controls whether tolerations can be specified. apiVersion : serving.knative.dev/v1 kind : Service ... spec : template : spec : tolerations : - key : \"example-key\" operator : \"Exists\" effect : \"NoSchedule\"","title":"Kubernetes Toleration"},{"location":"serving/feature-flags/#kubernetes-fieldref","text":"Type : extension ConfigMap key: kubernetes.podspec-fieldref This flag controls whether the Downward API (env based) can be specified. apiVersion : serving.knative.dev/v1 kind : Service ... spec : template : spec : containers : - name : user-container image : gcr.io/knative-samples/helloworld-go env : - name : MY_NODE_NAME valueFrom : fieldRef : fieldPath : spec.nodeName","title":"Kubernetes FieldRef"},{"location":"serving/feature-flags/#kubernetes-dry-run","text":"Type : extension ConfigMap key: kubernetes.podspec-dryrun This flag controls whether Knative will try to validate the Pod spec derived from the Knative Service spec using the Kubernetes API server before accepting the object. When \"enabled\", the server will always run the extra validation. When \"allowed\", the server will not run the dry-run validation by default. However, clients may enable the behavior on an individual Service by attaching the following metadata annotation: \"features.knative.dev/podspec-dryrun\":\"enabled\". apiVersion : serving.knative.dev/v1 kind : Service metadata : annotations : features.knative.dev/podspec-dryrun\":\"enabled ... spec : template : spec : ...","title":"Kubernetes Dry Run"},{"location":"serving/feature-flags/#kubernetes-runtime-class","text":"Type : extension ConfigMap key: kubernetes.podspec-runtimeclass This flag controls whether the runtime class can be used or not. apiVersion : serving.knative.dev/v1 kind : Service ... spec : template : spec : runtimeClassName : myclass ...","title":"Kubernetes Runtime Class"},{"location":"serving/feature-flags/#kubernetes-security-context","text":"Type : extension ConfigMap key: kubernetes.podspec-securitycontext This flag controls whether a subset of the security context can be used. When set to \"enabled\" or \"allowed\" it allows the following PodSecurityContext properties: - FSGroup - RunAsGroup - RunAsNonRoot - SupplementalGroups - RunAsUser When set to \"enabled\" or \"allowed\" it allows the following Container SecurityContext properties: - RunAsNonRoot - RunAsGroup - RunAsUser (already allowed without this flag) This flag should be used with caution as the PodSecurityContext properties may have a side-effect on non-user sidecar containers that come from Knative or your service mesh apiVersion : serving.knative.dev/v1 kind : Service ... spec : template : spec : securityContext : runAsUser : 1000 ...","title":"Kubernetes Security Context"},{"location":"serving/feature-flags/#responsive-revision-garbage-collector","text":"Type : extension ConfigMap key: responsive-revision-gc This flag controls whether new responsive garbage collection is enabled. This feature labels revisions in real-time as they become referenced and dereferenced by Routes. This allows us to reap revisions shortly after they are no longer active.","title":"Responsive Revision Garbage Collector"},{"location":"serving/feature-flags/#tag-header-based-routing","text":"Type : extension ConfigMap key: tag-header-based-routing This flags controls whether tag header based routing is enabled.","title":"Tag Header Based Routing"},{"location":"serving/gke-assigning-static-ip-address/","text":"Assigning a static IP address for Knative on Kubernetes Engine \u00b6 If you are running Knative on Google Kubernetes Engine and want to use a custom domain with your apps, you need to configure a static IP address to ensure that your custom domain mapping doesn't break. Knative configures an Istio Gateway CRD named knative-ingress-gateway under the knative-serving namespace to serve all incoming traffic within the Knative service mesh. The IP address to access the gateway is the external IP address of the \"istio-ingressgateway\" service under the istio-system namespace. Therefore, in order to set a static IP for the gateway you must to set the external IP address of the istio-ingressgateway service to a static IP. If you have configured a custom ingress gateway , replace istio-ingressgateway with the name of your gateway service in the steps below. Step 1: Reserve a static IP address \u00b6 You can reserve a regional static IP address using the Google Cloud SDK or the Google Cloud Platform console. Using the Google Cloud SDK: Enter the following command, replacing IP_NAME and REGION with appropriate values. For example, select the us-west1 region if you deployed your cluster to the us-west1-c zone: gcloud beta compute addresses create IP_NAME --region = REGION For example: gcloud beta compute addresses create knative-ip --region = us-west1 Enter the following command to get the newly created static IP address: gcloud beta compute addresses list In the GCP console : Enter a name for your static address. For IP version , choose IPv4. For Type , choose Regional . From the Region drop-down, choose the region where your Knative cluster is running. For example, select the us-west1 region if you deployed your cluster to the us-west1-c zone. Leave the Attached To field set to None since we'll attach the IP address through a config-map later. Copy the External Address of the static IP you created. Step 2: Update the external IP of istio-ingressgateway service \u00b6 Run following command to configure the external IP of the istio-ingressgateway service to the static IP that you reserved: INGRESSGATEWAY = istio-ingressgateway kubectl patch svc $INGRESSGATEWAY --namespace istio-system --patch '{\"spec\": { \"loadBalancerIP\": \"<your-reserved-static-ip>\" }}' Step 3: Verify the static IP address of istio-ingressgateway service \u00b6 Run the following command to ensure that the external IP of the ingressgateway service has been updated: kubectl get svc $INGRESSGATEWAY --namespace istio-system The output should show the assigned static IP address under the EXTERNAL-IP column: NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE xxxxxxx-ingressgateway LoadBalancer 12.34.567.890 98.765.43.210 80:32380/TCP,443:32390/TCP,32400:32400/TCP 5m Note: Updating the external IP address can take several minutes. The external IP address should have a value now in the In use by column and should not be None anymore:","title":"Assigning Static IPs in GKE"},{"location":"serving/gke-assigning-static-ip-address/#assigning-a-static-ip-address-for-knative-on-kubernetes-engine","text":"If you are running Knative on Google Kubernetes Engine and want to use a custom domain with your apps, you need to configure a static IP address to ensure that your custom domain mapping doesn't break. Knative configures an Istio Gateway CRD named knative-ingress-gateway under the knative-serving namespace to serve all incoming traffic within the Knative service mesh. The IP address to access the gateway is the external IP address of the \"istio-ingressgateway\" service under the istio-system namespace. Therefore, in order to set a static IP for the gateway you must to set the external IP address of the istio-ingressgateway service to a static IP. If you have configured a custom ingress gateway , replace istio-ingressgateway with the name of your gateway service in the steps below.","title":"Assigning a static IP address for Knative on Kubernetes Engine"},{"location":"serving/gke-assigning-static-ip-address/#step-1-reserve-a-static-ip-address","text":"You can reserve a regional static IP address using the Google Cloud SDK or the Google Cloud Platform console. Using the Google Cloud SDK: Enter the following command, replacing IP_NAME and REGION with appropriate values. For example, select the us-west1 region if you deployed your cluster to the us-west1-c zone: gcloud beta compute addresses create IP_NAME --region = REGION For example: gcloud beta compute addresses create knative-ip --region = us-west1 Enter the following command to get the newly created static IP address: gcloud beta compute addresses list In the GCP console : Enter a name for your static address. For IP version , choose IPv4. For Type , choose Regional . From the Region drop-down, choose the region where your Knative cluster is running. For example, select the us-west1 region if you deployed your cluster to the us-west1-c zone. Leave the Attached To field set to None since we'll attach the IP address through a config-map later. Copy the External Address of the static IP you created.","title":"Step 1: Reserve a static IP address"},{"location":"serving/gke-assigning-static-ip-address/#step-2-update-the-external-ip-of-istio-ingressgateway-service","text":"Run following command to configure the external IP of the istio-ingressgateway service to the static IP that you reserved: INGRESSGATEWAY = istio-ingressgateway kubectl patch svc $INGRESSGATEWAY --namespace istio-system --patch '{\"spec\": { \"loadBalancerIP\": \"<your-reserved-static-ip>\" }}'","title":"Step 2: Update the external IP of istio-ingressgateway service"},{"location":"serving/gke-assigning-static-ip-address/#step-3-verify-the-static-ip-address-of-istio-ingressgateway-service","text":"Run the following command to ensure that the external IP of the ingressgateway service has been updated: kubectl get svc $INGRESSGATEWAY --namespace istio-system The output should show the assigned static IP address under the EXTERNAL-IP column: NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE xxxxxxx-ingressgateway LoadBalancer 12.34.567.890 98.765.43.210 80:32380/TCP,443:32390/TCP,32400:32400/TCP 5m Note: Updating the external IP address can take several minutes. The external IP address should have a value now in the In use by column and should not be None anymore:","title":"Step 3: Verify the static IP address of istio-ingressgateway service"},{"location":"serving/installing-cert-manager/","text":"Installing cert-manager for TLS certificates \u00b6 Install the Cert-Manager tool to obtain TLS certificates that you can use for secure HTTPS connections in Knative. For more information about enabling HTTPS connections in Knative, see Configuring HTTPS with TLS certificates . You can use cert-manager to either manually obtain certificates, or to enable Knative for automatic certificate provisioning. Complete instructions about automatic certificate provisioning are provided in Enabling automatic TLS cert provisioning . Regardless of if your want to manually obtain certificates, or configure Knative for automatic provisioning, you can use the following steps to install cert-manager. Before you begin \u00b6 You must meet the following requirements to install cert-manager for Knative: Knative Serving must be installed. For details about installing the Serving component, see the Knative installation guides . You must configure your Knative cluster to use a custom domain . Knative currently supports cert-manager version 1.0.0 and higher. Downloading and installing cert-manager \u00b6 Follow the steps from the official cert-manager website to download and install cert-manager Installation steps Completing the Knative configuration for TLS support \u00b6 Before you can use a TLS certificate for secure connections, you must finish configuring Knative: Manual : If you installed cert-manager to manually obtain certificates, continue to the following topic for instructions about creating a Kubernetes secret: Manually adding a TLS certificate Automatic : If you installed cert-manager to use for automatic certificate provisioning, continue to the following topic to enable that feature: Enabling automatic TLS certificate provisioning in Knative","title":"Installing cert-manager for TLS Certificates"},{"location":"serving/installing-cert-manager/#installing-cert-manager-for-tls-certificates","text":"Install the Cert-Manager tool to obtain TLS certificates that you can use for secure HTTPS connections in Knative. For more information about enabling HTTPS connections in Knative, see Configuring HTTPS with TLS certificates . You can use cert-manager to either manually obtain certificates, or to enable Knative for automatic certificate provisioning. Complete instructions about automatic certificate provisioning are provided in Enabling automatic TLS cert provisioning . Regardless of if your want to manually obtain certificates, or configure Knative for automatic provisioning, you can use the following steps to install cert-manager.","title":"Installing cert-manager for TLS certificates"},{"location":"serving/installing-cert-manager/#before-you-begin","text":"You must meet the following requirements to install cert-manager for Knative: Knative Serving must be installed. For details about installing the Serving component, see the Knative installation guides . You must configure your Knative cluster to use a custom domain . Knative currently supports cert-manager version 1.0.0 and higher.","title":"Before you begin"},{"location":"serving/installing-cert-manager/#downloading-and-installing-cert-manager","text":"Follow the steps from the official cert-manager website to download and install cert-manager Installation steps","title":"Downloading and installing cert-manager"},{"location":"serving/installing-cert-manager/#completing-the-knative-configuration-for-tls-support","text":"Before you can use a TLS certificate for secure connections, you must finish configuring Knative: Manual : If you installed cert-manager to manually obtain certificates, continue to the following topic for instructions about creating a Kubernetes secret: Manually adding a TLS certificate Automatic : If you installed cert-manager to use for automatic certificate provisioning, continue to the following topic to enable that feature: Enabling automatic TLS certificate provisioning in Knative","title":"Completing the Knative configuration for TLS support"},{"location":"serving/istio-authorization/","text":"Enabling requests to Knative services when additional authorization policies are enabled \u00b6 Knative Serving system pods, such as the activator and autoscaler components, require access to your deployed Knative services. If you have configured additional security features, such as Istio's authorization policy, you must enable access to your Knative service for these system pods. Before you begin \u00b6 You must meet the following prerequisites to use Istio AuthorizationPolicy: Istio must be used for your Knative Ingress. See Install a networking layer . Istio sidecar injection must be enabled. See the Istio Documentation . Mutual TLS in Knative \u00b6 Because Knative requests are frequently routed through activator, some considerations need to be made when using mutual TLS. Generally, mutual TLS can be configured normally as in Istio's documentation . However, since the activator can be in the request path of Knative services, it must have sidecars injected. The simplest way to do this is to label the knative-serving namespace: kubectl label namespace knative-serving istio-injection=enabled If the activator isn't injected: In PERMISSIVE mode, you'll see requests appear without the expected X-Forwarded-Client-Cert header when forwarded by the activator. $ kubectl exec deployment/httpbin -c httpbin -it -- curl -s http://httpbin.knative.svc.cluster.local/headers { \"headers\": { \"Accept\": \"*/*\", \"Accept-Encoding\": \"gzip\", \"Forwarded\": \"for=10.72.0.30;proto=http\", \"Host\": \"httpbin.knative.svc.cluster.local\", \"K-Proxy-Request\": \"activator\", \"User-Agent\": \"curl/7.58.0\", \"X-B3-Parentspanid\": \"b240bdb1c29ae638\", \"X-B3-Sampled\": \"0\", \"X-B3-Spanid\": \"416960c27be6d484\", \"X-B3-Traceid\": \"750362ce9d878281b240bdb1c29ae638\", \"X-Envoy-Attempt-Count\": \"1\", \"X-Envoy-Internal\": \"true\" } } In STRICT mode, requests will simply be rejected. To understand when requests are forwarded through the activator, see documentation on the TargetBurstCapacity setting. This also means that many Istio AuthorizationPolicies won't work as expected. For example, if you set up a rule allowing requests from a particular source into a Knative service, you will see requests being rejected if they are forwarded by the activator. For example, the following policy allows requests from within pods in the serving-tests namespace to other pods in the serving-tests namespace. apiVersion: security.istio.io/v1beta1 kind: AuthorizationPolicy metadata: name: allow-serving-tests namespace: serving-tests spec: action: ALLOW rules: - from: - source: namespaces: [\"serving-tests\"] Requests here will fail when forwarded by the activator, because the Istio proxy at the destination service will see the source namespace of the requests as knative-serving , which is the namespace of the activator. Currently, the easiest way around this is to explicitly allow requests from the knative-serving namespace, for example by adding it to the list in the above policy: apiVersion: security.istio.io/v1beta1 kind: AuthorizationPolicy metadata: name: allow-serving-tests namespace: serving-tests spec: action: ALLOW rules: - from: - source: namespaces: [\"serving-tests\", \"knative-serving\"] Health checking and metrics collection \u00b6 In addition to allowing your application path, you'll need to configure Istio AuthorizationPolicy to allow health checking and metrics collection to your applications from system pods. You can allow access from system pods by paths . Allowing access from system pods by paths \u00b6 Knative system pods access your application using the following paths: /metrics /healthz The /metrics path allows the autoscaler pod to collect metrics. The /healthz path allows system pods to probe the service. You can add the /metrics and /healthz paths to the AuthorizationPolicy as shown in the example: $ cat <<EOF | kubectl apply -f - apiVersion: security.istio.io/v1beta1 kind: AuthorizationPolicy metadata: name: allowlist-by-paths namespace: serving-tests spec: action: ALLOW rules: - to: - operation: paths: - /metrics # The path to collect metrics by system pod. - /healthz # The path to probe by system pod. EOF","title":"Enabling requests to Knative services when additional authorization policies are enabled"},{"location":"serving/istio-authorization/#enabling-requests-to-knative-services-when-additional-authorization-policies-are-enabled","text":"Knative Serving system pods, such as the activator and autoscaler components, require access to your deployed Knative services. If you have configured additional security features, such as Istio's authorization policy, you must enable access to your Knative service for these system pods.","title":"Enabling requests to Knative services when additional authorization policies are enabled"},{"location":"serving/istio-authorization/#before-you-begin","text":"You must meet the following prerequisites to use Istio AuthorizationPolicy: Istio must be used for your Knative Ingress. See Install a networking layer . Istio sidecar injection must be enabled. See the Istio Documentation .","title":"Before you begin"},{"location":"serving/istio-authorization/#mutual-tls-in-knative","text":"Because Knative requests are frequently routed through activator, some considerations need to be made when using mutual TLS. Generally, mutual TLS can be configured normally as in Istio's documentation . However, since the activator can be in the request path of Knative services, it must have sidecars injected. The simplest way to do this is to label the knative-serving namespace: kubectl label namespace knative-serving istio-injection=enabled If the activator isn't injected: In PERMISSIVE mode, you'll see requests appear without the expected X-Forwarded-Client-Cert header when forwarded by the activator. $ kubectl exec deployment/httpbin -c httpbin -it -- curl -s http://httpbin.knative.svc.cluster.local/headers { \"headers\": { \"Accept\": \"*/*\", \"Accept-Encoding\": \"gzip\", \"Forwarded\": \"for=10.72.0.30;proto=http\", \"Host\": \"httpbin.knative.svc.cluster.local\", \"K-Proxy-Request\": \"activator\", \"User-Agent\": \"curl/7.58.0\", \"X-B3-Parentspanid\": \"b240bdb1c29ae638\", \"X-B3-Sampled\": \"0\", \"X-B3-Spanid\": \"416960c27be6d484\", \"X-B3-Traceid\": \"750362ce9d878281b240bdb1c29ae638\", \"X-Envoy-Attempt-Count\": \"1\", \"X-Envoy-Internal\": \"true\" } } In STRICT mode, requests will simply be rejected. To understand when requests are forwarded through the activator, see documentation on the TargetBurstCapacity setting. This also means that many Istio AuthorizationPolicies won't work as expected. For example, if you set up a rule allowing requests from a particular source into a Knative service, you will see requests being rejected if they are forwarded by the activator. For example, the following policy allows requests from within pods in the serving-tests namespace to other pods in the serving-tests namespace. apiVersion: security.istio.io/v1beta1 kind: AuthorizationPolicy metadata: name: allow-serving-tests namespace: serving-tests spec: action: ALLOW rules: - from: - source: namespaces: [\"serving-tests\"] Requests here will fail when forwarded by the activator, because the Istio proxy at the destination service will see the source namespace of the requests as knative-serving , which is the namespace of the activator. Currently, the easiest way around this is to explicitly allow requests from the knative-serving namespace, for example by adding it to the list in the above policy: apiVersion: security.istio.io/v1beta1 kind: AuthorizationPolicy metadata: name: allow-serving-tests namespace: serving-tests spec: action: ALLOW rules: - from: - source: namespaces: [\"serving-tests\", \"knative-serving\"]","title":"Mutual TLS in Knative"},{"location":"serving/istio-authorization/#health-checking-and-metrics-collection","text":"In addition to allowing your application path, you'll need to configure Istio AuthorizationPolicy to allow health checking and metrics collection to your applications from system pods. You can allow access from system pods by paths .","title":"Health checking and metrics collection"},{"location":"serving/istio-authorization/#allowing-access-from-system-pods-by-paths","text":"Knative system pods access your application using the following paths: /metrics /healthz The /metrics path allows the autoscaler pod to collect metrics. The /healthz path allows system pods to probe the service. You can add the /metrics and /healthz paths to the AuthorizationPolicy as shown in the example: $ cat <<EOF | kubectl apply -f - apiVersion: security.istio.io/v1beta1 kind: AuthorizationPolicy metadata: name: allowlist-by-paths namespace: serving-tests spec: action: ALLOW rules: - to: - operation: paths: - /metrics # The path to collect metrics by system pod. - /healthz # The path to probe by system pod. EOF","title":"Allowing access from system pods by paths"},{"location":"serving/knative-kubernetes-services/","text":"Knative Kubernetes Services \u00b6 This guide describes the Kubernetes Services that are active when running Knative Serving. Before You Begin \u00b6 This guide assumes that you have installed Knative Serving. If you have not, instructions on how to do this are located here . Verify that you have the proper components in your cluster. To view the services installed in your cluster, use the command: $ kubectl get services -n knative-serving This should return the following output: NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE activator-service ClusterIP 10 .96.61.11 <none> 80 /TCP,81/TCP,9090/TCP 1h autoscaler ClusterIP 10 .104.217.223 <none> 8080 /TCP,9090/TCP 1h controller ClusterIP 10 .101.39.220 <none> 9090 /TCP 1h webhook ClusterIP 10 .107.144.50 <none> 443 /TCP 1h To view the deployments in your cluster, use the following command: $ kubectl get deployments -n knative-serving This should return the following output: NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE activator 1 1 1 1 1h autoscaler 1 1 1 1 1h controller 1 1 1 1 1h networking-certmanager 1 1 1 1 1h networking-istio 1 1 1 1 1h webhook 1 1 1 1 1h These services and deployments are installed by the serving.yaml file during install. The next section describes their function. Components \u00b6 Service: activator \u00b6 The activator is responsible for receiving & buffering requests for inactive revisions and reporting metrics to the autoscaler. It also retries requests to a revision after the autoscaler scales the revision based on the reported metrics. Service: autoscaler \u00b6 The autoscaler receives request metrics and adjusts the number of pods required to handle the load of traffic. Service: controller \u00b6 The controller service reconciles all the public Knative objects and autoscaling CRDs. When a user applies a Knative service to the Kubernetes API, this creates the configuration and route. It will convert the configuration into revisions and the revisions into deployments and Knative Pod Autoscalers (KPAs). Service: webhook \u00b6 The webhook intercepts all Kubernetes API calls as well as all CRD insertions and updates. It sets default values, rejects inconsitent and invalid objects, and validates and mutates Kubernetes API calls. Deployment: networking-certmanager \u00b6 The certmanager reconciles cluster ingresses into cert manager objects. Deployment: networking-istio \u00b6 The networking-istio deployment reconciles a cluster's ingress into an Istio virtual service . What's Next \u00b6 For a deeper look at the services and deployments involved in Knative Serving, click here . For a high-level analysis of Serving, look at the documentation here . Check out the Knative Serving code samples here for more hands-on tutorials.","title":"Knative Kubernetes Components"},{"location":"serving/knative-kubernetes-services/#knative-kubernetes-services","text":"This guide describes the Kubernetes Services that are active when running Knative Serving.","title":"Knative Kubernetes Services"},{"location":"serving/knative-kubernetes-services/#before-you-begin","text":"This guide assumes that you have installed Knative Serving. If you have not, instructions on how to do this are located here . Verify that you have the proper components in your cluster. To view the services installed in your cluster, use the command: $ kubectl get services -n knative-serving This should return the following output: NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE activator-service ClusterIP 10 .96.61.11 <none> 80 /TCP,81/TCP,9090/TCP 1h autoscaler ClusterIP 10 .104.217.223 <none> 8080 /TCP,9090/TCP 1h controller ClusterIP 10 .101.39.220 <none> 9090 /TCP 1h webhook ClusterIP 10 .107.144.50 <none> 443 /TCP 1h To view the deployments in your cluster, use the following command: $ kubectl get deployments -n knative-serving This should return the following output: NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE activator 1 1 1 1 1h autoscaler 1 1 1 1 1h controller 1 1 1 1 1h networking-certmanager 1 1 1 1 1h networking-istio 1 1 1 1 1h webhook 1 1 1 1 1h These services and deployments are installed by the serving.yaml file during install. The next section describes their function.","title":"Before You Begin"},{"location":"serving/knative-kubernetes-services/#components","text":"","title":"Components"},{"location":"serving/knative-kubernetes-services/#service-activator","text":"The activator is responsible for receiving & buffering requests for inactive revisions and reporting metrics to the autoscaler. It also retries requests to a revision after the autoscaler scales the revision based on the reported metrics.","title":"Service: activator"},{"location":"serving/knative-kubernetes-services/#service-autoscaler","text":"The autoscaler receives request metrics and adjusts the number of pods required to handle the load of traffic.","title":"Service: autoscaler"},{"location":"serving/knative-kubernetes-services/#service-controller","text":"The controller service reconciles all the public Knative objects and autoscaling CRDs. When a user applies a Knative service to the Kubernetes API, this creates the configuration and route. It will convert the configuration into revisions and the revisions into deployments and Knative Pod Autoscalers (KPAs).","title":"Service: controller"},{"location":"serving/knative-kubernetes-services/#service-webhook","text":"The webhook intercepts all Kubernetes API calls as well as all CRD insertions and updates. It sets default values, rejects inconsitent and invalid objects, and validates and mutates Kubernetes API calls.","title":"Service: webhook"},{"location":"serving/knative-kubernetes-services/#deployment-networking-certmanager","text":"The certmanager reconciles cluster ingresses into cert manager objects.","title":"Deployment: networking-certmanager"},{"location":"serving/knative-kubernetes-services/#deployment-networking-istio","text":"The networking-istio deployment reconciles a cluster's ingress into an Istio virtual service .","title":"Deployment: networking-istio"},{"location":"serving/knative-kubernetes-services/#whats-next","text":"For a deeper look at the services and deployments involved in Knative Serving, click here . For a high-level analysis of Serving, look at the documentation here . Check out the Knative Serving code samples here for more hands-on tutorials.","title":"What's Next"},{"location":"serving/rolling-out-latest-revision/","text":"Gradually rolling out latest Revisions \u00b6 If your traffic configuration points to a Configuration target, rather than revision target, it means that when a new Revision is created and ready 100% of that target's traffic will be immediately shifted to the new revision, which might not be ready to accept that scale with a single pod and with cold starts taking some time it is possible to end up in a situation where a lot of requests are backed up either at QP or Activator and after a while they might expire or QP might outright reject the requests. To mitigate this problem Knative as of 0.20 release Knative provides users with a possibility to gradually shift the traffic to the latest revision. This is governed by a single parameter which denotes rollout-duration . The affected Configuration targets will be rolled out to 1% of traffic first and then in equal incremental steps for the rest of the assigned traffic. Note, that the rollout is purely time based and does not interact with the Autoscaling subsystem. This feature is available to untagged and tagged traffic targets configured for both Kservices and Kservice-less Routes. Configuring gradual Rollout \u00b6 This value currently can be configured on the cluster level (starting v0.20) via a setting in the config-network ConfigMap or per Kservice or Route using an annotation (staring v.0.21). Per-revision apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default annotations : serving.knative.dev/rolloutDuration : \"380s\" ... Global (ConfigMap) apiVersion : v1 kind : ConfigMap metadata : name : config-network namespace : knative-serving data : rolloutDuration : \"380s\" # Value in seconds. Global (Operator) apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : network : rolloutDuration : \"380s\" Route Status updates \u00b6 During the rollout the system will update the Route and Kservice status. Both traffic and conditions status fields will be affected. For example, a possible rollout of the following traffic configuration traffic : - percent : 55 configurationName : config # Pinned to latest ready Revision - percent : 45 revisionName : config-00005 # Pinned to a specific Revision. would be (if inspecting the route status): traffic : - percent : 54 revisionName : config-00008 - percent : 1 revisionName : config-00009 - percent : 45 revisionName : config-00005 # Pinned to a specific Revision. and then, presuming steps of 18%: traffic : - percent : 36 revisionName : config-00008 - percent : 19 revisionName : config-00009 - percent : 45 revisionName : config-00005 # Pinned to a specific Revision. and so on until final state is achieved: traffic : - percent : 55 revisionName : config-00009 - percent : 45 revisionName : config-00005 # Pinned to a specific Revision. During the rollout the Route and (Kservice, if present) status conditions will be the following: ... status : conditions : ... - lastTransitionTime : \"...\" message : A gradual rollout of the latest revision(s) is in progress. reason : RolloutInProgress status : Unknown type : Ready ... Multiple Rollouts \u00b6 If a new revision is created while the rollout is in progress then the system would start shifting the traffic immediately to the newest revision and it will drain the incomplete rollouts from newest to the oldest.","title":"Gradually Rolling out Revisions"},{"location":"serving/rolling-out-latest-revision/#gradually-rolling-out-latest-revisions","text":"If your traffic configuration points to a Configuration target, rather than revision target, it means that when a new Revision is created and ready 100% of that target's traffic will be immediately shifted to the new revision, which might not be ready to accept that scale with a single pod and with cold starts taking some time it is possible to end up in a situation where a lot of requests are backed up either at QP or Activator and after a while they might expire or QP might outright reject the requests. To mitigate this problem Knative as of 0.20 release Knative provides users with a possibility to gradually shift the traffic to the latest revision. This is governed by a single parameter which denotes rollout-duration . The affected Configuration targets will be rolled out to 1% of traffic first and then in equal incremental steps for the rest of the assigned traffic. Note, that the rollout is purely time based and does not interact with the Autoscaling subsystem. This feature is available to untagged and tagged traffic targets configured for both Kservices and Kservice-less Routes.","title":"Gradually rolling out latest Revisions"},{"location":"serving/rolling-out-latest-revision/#configuring-gradual-rollout","text":"This value currently can be configured on the cluster level (starting v0.20) via a setting in the config-network ConfigMap or per Kservice or Route using an annotation (staring v.0.21). Per-revision apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default annotations : serving.knative.dev/rolloutDuration : \"380s\" ... Global (ConfigMap) apiVersion : v1 kind : ConfigMap metadata : name : config-network namespace : knative-serving data : rolloutDuration : \"380s\" # Value in seconds. Global (Operator) apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : network : rolloutDuration : \"380s\"","title":"Configuring gradual Rollout"},{"location":"serving/rolling-out-latest-revision/#route-status-updates","text":"During the rollout the system will update the Route and Kservice status. Both traffic and conditions status fields will be affected. For example, a possible rollout of the following traffic configuration traffic : - percent : 55 configurationName : config # Pinned to latest ready Revision - percent : 45 revisionName : config-00005 # Pinned to a specific Revision. would be (if inspecting the route status): traffic : - percent : 54 revisionName : config-00008 - percent : 1 revisionName : config-00009 - percent : 45 revisionName : config-00005 # Pinned to a specific Revision. and then, presuming steps of 18%: traffic : - percent : 36 revisionName : config-00008 - percent : 19 revisionName : config-00009 - percent : 45 revisionName : config-00005 # Pinned to a specific Revision. and so on until final state is achieved: traffic : - percent : 55 revisionName : config-00009 - percent : 45 revisionName : config-00005 # Pinned to a specific Revision. During the rollout the Route and (Kservice, if present) status conditions will be the following: ... status : conditions : ... - lastTransitionTime : \"...\" message : A gradual rollout of the latest revision(s) is in progress. reason : RolloutInProgress status : Unknown type : Ready ...","title":"Route Status updates"},{"location":"serving/rolling-out-latest-revision/#multiple-rollouts","text":"If a new revision is created while the rollout is in progress then the system would start shifting the traffic immediately to the newest revision and it will drain the incomplete rollouts from newest to the oldest.","title":"Multiple Rollouts"},{"location":"serving/setting-up-custom-ingress-gateway/","text":"Setting up custom ingress gateway \u00b6 Knative uses a shared ingress Gateway to serve all incoming traffic within Knative service mesh, which is the knative-ingress-gateway Gateway under the knative-serving namespace. By default, we use Istio gateway service istio-ingressgateway under istio-system namespace as its underlying service. You can replace the service with that of your own as follows. Step 1: Create Gateway Service and Deployment Instance \u00b6 You'll need to create the gateway service and deployment instance to handle traffic first. Let's say you customized the default istio-ingressgateway to custom-ingressgateway as follows. apiVersion : install.istio.io/v1alpha1 kind : IstioOperator spec : values : global : proxy : autoInject : disabled useMCP : false # The third-party-jwt is not enabled on all k8s. # See: https://istio.io/docs/ops/best-practices/security/#configure-third-party-service-account-tokens jwtPolicy : first-party-jwt addonComponents : pilot : enabled : true prometheus : enabled : false components : ingressGateways : - name : custom-ingressgateway enabled : true namespace : custom-ns label : istio : custom-gateway Step 2: Update Knative Gateway \u00b6 Update gateway instance knative-ingress-gateway under knative-serving namespace: kubectl edit gateway knative-ingress-gateway -n knative-serving Replace the label selector with the label of your service: istio: ingressgateway For the service above, it should be updated to: istio: custom-gateway If there is a change in service ports (compared with that of istio-ingressgateway ), update the port info in the gateway accordingly. Step 3: Update Gateway Configmap \u00b6 Update gateway configmap config-istio under knative-serving namespace: kubectl edit configmap config-istio -n knative-serving Replace the istio-ingressgateway.istio-system.svc.cluster.local field with the fully qualified url of your service. gateway.knative-serving.knative-ingress-gateway: \"istio-ingressgateway.istio-system.svc.cluster.local\" For the service above, it should be updated to: gateway.knative-serving.knative-ingress-gateway: custom-ingressgateway.custom-ns.svc.cluster.local","title":"Setting up custom ingress gateway"},{"location":"serving/setting-up-custom-ingress-gateway/#setting-up-custom-ingress-gateway","text":"Knative uses a shared ingress Gateway to serve all incoming traffic within Knative service mesh, which is the knative-ingress-gateway Gateway under the knative-serving namespace. By default, we use Istio gateway service istio-ingressgateway under istio-system namespace as its underlying service. You can replace the service with that of your own as follows.","title":"Setting up custom ingress gateway"},{"location":"serving/setting-up-custom-ingress-gateway/#step-1-create-gateway-service-and-deployment-instance","text":"You'll need to create the gateway service and deployment instance to handle traffic first. Let's say you customized the default istio-ingressgateway to custom-ingressgateway as follows. apiVersion : install.istio.io/v1alpha1 kind : IstioOperator spec : values : global : proxy : autoInject : disabled useMCP : false # The third-party-jwt is not enabled on all k8s. # See: https://istio.io/docs/ops/best-practices/security/#configure-third-party-service-account-tokens jwtPolicy : first-party-jwt addonComponents : pilot : enabled : true prometheus : enabled : false components : ingressGateways : - name : custom-ingressgateway enabled : true namespace : custom-ns label : istio : custom-gateway","title":"Step 1: Create Gateway Service and Deployment Instance"},{"location":"serving/setting-up-custom-ingress-gateway/#step-2-update-knative-gateway","text":"Update gateway instance knative-ingress-gateway under knative-serving namespace: kubectl edit gateway knative-ingress-gateway -n knative-serving Replace the label selector with the label of your service: istio: ingressgateway For the service above, it should be updated to: istio: custom-gateway If there is a change in service ports (compared with that of istio-ingressgateway ), update the port info in the gateway accordingly.","title":"Step 2: Update Knative Gateway"},{"location":"serving/setting-up-custom-ingress-gateway/#step-3-update-gateway-configmap","text":"Update gateway configmap config-istio under knative-serving namespace: kubectl edit configmap config-istio -n knative-serving Replace the istio-ingressgateway.istio-system.svc.cluster.local field with the fully qualified url of your service. gateway.knative-serving.knative-ingress-gateway: \"istio-ingressgateway.istio-system.svc.cluster.local\" For the service above, it should be updated to: gateway.knative-serving.knative-ingress-gateway: custom-ingressgateway.custom-ns.svc.cluster.local","title":"Step 3: Update Gateway Configmap"},{"location":"serving/tag-resolution/","text":"Enabling tag to digest resolution \u00b6 Knative serving resolves image tags to a digest when you create a revision. This gives knative revisions some very nice properties, e.g. your deployments will be consistent, you don't have to worry about \"immutable tags\", etc. For more info, see Why we resolve tags in Knative . Unfortunately, this means that the knative serving controller needs to be configured to access your container registry. Custom Certificates \u00b6 If you're using a registry that has a self-signed certificate, you'll need to convince the serving controller to trust that certificate. We respect the SSL_CERT_FILE and SSL_CERT_DIR environment variables, so you can trust them by mounting the certificates into the controller's deployment and setting the environment variable appropriately, assuming you have a custom-certs secret containing your CA certs: apiVersion : apps/v1 kind : Deployment metadata : name : controller namespace : knative-serving spec : template : spec : containers : - name : controller volumeMounts : - name : custom-certs mountPath : /path/to/custom/certs env : - name : SSL_CERT_DIR value : /path/to/custom/certs volumes : - name : custom-certs secret : secretName : custom-certs Corporate Proxy \u00b6 If you're behind a corporate proxy, you'll need to proxy the tag resolution requests between the controller and your registry. We respect the HTTP_PROXY and HTTPS_PROXY environment variables, so you can configure the controller's deployment via: apiVersion : apps/v1 kind : Deployment metadata : name : controller namespace : knative-serving spec : template : spec : containers : - name : controller env : - name : HTTP_PROXY value : http://proxy.example.com - name : HTTPS_PROXY value : https://proxy.example.com Skipping tag resolution \u00b6 If this all seems like too much trouble, you can configure serving to skip tag resolution via the registriesSkippingTagResolving configmap field: kubectl -n knative-serving edit configmap config-deployment E.g., to disable tag resolution for registry.example.com (note: This is not a complete configmap, it is a snippet showing registriesSkippingTagResolving): apiVersion : v1 kind : ConfigMap metadata : name : config-deployment namespace : knative-serving data : # List of repositories for which tag to digest resolving should be skipped registriesSkippingTagResolving : registry.example.com","title":"Enabling tag to digest resolution"},{"location":"serving/tag-resolution/#enabling-tag-to-digest-resolution","text":"Knative serving resolves image tags to a digest when you create a revision. This gives knative revisions some very nice properties, e.g. your deployments will be consistent, you don't have to worry about \"immutable tags\", etc. For more info, see Why we resolve tags in Knative . Unfortunately, this means that the knative serving controller needs to be configured to access your container registry.","title":"Enabling tag to digest resolution"},{"location":"serving/tag-resolution/#custom-certificates","text":"If you're using a registry that has a self-signed certificate, you'll need to convince the serving controller to trust that certificate. We respect the SSL_CERT_FILE and SSL_CERT_DIR environment variables, so you can trust them by mounting the certificates into the controller's deployment and setting the environment variable appropriately, assuming you have a custom-certs secret containing your CA certs: apiVersion : apps/v1 kind : Deployment metadata : name : controller namespace : knative-serving spec : template : spec : containers : - name : controller volumeMounts : - name : custom-certs mountPath : /path/to/custom/certs env : - name : SSL_CERT_DIR value : /path/to/custom/certs volumes : - name : custom-certs secret : secretName : custom-certs","title":"Custom Certificates"},{"location":"serving/tag-resolution/#corporate-proxy","text":"If you're behind a corporate proxy, you'll need to proxy the tag resolution requests between the controller and your registry. We respect the HTTP_PROXY and HTTPS_PROXY environment variables, so you can configure the controller's deployment via: apiVersion : apps/v1 kind : Deployment metadata : name : controller namespace : knative-serving spec : template : spec : containers : - name : controller env : - name : HTTP_PROXY value : http://proxy.example.com - name : HTTPS_PROXY value : https://proxy.example.com","title":"Corporate Proxy"},{"location":"serving/tag-resolution/#skipping-tag-resolution","text":"If this all seems like too much trouble, you can configure serving to skip tag resolution via the registriesSkippingTagResolving configmap field: kubectl -n knative-serving edit configmap config-deployment E.g., to disable tag resolution for registry.example.com (note: This is not a complete configmap, it is a snippet showing registriesSkippingTagResolving): apiVersion : v1 kind : ConfigMap metadata : name : config-deployment namespace : knative-serving data : # List of repositories for which tag to digest resolving should be skipped registriesSkippingTagResolving : registry.example.com","title":"Skipping tag resolution"},{"location":"serving/using-a-custom-domain-per-service/","text":"Setting up a custom domain per Service \u00b6 By default, Knative uses the {route}.{namespace}.{default-domain} fully qualified domain name for the Service, where default-domain is example.com . You are able to change the default-domain following the Setting up a custom domain guide. This guide documents the process to use a custom FQDN for a Service, like my-service.example.com , created by @bsideup . NOTE There is currently no official process to set up a custom domain per Service. The topic is being discussed here . Edit using kubectl \u00b6 Edit the domainTemplate entry on the config-network configuration. You can find more information about it here : kubectl edit cm config-network --namespace knative-serving Replace the domainTemplate with the following (the spaces must be respected): [ ... ] data : [ ... ] domainTemplate : |- {{if index .Annotations \"custom-hostname\" -}} {{- index .Annotations \"custom-hostname\" -}} {{else -}} {{- .Name}}.{{.Namespace -}} {{end -}} .{{.Domain}} Save and close your editor. Edit the Service \u00b6 In a Service definition, add the custom-hostname annotation: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : hello-world annotations : # the Service FQDN will become hello-world.{default-domain} custom-hostname : hello-world spec : [ ... ] Apply your changes. Verify the changes \u00b6 Verify that the Service was created with the specified hostname: kubectl get ksvc hello-world NAME URL LATESTCREATED LATESTREADY READY REASON hello-world http://hello-world.example.com hello-world-nfqh2 hello-world-nfqh2 True","title":"Setting up a Custom Domain per Service"},{"location":"serving/using-a-custom-domain-per-service/#setting-up-a-custom-domain-per-service","text":"By default, Knative uses the {route}.{namespace}.{default-domain} fully qualified domain name for the Service, where default-domain is example.com . You are able to change the default-domain following the Setting up a custom domain guide. This guide documents the process to use a custom FQDN for a Service, like my-service.example.com , created by @bsideup . NOTE There is currently no official process to set up a custom domain per Service. The topic is being discussed here .","title":"Setting up a custom domain per Service"},{"location":"serving/using-a-custom-domain-per-service/#edit-using-kubectl","text":"Edit the domainTemplate entry on the config-network configuration. You can find more information about it here : kubectl edit cm config-network --namespace knative-serving Replace the domainTemplate with the following (the spaces must be respected): [ ... ] data : [ ... ] domainTemplate : |- {{if index .Annotations \"custom-hostname\" -}} {{- index .Annotations \"custom-hostname\" -}} {{else -}} {{- .Name}}.{{.Namespace -}} {{end -}} .{{.Domain}} Save and close your editor.","title":"Edit using kubectl"},{"location":"serving/using-a-custom-domain-per-service/#edit-the-service","text":"In a Service definition, add the custom-hostname annotation: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : hello-world annotations : # the Service FQDN will become hello-world.{default-domain} custom-hostname : hello-world spec : [ ... ] Apply your changes.","title":"Edit the Service"},{"location":"serving/using-a-custom-domain-per-service/#verify-the-changes","text":"Verify that the Service was created with the specified hostname: kubectl get ksvc hello-world NAME URL LATESTCREATED LATESTREADY READY REASON hello-world http://hello-world.example.com hello-world-nfqh2 hello-world-nfqh2 True","title":"Verify the changes"},{"location":"serving/using-a-custom-domain/","text":"Setting up a custom domain \u00b6 By default, Knative Serving routes use example.com as the default domain. The fully qualified domain name for a route by default is {route}.{namespace}.{default-domain} . To change the {default-domain} value there are a few steps involved: Edit using kubectl \u00b6 Edit the domain configuration config-map to replace example.com with your own domain, for example mydomain.com : kubectl edit cm config-domain --namespace knative-serving This command opens your default text editor and allows you to edit the config map . apiVersion : v1 data : _example : | ################################ # # # EXAMPLE CONFIGURATION # # # ################################ # ... example.com: | kind : ConfigMap Edit the file to replace example.com with the domain you'd like to use, remove the _example key and save your changes. In this example, we configure mydomain.com for all routes: apiVersion : v1 data : mydomain.com : \"\" kind : ConfigMap [ ... ] Apply from a file \u00b6 You can also apply an updated domain configuration: Create a new file, config-domain.yaml and paste the following text, replacing the example.org and example.com values with the new domain you want to use: apiVersion : v1 kind : ConfigMap metadata : name : config-domain namespace : knative-serving data : # These are example settings of domain. # example.org will be used for routes having app=prod. example.org : | selector: app: prod # Default value for domain, for routes that does not have app=prod labels. # Although it will match all routes, it is the least-specific rule so it # will only be used if no other domain matches. example.com : \"\" Apply updated domain configuration to your cluster: kubectl apply --filename config-domain.yaml Deploy an application \u00b6 If you have an existing deployment, Knative will reconcile the change made to the configuration map and automatically update the host name for all of the deployed services and routes. Deploy an app (for example, helloworld-go ), to your cluster as normal. You can retrieve the URL in Knative Route \"helloworld-go\" with the following command: kubectl get route helloworld-go --output jsonpath = \"{.status.url}\" You should see the full customized domain: helloworld-go.default.mydomain.com . And you can check the IP address of your Knative gateway by running: export INGRESSGATEWAY = istio-ingressgateway if kubectl get configmap config-istio -n knative-serving & > /dev/null ; then export INGRESSGATEWAY = istio-ingressgateway fi kubectl get svc $INGRESSGATEWAY --namespace istio-system --output jsonpath = \"{.status.loadBalancer.ingress[*]['ip']}\" Local DNS setup \u00b6 You can map the domain to the IP address of your Knative gateway in your local machine with: INGRESSGATEWAY = istio-ingressgateway export GATEWAY_IP = ` kubectl get svc $INGRESSGATEWAY --namespace istio-system --output jsonpath = \"{.status.loadBalancer.ingress[*]['ip']}\" ` # helloworld-go is the generated Knative Route of \"helloworld-go\" sample. # You need to replace it with your own Route in your project. export DOMAIN_NAME = ` kubectl get route helloworld-go --output jsonpath = \"{.status.url}\" | cut -d '/' -f 3 ` # Add the record of Gateway IP and domain name into file \"/etc/hosts\" echo -e \" $GATEWAY_IP \\t $DOMAIN_NAME \" | sudo tee -a /etc/hosts You can now access your domain from the browser in your machine and do some quick checks. Publish your Domain \u00b6 Follow these steps to make your domain publicly accessible: Set static IP for Knative Gateway \u00b6 You might want to set a static IP for your Knative gateway , so that the gateway IP does not change each time your cluster is restarted. Update your DNS records \u00b6 To publish your domain, you need to update your DNS provider to point to the IP address for your service ingress. Create a wildcard record for the namespace and custom domain to the ingress IP Address, which would enable hostnames for multiple services in the same namespace to work without creating additional DNS entries. *.default.mydomain.com 59 IN A 35.237.28.44 Create an A record to point from the fully qualified domain name to the IP address of your Knative gateway. This step needs to be done for each Knative Service or Route created. helloworld-go.default.mydomain.com 59 IN A 35.237.28.44 If you are using Google Cloud DNS, you can find step-by-step instructions in the Cloud DNS quickstart . Once the domain update has propagated, you can access your app using the fully qualified domain name of the deployed route, for example http://helloworld-go.default.mydomain.com","title":"Setting up a Custom Domain"},{"location":"serving/using-a-custom-domain/#setting-up-a-custom-domain","text":"By default, Knative Serving routes use example.com as the default domain. The fully qualified domain name for a route by default is {route}.{namespace}.{default-domain} . To change the {default-domain} value there are a few steps involved:","title":"Setting up a custom domain"},{"location":"serving/using-a-custom-domain/#edit-using-kubectl","text":"Edit the domain configuration config-map to replace example.com with your own domain, for example mydomain.com : kubectl edit cm config-domain --namespace knative-serving This command opens your default text editor and allows you to edit the config map . apiVersion : v1 data : _example : | ################################ # # # EXAMPLE CONFIGURATION # # # ################################ # ... example.com: | kind : ConfigMap Edit the file to replace example.com with the domain you'd like to use, remove the _example key and save your changes. In this example, we configure mydomain.com for all routes: apiVersion : v1 data : mydomain.com : \"\" kind : ConfigMap [ ... ]","title":"Edit using kubectl"},{"location":"serving/using-a-custom-domain/#apply-from-a-file","text":"You can also apply an updated domain configuration: Create a new file, config-domain.yaml and paste the following text, replacing the example.org and example.com values with the new domain you want to use: apiVersion : v1 kind : ConfigMap metadata : name : config-domain namespace : knative-serving data : # These are example settings of domain. # example.org will be used for routes having app=prod. example.org : | selector: app: prod # Default value for domain, for routes that does not have app=prod labels. # Although it will match all routes, it is the least-specific rule so it # will only be used if no other domain matches. example.com : \"\" Apply updated domain configuration to your cluster: kubectl apply --filename config-domain.yaml","title":"Apply from a file"},{"location":"serving/using-a-custom-domain/#deploy-an-application","text":"If you have an existing deployment, Knative will reconcile the change made to the configuration map and automatically update the host name for all of the deployed services and routes. Deploy an app (for example, helloworld-go ), to your cluster as normal. You can retrieve the URL in Knative Route \"helloworld-go\" with the following command: kubectl get route helloworld-go --output jsonpath = \"{.status.url}\" You should see the full customized domain: helloworld-go.default.mydomain.com . And you can check the IP address of your Knative gateway by running: export INGRESSGATEWAY = istio-ingressgateway if kubectl get configmap config-istio -n knative-serving & > /dev/null ; then export INGRESSGATEWAY = istio-ingressgateway fi kubectl get svc $INGRESSGATEWAY --namespace istio-system --output jsonpath = \"{.status.loadBalancer.ingress[*]['ip']}\"","title":"Deploy an application"},{"location":"serving/using-a-custom-domain/#local-dns-setup","text":"You can map the domain to the IP address of your Knative gateway in your local machine with: INGRESSGATEWAY = istio-ingressgateway export GATEWAY_IP = ` kubectl get svc $INGRESSGATEWAY --namespace istio-system --output jsonpath = \"{.status.loadBalancer.ingress[*]['ip']}\" ` # helloworld-go is the generated Knative Route of \"helloworld-go\" sample. # You need to replace it with your own Route in your project. export DOMAIN_NAME = ` kubectl get route helloworld-go --output jsonpath = \"{.status.url}\" | cut -d '/' -f 3 ` # Add the record of Gateway IP and domain name into file \"/etc/hosts\" echo -e \" $GATEWAY_IP \\t $DOMAIN_NAME \" | sudo tee -a /etc/hosts You can now access your domain from the browser in your machine and do some quick checks.","title":"Local DNS setup"},{"location":"serving/using-a-custom-domain/#publish-your-domain","text":"Follow these steps to make your domain publicly accessible:","title":"Publish your Domain"},{"location":"serving/using-a-custom-domain/#set-static-ip-for-knative-gateway","text":"You might want to set a static IP for your Knative gateway , so that the gateway IP does not change each time your cluster is restarted.","title":"Set static IP for Knative Gateway"},{"location":"serving/using-a-custom-domain/#update-your-dns-records","text":"To publish your domain, you need to update your DNS provider to point to the IP address for your service ingress. Create a wildcard record for the namespace and custom domain to the ingress IP Address, which would enable hostnames for multiple services in the same namespace to work without creating additional DNS entries. *.default.mydomain.com 59 IN A 35.237.28.44 Create an A record to point from the fully qualified domain name to the IP address of your Knative gateway. This step needs to be done for each Knative Service or Route created. helloworld-go.default.mydomain.com 59 IN A 35.237.28.44 If you are using Google Cloud DNS, you can find step-by-step instructions in the Cloud DNS quickstart . Once the domain update has propagated, you can access your app using the fully qualified domain name of the deployed route, for example http://helloworld-go.default.mydomain.com","title":"Update your DNS records"},{"location":"serving/using-a-tls-cert/","text":"Configuring HTTPS with TLS certificates \u00b6 Learn how to configure secure HTTPS connections in Knative using TLS certificates ( TLS replaces SSL ). Configure secure HTTPS connections to enable your Knative services and routes to terminate external TLS connections . You can configure Knative to handle certificates that you manually specify, or you can enable Knative to automatically obtain and renew certificates. You can use either Certbot or cert-manager to obtain certificates. Both tools support TLS certificates but if you want to enable Knative for automatic TLS certificate provisioning, you must install and configure the cert-manager tool: Manually obtain and renew certificates : Both the Certbot and cert-manager tools can be used to manually obtain TLS certificates. In general, after you obtain a certificate, you must create a Kubernetes secret to use that certificate in your cluster. See the complete set of steps below for details about manually obtaining and configuring certificates. Enable Knative to automatically obtain and renew TLS certificates : You can also use cert-manager to configure Knative to automatically obtain new TLS certificates and renew existing ones. If you want to enable Knative to automatically provision TLS certificates, instead see the Enabling automatic TLS certificate provisioning topic. By default, the Let's Encrypt Certificate Authority (CA) is used to demonstrate how to enable HTTPS connections, but you can configure Knative to use any certificate from a CA that supports the ACME protocol. However, you must use and configure your certificate issuer to use the DNS-01 challenge type . Important: Certificates issued by Let's Encrypt are valid for only 90 days . Therefore, if you choose to manually obtain and configure your certificates, you must ensure that you renew each certificate before it expires. Before you begin \u00b6 You must meet the following requirements to enable secure HTTPS connections: Knative Serving must be installed. For details about installing the Serving component, see the Knative installation guides . You must configure your Knative cluster to use a custom domain . Important: Istio only supports a single certificate per Kubernetes cluster. To serve multiple domains using your Knative cluster, you must ensure that your new or existing certificate is signed for each of the domains that you want to serve. Obtaining a TLS certificate \u00b6 If you already have a signed certificate for your domain, see Manually adding a TLS certificate for details about configuring your Knative cluster. If you need a new TLS certificate, you can choose to use one of the following tools to obtain a certificate from Let's Encrypt: Setup Certbot to manually obtain Let's Encrypt certificates Setup cert-manager to either manually obtain a certificate, or to automatically provision certificates This page covers details for both of the above options. For details about using other CA's, see the tool's reference documentation: Certbot supported providers cert-manager supported providers Using Certbot to manually obtain Let\u2019s Encrypt certificates \u00b6 Use the following steps to install Certbot and the use the tool to manually obtain a TLS certificate from Let's Encrypt. Install Certbot by following the certbot-auto wrapper script instructions. Run the following command to use Certbot to request a certificate using DNS challenge during authorization: ./certbot-auto certonly --manual --preferred-challenges dns -d '*.default.yourdomain.com' where -d specifies your domain. If you want to validate multiple domain's, you can include multiple flags: -d MY.EXAMPLEDOMAIN.1 -d MY.EXAMPLEDOMAIN.2 . For more information, see the Cerbot command-line reference. The Certbot tool walks you through the steps of validating that you own each domain that you specify by creating TXT records in those domains. Result: CertBot creates two files: Certificate: fullchain.pem Private key: privkey.pem What's next: Add the certificate and private key to your Knative cluster by creating a Kubernetes secret . Using cert-manager to obtain Let's Encrypt certificates \u00b6 You can install and use cert-manager to either manually obtain a certificate or to configure your Knative cluster for automatic certificate provisioning: Manual certificates : Install cert-manager and then use the tool to manually obtain a certificate. To use cert-manager to manually obtain certificates: Install and configure cert-manager . Continue to the steps below about manually adding a TLS certificate by creating and using a Kubernetes secret. Automatic certificates : Configure Knative to use cert-manager for automatically obtaining and renewing TLS certificate. The steps for installing and configuring cert-manager for this method are covered in full in the Enabling automatic TLS cert provisioning topic. Manually adding a TLS certificate \u00b6 If you have an existing certificate or have used one of the Certbot or cert-manager tool to manually obtain a new certificate, you can use the following steps to add that certificate to your Knative cluster. For instructions about enabling Knative for automatic certificate provisioning, see Enabling automatic TLS cert provisioning . Otherwise, continue below for instructions about manually adding a certificate. Contour To manually add a TLS certificate to your Knative cluster, you must create a Kubernetes secret and then configure the Knative Contour plugin Create a Kubernetes secret to hold your TLS certificate, cert.pem , and the private key, key.pem , by entering the following command: kubectl create --namespace contour-external secret tls default-cert \\ --key key.pem \\ --cert cert.pem IMPORTANT Take note of the namespace and secret name. You will need these in future steps. Contour requires you to create a delegation to use this certificate and private key in different namespaces. This can be done by creating the following resource: apiVersion : projectcontour.io/v1 kind : TLSCertificateDelegation metadata : name : default-delegation namespace : contour-external spec : delegations : - secretName : default-cert targetNamespaces : - \"*\" Update the Knative Contour plugin to start using the certificate as a fallback when auto-TLS is disabled. This can be done with the following patch: kubectl patch cm config-contour -n knative-serving \\ -p '{\"data\":{\"default-tls-secret\":\"contour-external/default-cert\"}}' Istio To manually add a TLS certificate to your Knative cluster, you create a Kubernetes secret and then configure the knative-ingress-gateway : Create a Kubernetes secret to hold your TLS certificate, cert.pem , and the private key, key.pem , by entering the following command: kubectl create --namespace istio-system secret tls tls-cert \\ --key key.pem \\ --cert cert.pem Configure Knative to use the new secret that you created for HTTPS connections: Run the following command to open the Knative shared gateway in edit mode: kubectl edit gateway knative-ingress-gateway --namespace knative-serving Update the gateway to include the following tls: section and configuration: tls : mode : SIMPLE credentialName : tls-cert Example: # Please edit the object below. Lines beginning with a '#' will be ignored. # and an empty file will abort the edit. If an error occurs while saving this # file will be reopened with the relevant failures. apiVersion : networking.istio.io/v1alpha3 kind : Gateway metadata : # ... skipped ... spec : selector : istio : ingressgateway servers : - hosts : - \"*\" port : name : http number : 80 protocol : HTTP - hosts : - TLS_HOSTS port : name : https number : 443 protocol : HTTPS tls : mode : SIMPLE credentialName : tls-cert In the example above, TLS_HOSTS represents the hosts of your TLS certificate. It can be a single host, multiple hosts, or a wildcard host. For detailed instructions, please refer Istio documentation What's next: \u00b6 After your changes are running on your Knative cluster, you can begin using the HTTPS protocol for secure access your deployed Knative services.","title":"Configuring HTTPS Connections"},{"location":"serving/using-a-tls-cert/#configuring-https-with-tls-certificates","text":"Learn how to configure secure HTTPS connections in Knative using TLS certificates ( TLS replaces SSL ). Configure secure HTTPS connections to enable your Knative services and routes to terminate external TLS connections . You can configure Knative to handle certificates that you manually specify, or you can enable Knative to automatically obtain and renew certificates. You can use either Certbot or cert-manager to obtain certificates. Both tools support TLS certificates but if you want to enable Knative for automatic TLS certificate provisioning, you must install and configure the cert-manager tool: Manually obtain and renew certificates : Both the Certbot and cert-manager tools can be used to manually obtain TLS certificates. In general, after you obtain a certificate, you must create a Kubernetes secret to use that certificate in your cluster. See the complete set of steps below for details about manually obtaining and configuring certificates. Enable Knative to automatically obtain and renew TLS certificates : You can also use cert-manager to configure Knative to automatically obtain new TLS certificates and renew existing ones. If you want to enable Knative to automatically provision TLS certificates, instead see the Enabling automatic TLS certificate provisioning topic. By default, the Let's Encrypt Certificate Authority (CA) is used to demonstrate how to enable HTTPS connections, but you can configure Knative to use any certificate from a CA that supports the ACME protocol. However, you must use and configure your certificate issuer to use the DNS-01 challenge type . Important: Certificates issued by Let's Encrypt are valid for only 90 days . Therefore, if you choose to manually obtain and configure your certificates, you must ensure that you renew each certificate before it expires.","title":"Configuring HTTPS with TLS certificates"},{"location":"serving/using-a-tls-cert/#before-you-begin","text":"You must meet the following requirements to enable secure HTTPS connections: Knative Serving must be installed. For details about installing the Serving component, see the Knative installation guides . You must configure your Knative cluster to use a custom domain . Important: Istio only supports a single certificate per Kubernetes cluster. To serve multiple domains using your Knative cluster, you must ensure that your new or existing certificate is signed for each of the domains that you want to serve.","title":"Before you begin"},{"location":"serving/using-a-tls-cert/#obtaining-a-tls-certificate","text":"If you already have a signed certificate for your domain, see Manually adding a TLS certificate for details about configuring your Knative cluster. If you need a new TLS certificate, you can choose to use one of the following tools to obtain a certificate from Let's Encrypt: Setup Certbot to manually obtain Let's Encrypt certificates Setup cert-manager to either manually obtain a certificate, or to automatically provision certificates This page covers details for both of the above options. For details about using other CA's, see the tool's reference documentation: Certbot supported providers cert-manager supported providers","title":"Obtaining a TLS certificate"},{"location":"serving/using-a-tls-cert/#using-certbot-to-manually-obtain-lets-encrypt-certificates","text":"Use the following steps to install Certbot and the use the tool to manually obtain a TLS certificate from Let's Encrypt. Install Certbot by following the certbot-auto wrapper script instructions. Run the following command to use Certbot to request a certificate using DNS challenge during authorization: ./certbot-auto certonly --manual --preferred-challenges dns -d '*.default.yourdomain.com' where -d specifies your domain. If you want to validate multiple domain's, you can include multiple flags: -d MY.EXAMPLEDOMAIN.1 -d MY.EXAMPLEDOMAIN.2 . For more information, see the Cerbot command-line reference. The Certbot tool walks you through the steps of validating that you own each domain that you specify by creating TXT records in those domains. Result: CertBot creates two files: Certificate: fullchain.pem Private key: privkey.pem What's next: Add the certificate and private key to your Knative cluster by creating a Kubernetes secret .","title":"Using Certbot to manually obtain Let\u2019s Encrypt certificates"},{"location":"serving/using-a-tls-cert/#using-cert-manager-to-obtain-lets-encrypt-certificates","text":"You can install and use cert-manager to either manually obtain a certificate or to configure your Knative cluster for automatic certificate provisioning: Manual certificates : Install cert-manager and then use the tool to manually obtain a certificate. To use cert-manager to manually obtain certificates: Install and configure cert-manager . Continue to the steps below about manually adding a TLS certificate by creating and using a Kubernetes secret. Automatic certificates : Configure Knative to use cert-manager for automatically obtaining and renewing TLS certificate. The steps for installing and configuring cert-manager for this method are covered in full in the Enabling automatic TLS cert provisioning topic.","title":"Using cert-manager to obtain Let's Encrypt certificates"},{"location":"serving/using-a-tls-cert/#manually-adding-a-tls-certificate","text":"If you have an existing certificate or have used one of the Certbot or cert-manager tool to manually obtain a new certificate, you can use the following steps to add that certificate to your Knative cluster. For instructions about enabling Knative for automatic certificate provisioning, see Enabling automatic TLS cert provisioning . Otherwise, continue below for instructions about manually adding a certificate. Contour To manually add a TLS certificate to your Knative cluster, you must create a Kubernetes secret and then configure the Knative Contour plugin Create a Kubernetes secret to hold your TLS certificate, cert.pem , and the private key, key.pem , by entering the following command: kubectl create --namespace contour-external secret tls default-cert \\ --key key.pem \\ --cert cert.pem IMPORTANT Take note of the namespace and secret name. You will need these in future steps. Contour requires you to create a delegation to use this certificate and private key in different namespaces. This can be done by creating the following resource: apiVersion : projectcontour.io/v1 kind : TLSCertificateDelegation metadata : name : default-delegation namespace : contour-external spec : delegations : - secretName : default-cert targetNamespaces : - \"*\" Update the Knative Contour plugin to start using the certificate as a fallback when auto-TLS is disabled. This can be done with the following patch: kubectl patch cm config-contour -n knative-serving \\ -p '{\"data\":{\"default-tls-secret\":\"contour-external/default-cert\"}}' Istio To manually add a TLS certificate to your Knative cluster, you create a Kubernetes secret and then configure the knative-ingress-gateway : Create a Kubernetes secret to hold your TLS certificate, cert.pem , and the private key, key.pem , by entering the following command: kubectl create --namespace istio-system secret tls tls-cert \\ --key key.pem \\ --cert cert.pem Configure Knative to use the new secret that you created for HTTPS connections: Run the following command to open the Knative shared gateway in edit mode: kubectl edit gateway knative-ingress-gateway --namespace knative-serving Update the gateway to include the following tls: section and configuration: tls : mode : SIMPLE credentialName : tls-cert Example: # Please edit the object below. Lines beginning with a '#' will be ignored. # and an empty file will abort the edit. If an error occurs while saving this # file will be reopened with the relevant failures. apiVersion : networking.istio.io/v1alpha3 kind : Gateway metadata : # ... skipped ... spec : selector : istio : ingressgateway servers : - hosts : - \"*\" port : name : http number : 80 protocol : HTTP - hosts : - TLS_HOSTS port : name : https number : 443 protocol : HTTPS tls : mode : SIMPLE credentialName : tls-cert In the example above, TLS_HOSTS represents the hosts of your TLS certificate. It can be a single host, multiple hosts, or a wildcard host. For detailed instructions, please refer Istio documentation","title":"Manually adding a TLS certificate"},{"location":"serving/using-a-tls-cert/#whats-next","text":"After your changes are running on your Knative cluster, you can begin using the HTTPS protocol for secure access your deployed Knative services.","title":"What's next:"},{"location":"serving/using-auto-tls/","text":"Enabling automatic TLS certificate provisioning \u00b6 If you install and configure cert-manager, you can configure Knative to automatically obtain new TLS certificates and renew existing ones for Knative Services. To learn more about using secure connections in Knative, see Configuring HTTPS with TLS certificates . Automatic TLS provision mode \u00b6 Knative supports the following Auto TLS modes: Using DNS-01 challenge In this mode, your cluster needs to be able to talk to your DNS server to verify the ownership of your domain. - Provision Certificate per namespace is supported when using DNS-01 challenge mode. - This is the recommended mode for faster certificate provision. - In this mode, a single Certificate will be provisioned per namespace and is reused across the Knative Services within the same namespace. Provision Certificate per Knative Service is supported when using DNS-01 challenge mode. This is the recommended mode for better certificate islation between Knative Services. In this mode, a Certificate will be provisioned for each Knative Service. The TLS effective time is longer as it needs Certificate provision for each Knative Service creation. Using HTTP-01 challenge In this type, your cluster does not need to be able to talk to your DNS server. You must map your domain to the IP of the cluser ingress. When using HTTP-01 challenge, a certificate will be provisioned per Knative Service. HTTP-01 does not support provisioning a certificate per namespace. Before you begin \u00b6 You must meet the following prerequisites to enable Auto TLS: The following must be installed on your Knative cluster: Knative Serving . A Networking layer such as Kourier, Istio with SDS v1.3 or higher, Contour v1.1 or higher, or Gloo v0.18.16 or higher. See Install a networking layer or Istio with SDS, version 1.3 or higher . Note: Currently, Ambassador is unsupported for use with Auto TLS. cert-manager version 1.0.0 and higher . Your Knative cluster must be configured to use a custom domain . Your DNS provider must be setup and configured to your domain. If you want to use HTTP-01 challenge, you need to configure your custom domain to map to the IP of ingress. You can achieve this by adding a DNS A record to map the domain to the IP according to the instructions of your DNS provider. Enabling Auto TLS \u00b6 To enable support for Auto TLS in Knative: Create cert-manager ClusterIssuer \u00b6 Create and add the ClusterIssuer configuration file to your Knative cluster to define who issues the TLS certificates, how requests are validated, and which DNS provider validates those requests. ClusterIssuer for DNS-01 challenge \u00b6 Use the cert-manager reference to determine how to configure your ClusterIssuer file: - See the generic ClusterIssuer example - Also see the DNS01 example Example : Cloud DNS ClusterIssuer configuration file: The following letsencrypt-issuer named ClusterIssuer file is configured for the Let's Encrypt CA and Google Cloud DNS. Under spec , the Let's Encrypt account info, required DNS-01 challenge type, and Cloud DNS provider info defined. For the complete Google Cloud DNS example, see Configuring HTTPS with cert-manager and Google Cloud DNS . apiVersion: cert-manager.io/v1alpha2 kind: ClusterIssuer metadata: name: letsencrypt-dns-issuer spec: acme: server: https://acme-v02.api.letsencrypt.org/directory # This will register an issuer with LetsEncrypt. Replace # with your admin email address. email: myemail@gmail.com privateKeySecretRef: # Set privateKeySecretRef to any unused secret name. name: letsencrypt-dns-issuer solvers: - dns01: clouddns: # Set this to your GCP project-id project: $PROJECT_ID # Set this to the secret that we publish our service account key # in the previous step. serviceAccountSecretRef: name: cloud-dns-key key: key.json ClusterIssuer for HTTP-01 challenge \u00b6 Run the following command to apply the ClusterIssuer for HTT01 challenge: kubectl apply -f - <<EOF apiVersion: cert-manager.io/v1alpha2 kind: ClusterIssuer metadata: name: letsencrypt-http01-issuer spec: acme: privateKeySecretRef: name: letsencrypt server: https://acme-v02.api.letsencrypt.org/directory solvers: - http01: ingress: class: istio EOF Ensure that the ClusterIssuer is created successfully: kubectl get clusterissuer <cluster-issuer-name> --output yaml Result: The Status.Conditions should include Ready=True . DNS-01 challenge only: Configure your DNS provider \u00b6 If you choose to use DNS-01 challenge, configure which DNS provider is used to validate the DNS-01 challenge requests. Instructions about configuring cert-manager, for all the supported DNS providers, are provided in DNS01 challenge providers and configuration instructions . Example: See how the Google Cloud DNS is defined as the provider: Configuring HTTPS with cert-manager and Google Cloud DNS Install networking-certmanager deployment \u00b6 Determine if networking-certmanager is already installed by running the following command: kubectl get deployment networking-certmanager -n knative-serving If networking-certmanager is not found, run the following command: kubectl apply --filename http://github.com/knative/net-certmanager/releases/download/ { 'provider' : 'mike' } /release.yaml Install networking-ns-cert component \u00b6 If you choose to use the mode of provisioning certificate per namespace, you need to install networking-ns-cert components. IMPORTANT: Provisioning a certificate per namespace only works with DNS-01 challenge. This component cannot be used with HTTP-01 challenge. Determine if networking-ns-cert deployment is already installed by running the following command: kubectl get deployment networking-ns-cert -n knative-serving If networking-ns-cert deployment is not found, run the following command: kubectl apply --filename http://github.com/knative/serving/releases/download/ { 'provider' : 'mike' } /serving-nscert.yaml Configure config-certmanager ConfigMap \u00b6 Update your config-certmanager ConfigMap in the knative-serving namespace to reference your new ClusterIssuer . Run the following command to edit your config-certmanager ConfigMap: kubectl edit configmap config-certmanager --namespace knative-serving Add the issuerRef within the data section: ... data: ... issuerRef: | kind: ClusterIssuer name: letsencrypt-issuer Example: apiVersion: v1 kind: ConfigMap metadata: name: config-certmanager namespace: knative-serving labels: networking.knative.dev/certificate-provider: cert-manager data: issuerRef: | kind: ClusterIssuer name: letsencrypt-http01-issuer issueRef defines which ClusterIssuer will be used by Knative to issue certificates. Ensure that the file was updated successfully: kubectl get configmap config-certmanager --namespace knative-serving --output yaml Turn on Auto TLS \u00b6 Update the config-network ConfigMap in the knative-serving namespace to enable autoTLS and specify how HTTP requests are handled: Run the following command to edit your config-network ConfigMap: kubectl edit configmap config-network --namespace knative-serving Add the autoTLS: Enabled attribute under the data section: ... data: ... autoTLS: Enabled ... Example: apiVersion: v1 kind: ConfigMap metadata: name: config-network namespace: knative-serving data: ... autoTLS: Enabled ... Configure how HTTP and HTTPS requests are handled in the httpProtocol attribute. By default, Knative ingress is configured to serve HTTP traffic ( httpProtocol: Enabled ). Now that your cluster is configured to use TLS certificates and handle HTTPS traffic, you can specify whether or not any HTTP traffic is allowed. Supported httpProtocol values: Enabled : Serve HTTP traffic. Disabled : Rejects all HTTP traffic. Redirected : Responds to HTTP request with a 302 redirect to ask the clients to use HTTPS. ... data: ... autoTLS: Enabled ... Example: apiVersion: v1 kind: ConfigMap metadata: name: config-network namespace: knative-serving data: ... autoTLS: Enabled ... httpProtocol: Redirected ... Note: When using HTTP-01 challenge, httpProtocol field has to be set to Enabled to make sure HTTP-01 challenge requests can be accepted by the cluster. Ensure that the file was updated successfully: kubectl get configmap config-network --namespace knative-serving --output yaml Congratulations! Knative is now configured to obtain and renew TLS certificates. When your TLS certificate is active on your cluster, your Knative services will be able to handle HTTPS traffic. Verify Auto TLS \u00b6 Run the following comand to create a Knative Service: kubectl apply -f https://raw.githubusercontent.com/knative/docs/main/docs/serving/autoscaling/autoscale-go/service.yaml When the certificate is provisioned (which could take up to several minutes depending on the challenge type), you should see something like: NAME URL LATESTCREATED LATESTREADY READY REASON autoscale-go https://autoscale-go.default.{custom-domain} autoscale-go-6jf85 autoscale-go-6jf85 True Note that the URL will be https in this case. Disable Auto TLS per service or route \u00b6 If you have Auto TLS enabled in your cluster, you can choose to disable Auto TLS for individual services or routes by adding the annotation networking.knative.dev/disableAutoTLS: true . Using the previous autoscale-go example: Edit the service using kubectl edit service.serving.knative.dev/autoscale-go -n default and add the annotation: apiVersion : serving.knative.dev/v1 kind : Service metadata : annotations : ... networking.knative.dev/disableAutoTLS : \"true\" ... The service URL should now be http , indicating that AutoTLS is disabled: NAME URL LATEST AGE CONDITIONS READY REASON autoscale-go http://autoscale-go.default.1.arenault.dev autoscale-go-dd42t 8m17s 3 OK / 3 True","title":"Enabling automatic TLS certificate provisioning"},{"location":"serving/using-auto-tls/#enabling-automatic-tls-certificate-provisioning","text":"If you install and configure cert-manager, you can configure Knative to automatically obtain new TLS certificates and renew existing ones for Knative Services. To learn more about using secure connections in Knative, see Configuring HTTPS with TLS certificates .","title":"Enabling automatic TLS certificate provisioning"},{"location":"serving/using-auto-tls/#automatic-tls-provision-mode","text":"Knative supports the following Auto TLS modes: Using DNS-01 challenge In this mode, your cluster needs to be able to talk to your DNS server to verify the ownership of your domain. - Provision Certificate per namespace is supported when using DNS-01 challenge mode. - This is the recommended mode for faster certificate provision. - In this mode, a single Certificate will be provisioned per namespace and is reused across the Knative Services within the same namespace. Provision Certificate per Knative Service is supported when using DNS-01 challenge mode. This is the recommended mode for better certificate islation between Knative Services. In this mode, a Certificate will be provisioned for each Knative Service. The TLS effective time is longer as it needs Certificate provision for each Knative Service creation. Using HTTP-01 challenge In this type, your cluster does not need to be able to talk to your DNS server. You must map your domain to the IP of the cluser ingress. When using HTTP-01 challenge, a certificate will be provisioned per Knative Service. HTTP-01 does not support provisioning a certificate per namespace.","title":"Automatic TLS provision mode"},{"location":"serving/using-auto-tls/#before-you-begin","text":"You must meet the following prerequisites to enable Auto TLS: The following must be installed on your Knative cluster: Knative Serving . A Networking layer such as Kourier, Istio with SDS v1.3 or higher, Contour v1.1 or higher, or Gloo v0.18.16 or higher. See Install a networking layer or Istio with SDS, version 1.3 or higher . Note: Currently, Ambassador is unsupported for use with Auto TLS. cert-manager version 1.0.0 and higher . Your Knative cluster must be configured to use a custom domain . Your DNS provider must be setup and configured to your domain. If you want to use HTTP-01 challenge, you need to configure your custom domain to map to the IP of ingress. You can achieve this by adding a DNS A record to map the domain to the IP according to the instructions of your DNS provider.","title":"Before you begin"},{"location":"serving/using-auto-tls/#enabling-auto-tls","text":"To enable support for Auto TLS in Knative:","title":"Enabling Auto TLS"},{"location":"serving/using-auto-tls/#create-cert-manager-clusterissuer","text":"Create and add the ClusterIssuer configuration file to your Knative cluster to define who issues the TLS certificates, how requests are validated, and which DNS provider validates those requests.","title":"Create cert-manager ClusterIssuer"},{"location":"serving/using-auto-tls/#clusterissuer-for-dns-01-challenge","text":"Use the cert-manager reference to determine how to configure your ClusterIssuer file: - See the generic ClusterIssuer example - Also see the DNS01 example Example : Cloud DNS ClusterIssuer configuration file: The following letsencrypt-issuer named ClusterIssuer file is configured for the Let's Encrypt CA and Google Cloud DNS. Under spec , the Let's Encrypt account info, required DNS-01 challenge type, and Cloud DNS provider info defined. For the complete Google Cloud DNS example, see Configuring HTTPS with cert-manager and Google Cloud DNS . apiVersion: cert-manager.io/v1alpha2 kind: ClusterIssuer metadata: name: letsencrypt-dns-issuer spec: acme: server: https://acme-v02.api.letsencrypt.org/directory # This will register an issuer with LetsEncrypt. Replace # with your admin email address. email: myemail@gmail.com privateKeySecretRef: # Set privateKeySecretRef to any unused secret name. name: letsencrypt-dns-issuer solvers: - dns01: clouddns: # Set this to your GCP project-id project: $PROJECT_ID # Set this to the secret that we publish our service account key # in the previous step. serviceAccountSecretRef: name: cloud-dns-key key: key.json","title":"ClusterIssuer for DNS-01 challenge"},{"location":"serving/using-auto-tls/#clusterissuer-for-http-01-challenge","text":"Run the following command to apply the ClusterIssuer for HTT01 challenge: kubectl apply -f - <<EOF apiVersion: cert-manager.io/v1alpha2 kind: ClusterIssuer metadata: name: letsencrypt-http01-issuer spec: acme: privateKeySecretRef: name: letsencrypt server: https://acme-v02.api.letsencrypt.org/directory solvers: - http01: ingress: class: istio EOF Ensure that the ClusterIssuer is created successfully: kubectl get clusterissuer <cluster-issuer-name> --output yaml Result: The Status.Conditions should include Ready=True .","title":"ClusterIssuer for HTTP-01 challenge"},{"location":"serving/using-auto-tls/#dns-01-challenge-only-configure-your-dns-provider","text":"If you choose to use DNS-01 challenge, configure which DNS provider is used to validate the DNS-01 challenge requests. Instructions about configuring cert-manager, for all the supported DNS providers, are provided in DNS01 challenge providers and configuration instructions . Example: See how the Google Cloud DNS is defined as the provider: Configuring HTTPS with cert-manager and Google Cloud DNS","title":"DNS-01 challenge only: Configure your DNS provider"},{"location":"serving/using-auto-tls/#install-networking-certmanager-deployment","text":"Determine if networking-certmanager is already installed by running the following command: kubectl get deployment networking-certmanager -n knative-serving If networking-certmanager is not found, run the following command: kubectl apply --filename http://github.com/knative/net-certmanager/releases/download/ { 'provider' : 'mike' } /release.yaml","title":"Install networking-certmanager deployment"},{"location":"serving/using-auto-tls/#install-networking-ns-cert-component","text":"If you choose to use the mode of provisioning certificate per namespace, you need to install networking-ns-cert components. IMPORTANT: Provisioning a certificate per namespace only works with DNS-01 challenge. This component cannot be used with HTTP-01 challenge. Determine if networking-ns-cert deployment is already installed by running the following command: kubectl get deployment networking-ns-cert -n knative-serving If networking-ns-cert deployment is not found, run the following command: kubectl apply --filename http://github.com/knative/serving/releases/download/ { 'provider' : 'mike' } /serving-nscert.yaml","title":"Install networking-ns-cert component"},{"location":"serving/using-auto-tls/#configure-config-certmanager-configmap","text":"Update your config-certmanager ConfigMap in the knative-serving namespace to reference your new ClusterIssuer . Run the following command to edit your config-certmanager ConfigMap: kubectl edit configmap config-certmanager --namespace knative-serving Add the issuerRef within the data section: ... data: ... issuerRef: | kind: ClusterIssuer name: letsencrypt-issuer Example: apiVersion: v1 kind: ConfigMap metadata: name: config-certmanager namespace: knative-serving labels: networking.knative.dev/certificate-provider: cert-manager data: issuerRef: | kind: ClusterIssuer name: letsencrypt-http01-issuer issueRef defines which ClusterIssuer will be used by Knative to issue certificates. Ensure that the file was updated successfully: kubectl get configmap config-certmanager --namespace knative-serving --output yaml","title":"Configure config-certmanager ConfigMap"},{"location":"serving/using-auto-tls/#turn-on-auto-tls","text":"Update the config-network ConfigMap in the knative-serving namespace to enable autoTLS and specify how HTTP requests are handled: Run the following command to edit your config-network ConfigMap: kubectl edit configmap config-network --namespace knative-serving Add the autoTLS: Enabled attribute under the data section: ... data: ... autoTLS: Enabled ... Example: apiVersion: v1 kind: ConfigMap metadata: name: config-network namespace: knative-serving data: ... autoTLS: Enabled ... Configure how HTTP and HTTPS requests are handled in the httpProtocol attribute. By default, Knative ingress is configured to serve HTTP traffic ( httpProtocol: Enabled ). Now that your cluster is configured to use TLS certificates and handle HTTPS traffic, you can specify whether or not any HTTP traffic is allowed. Supported httpProtocol values: Enabled : Serve HTTP traffic. Disabled : Rejects all HTTP traffic. Redirected : Responds to HTTP request with a 302 redirect to ask the clients to use HTTPS. ... data: ... autoTLS: Enabled ... Example: apiVersion: v1 kind: ConfigMap metadata: name: config-network namespace: knative-serving data: ... autoTLS: Enabled ... httpProtocol: Redirected ... Note: When using HTTP-01 challenge, httpProtocol field has to be set to Enabled to make sure HTTP-01 challenge requests can be accepted by the cluster. Ensure that the file was updated successfully: kubectl get configmap config-network --namespace knative-serving --output yaml Congratulations! Knative is now configured to obtain and renew TLS certificates. When your TLS certificate is active on your cluster, your Knative services will be able to handle HTTPS traffic.","title":"Turn on Auto TLS"},{"location":"serving/using-auto-tls/#verify-auto-tls","text":"Run the following comand to create a Knative Service: kubectl apply -f https://raw.githubusercontent.com/knative/docs/main/docs/serving/autoscaling/autoscale-go/service.yaml When the certificate is provisioned (which could take up to several minutes depending on the challenge type), you should see something like: NAME URL LATESTCREATED LATESTREADY READY REASON autoscale-go https://autoscale-go.default.{custom-domain} autoscale-go-6jf85 autoscale-go-6jf85 True Note that the URL will be https in this case.","title":"Verify Auto TLS"},{"location":"serving/using-auto-tls/#disable-auto-tls-per-service-or-route","text":"If you have Auto TLS enabled in your cluster, you can choose to disable Auto TLS for individual services or routes by adding the annotation networking.knative.dev/disableAutoTLS: true . Using the previous autoscale-go example: Edit the service using kubectl edit service.serving.knative.dev/autoscale-go -n default and add the annotation: apiVersion : serving.knative.dev/v1 kind : Service metadata : annotations : ... networking.knative.dev/disableAutoTLS : \"true\" ... The service URL should now be http , indicating that AutoTLS is disabled: NAME URL LATEST AGE CONDITIONS READY REASON autoscale-go http://autoscale-go.default.1.arenault.dev autoscale-go-dd42t 8m17s 3 OK / 3 True","title":"Disable Auto TLS per service or route"},{"location":"serving/using-cert-manager-on-gcp/","text":"Configuring HTTPS with cert-manager and Google Cloud DNS \u00b6 You can use cert-manager with Knative to automatically provision TLS certificates from Let's Encrypt and use Google Cloud DNS to handle HTTPS requests and validate DNS challenges. The following guide demonstrates how you can setup Knative to handle secure HTTPS requests on Google Cloud Platform, specifically using cert-manager for TLS certificates and Google Cloud DNS as the DNS provider. Learn more about using TLS certificates in Knative: Configuring HTTPS with TLS certificates Enabling automatic TLS certificate provisioning Before you begin \u00b6 You must meet the following prerequisites to configure Knative with cert-manager and Cloud DNS: You must have a GCP project ID with owner privileges . Google Cloud DNS must set up and configure for your domain. You must have a Knative cluster with the following requirements: Knative Serving running. The Knative cluster must be running on Google Cloud Platform. For details about installing the Serving component, see the Knative installation guides . Your Knative cluster must be configured to use a custom domain . cert-manager v0.6.1 or higher installed Your DNS provider must be setup and configured to your domain. Creating a service account and using a Kubernetes secret \u00b6 To allow cert-manager to access and update the DNS record, you must create a service account in GCP, add the key in a Kubernetes secret, and then add that secret to your Knative cluster. Note that several example names are used in the following commands, for example secret or file names, which can all be changed to your liking. Create a service account in GCP with dns.admin project role by running the following commands, where <your-project-id> is the ID of your GCP project: # Set this to your GCP project ID export PROJECT_ID = <your-project-id> # Name of the service account you want to create. export CLOUD_DNS_SA = cert-manager-cloud-dns-admin gcloud --project $PROJECT_ID iam service-accounts \\ create $CLOUD_DNS_SA \\ --display-name \"Service Account to support ACME DNS-01 challenge.\" # Fully-qualified service account name also has project-id information. export CLOUD_DNS_SA = $CLOUD_DNS_SA @ $PROJECT_ID .iam.gserviceaccount.com # Bind the role dns.admin to this service account, so it can be used to support # the ACME DNS01 challenge. gcloud projects add-iam-policy-binding $PROJECT_ID \\ --member serviceAccount: $CLOUD_DNS_SA \\ --role roles/dns.admin Download the service account key by running the following commands: # Make a temporary directory to store key KEY_DIRECTORY = ` mktemp -d ` # Download the secret key file for your service account. gcloud iam service-accounts keys create $KEY_DIRECTORY /cloud-dns-key.json \\ --iam-account = $CLOUD_DNS_SA Create a Kubernetes secret and then add that secret to your Knative cluster by running the following commands: # Upload that as a secret in your Kubernetes cluster. kubectl create secret --namespace cert-manager generic cloud-dns-key \\ --from-file = key.json = $KEY_DIRECTORY /cloud-dns-key.json # Delete the local secret rm -rf $KEY_DIRECTORY Adding your service account to cert-manager \u00b6 Create a ClusterIssuer configuration file to define how cert-manager obtains TLS certificates and how the requests are validated with Cloud DNS. Run the following command to create the ClusterIssuer configuration. The following creates the letsencrypt-issuer ClusterIssuer , that includes your Let's Encrypt account info, DNS-01 challenge type, and Cloud DNS provider info, including your cert-manager-cloud-dns-admin service account. kubectl apply --filename - <<EOF apiVersion: cert-manager.io/v1alpha2 kind: ClusterIssuer metadata: name: letsencrypt-issuer spec: acme: server: https://acme-v02.api.letsencrypt.org/directory # This will register an issuer with LetsEncrypt. Replace # with your admin email address. email: myemail@gmail.com privateKeySecretRef: # Set privateKeySecretRef to any unused secret name. name: letsencrypt-issuer solvers: - dns01: clouddns: # Set this to your GCP project-id project: $PROJECT_ID # Set this to the secret that we publish our service account key # in the previous step. serviceAccountSecretRef: name: cloud-dns-key key: key.json EOF Ensure that letsencrypt-issuer is created successfully by running the following command: kubectl get clusterissuer --namespace cert-manager letsencrypt-issuer --output yaml Result: The Status.Conditions should include Ready=True . For example: status : acme : uri : https://acme-v02.api.letsencrypt.org/acme/acct/40759665 conditions : - lastTransitionTime : 2018-08-23T01:44:54Z message : The ACME account was registered with the ACME server reason : ACMEAccountRegistered status : \"True\" type : Ready Add letsencrypt-issuer to your ingress secret to configure your certificate \u00b6 To configure how Knative uses your TLS certificates, you create a Certificate to add letsencrypt-issuer to the istio-ingressgateway-certs secret. Note that istio-ingressgateway-certs will be overridden if the secret already exists. Run the following commands to create the my-certificate Certificate , where <your-domain.com> is your domain: # Change this value to the domain you want to use. export DOMAIN = <your-domain.com> kubectl apply --filename - <<EOF apiVersion: cert-manager.io/v1alpha2 kind: Certificate metadata: name: my-certificate namespace: istio-system spec: secretName: istio-ingressgateway-certs issuerRef: name: letsencrypt-issuer kind: ClusterIssuer dnsNames: - \"*.default.$DOMAIN\" - \"*.other-namespace.$DOMAIN\" EOF Ensure that my-certificate is created successfully by running the following command: kubectl get certificate --namespace istio-system my-certificate --output yaml Result: The Status.Conditions should include Ready=True . For example: status : acme : order : url : https://acme-v02.api.letsencrypt.org/acme/order/40759665/45358362 conditions : - lastTransitionTime : 2018-08-23T02:28:44Z message : Certificate issued successfully reason : CertIssued status : \"True\" type : Ready Note: If Status.Conditions is Ready=False , that indicates a failure to obtain a certificate, which should be explained in the accompanying error message. Configuring the Knative ingress gateway \u00b6 To configure the knative-ingress-gateway to use the TLS certificate that you created, append the tls: section to the end of your HTTPS port configuration. Run the following commands to configure Knative to use HTTPS connections and send a 301 redirect response for all HTTP requests: kubectl apply --filename - <<EOF apiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: knative-ingress-gateway namespace: knative-serving spec: selector: istio: ingressgateway servers: - port: number: 80 name: http protocol: HTTP hosts: - \"*\" tls: # Sends 301 redirect for all http requests. # Omit to allow http and https. httpsRedirect: true - port: number: 443 name: https protocol: HTTPS hosts: - \"*\" tls: mode: SIMPLE privateKey: /etc/istio/ingressgateway-certs/tls.key serverCertificate: /etc/istio/ingressgateway-certs/tls.crt EOF Congratulations, you can now access your Knative services with secure HTTPS connections. Your Knative cluster is configured to use cert-manager to manually obtain TLS certificates but see the following section about automating that process. Configure Knative for automatic certificate provisioning \u00b6 You can update your Knative configuration to automatically obtain and renew TLS certificates before they expire. To learn more about automatic certificates, see Enabling automatic TLS certificate provisioning .","title":"Configuring HTTPS with cert-manager and Google Cloud DNS"},{"location":"serving/using-cert-manager-on-gcp/#configuring-https-with-cert-manager-and-google-cloud-dns","text":"You can use cert-manager with Knative to automatically provision TLS certificates from Let's Encrypt and use Google Cloud DNS to handle HTTPS requests and validate DNS challenges. The following guide demonstrates how you can setup Knative to handle secure HTTPS requests on Google Cloud Platform, specifically using cert-manager for TLS certificates and Google Cloud DNS as the DNS provider. Learn more about using TLS certificates in Knative: Configuring HTTPS with TLS certificates Enabling automatic TLS certificate provisioning","title":"Configuring HTTPS with cert-manager and Google Cloud DNS"},{"location":"serving/using-cert-manager-on-gcp/#before-you-begin","text":"You must meet the following prerequisites to configure Knative with cert-manager and Cloud DNS: You must have a GCP project ID with owner privileges . Google Cloud DNS must set up and configure for your domain. You must have a Knative cluster with the following requirements: Knative Serving running. The Knative cluster must be running on Google Cloud Platform. For details about installing the Serving component, see the Knative installation guides . Your Knative cluster must be configured to use a custom domain . cert-manager v0.6.1 or higher installed Your DNS provider must be setup and configured to your domain.","title":"Before you begin"},{"location":"serving/using-cert-manager-on-gcp/#creating-a-service-account-and-using-a-kubernetes-secret","text":"To allow cert-manager to access and update the DNS record, you must create a service account in GCP, add the key in a Kubernetes secret, and then add that secret to your Knative cluster. Note that several example names are used in the following commands, for example secret or file names, which can all be changed to your liking. Create a service account in GCP with dns.admin project role by running the following commands, where <your-project-id> is the ID of your GCP project: # Set this to your GCP project ID export PROJECT_ID = <your-project-id> # Name of the service account you want to create. export CLOUD_DNS_SA = cert-manager-cloud-dns-admin gcloud --project $PROJECT_ID iam service-accounts \\ create $CLOUD_DNS_SA \\ --display-name \"Service Account to support ACME DNS-01 challenge.\" # Fully-qualified service account name also has project-id information. export CLOUD_DNS_SA = $CLOUD_DNS_SA @ $PROJECT_ID .iam.gserviceaccount.com # Bind the role dns.admin to this service account, so it can be used to support # the ACME DNS01 challenge. gcloud projects add-iam-policy-binding $PROJECT_ID \\ --member serviceAccount: $CLOUD_DNS_SA \\ --role roles/dns.admin Download the service account key by running the following commands: # Make a temporary directory to store key KEY_DIRECTORY = ` mktemp -d ` # Download the secret key file for your service account. gcloud iam service-accounts keys create $KEY_DIRECTORY /cloud-dns-key.json \\ --iam-account = $CLOUD_DNS_SA Create a Kubernetes secret and then add that secret to your Knative cluster by running the following commands: # Upload that as a secret in your Kubernetes cluster. kubectl create secret --namespace cert-manager generic cloud-dns-key \\ --from-file = key.json = $KEY_DIRECTORY /cloud-dns-key.json # Delete the local secret rm -rf $KEY_DIRECTORY","title":"Creating a service account and using a Kubernetes secret"},{"location":"serving/using-cert-manager-on-gcp/#adding-your-service-account-to-cert-manager","text":"Create a ClusterIssuer configuration file to define how cert-manager obtains TLS certificates and how the requests are validated with Cloud DNS. Run the following command to create the ClusterIssuer configuration. The following creates the letsencrypt-issuer ClusterIssuer , that includes your Let's Encrypt account info, DNS-01 challenge type, and Cloud DNS provider info, including your cert-manager-cloud-dns-admin service account. kubectl apply --filename - <<EOF apiVersion: cert-manager.io/v1alpha2 kind: ClusterIssuer metadata: name: letsencrypt-issuer spec: acme: server: https://acme-v02.api.letsencrypt.org/directory # This will register an issuer with LetsEncrypt. Replace # with your admin email address. email: myemail@gmail.com privateKeySecretRef: # Set privateKeySecretRef to any unused secret name. name: letsencrypt-issuer solvers: - dns01: clouddns: # Set this to your GCP project-id project: $PROJECT_ID # Set this to the secret that we publish our service account key # in the previous step. serviceAccountSecretRef: name: cloud-dns-key key: key.json EOF Ensure that letsencrypt-issuer is created successfully by running the following command: kubectl get clusterissuer --namespace cert-manager letsencrypt-issuer --output yaml Result: The Status.Conditions should include Ready=True . For example: status : acme : uri : https://acme-v02.api.letsencrypt.org/acme/acct/40759665 conditions : - lastTransitionTime : 2018-08-23T01:44:54Z message : The ACME account was registered with the ACME server reason : ACMEAccountRegistered status : \"True\" type : Ready","title":"Adding your service account to cert-manager"},{"location":"serving/using-cert-manager-on-gcp/#add-letsencrypt-issuer-to-your-ingress-secret-to-configure-your-certificate","text":"To configure how Knative uses your TLS certificates, you create a Certificate to add letsencrypt-issuer to the istio-ingressgateway-certs secret. Note that istio-ingressgateway-certs will be overridden if the secret already exists. Run the following commands to create the my-certificate Certificate , where <your-domain.com> is your domain: # Change this value to the domain you want to use. export DOMAIN = <your-domain.com> kubectl apply --filename - <<EOF apiVersion: cert-manager.io/v1alpha2 kind: Certificate metadata: name: my-certificate namespace: istio-system spec: secretName: istio-ingressgateway-certs issuerRef: name: letsencrypt-issuer kind: ClusterIssuer dnsNames: - \"*.default.$DOMAIN\" - \"*.other-namespace.$DOMAIN\" EOF Ensure that my-certificate is created successfully by running the following command: kubectl get certificate --namespace istio-system my-certificate --output yaml Result: The Status.Conditions should include Ready=True . For example: status : acme : order : url : https://acme-v02.api.letsencrypt.org/acme/order/40759665/45358362 conditions : - lastTransitionTime : 2018-08-23T02:28:44Z message : Certificate issued successfully reason : CertIssued status : \"True\" type : Ready Note: If Status.Conditions is Ready=False , that indicates a failure to obtain a certificate, which should be explained in the accompanying error message.","title":"Add letsencrypt-issuer to your ingress secret to configure your certificate"},{"location":"serving/using-cert-manager-on-gcp/#configuring-the-knative-ingress-gateway","text":"To configure the knative-ingress-gateway to use the TLS certificate that you created, append the tls: section to the end of your HTTPS port configuration. Run the following commands to configure Knative to use HTTPS connections and send a 301 redirect response for all HTTP requests: kubectl apply --filename - <<EOF apiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: knative-ingress-gateway namespace: knative-serving spec: selector: istio: ingressgateway servers: - port: number: 80 name: http protocol: HTTP hosts: - \"*\" tls: # Sends 301 redirect for all http requests. # Omit to allow http and https. httpsRedirect: true - port: number: 443 name: https protocol: HTTPS hosts: - \"*\" tls: mode: SIMPLE privateKey: /etc/istio/ingressgateway-certs/tls.key serverCertificate: /etc/istio/ingressgateway-certs/tls.crt EOF Congratulations, you can now access your Knative services with secure HTTPS connections. Your Knative cluster is configured to use cert-manager to manually obtain TLS certificates but see the following section about automating that process.","title":"Configuring the Knative ingress gateway"},{"location":"serving/using-cert-manager-on-gcp/#configure-knative-for-automatic-certificate-provisioning","text":"You can update your Knative configuration to automatically obtain and renew TLS certificates before they expire. To learn more about automatic certificates, see Enabling automatic TLS certificate provisioning .","title":"Configure Knative for automatic certificate provisioning"},{"location":"serving/using-external-dns-on-gcp/","text":"Using ExternalDNS on Google Cloud Platform to automate DNS setup \u00b6 ExternalDNS is a tool that synchronizes exposed Kubernetes Services and Ingresses with DNS providers. This doc explains how to set up ExternalDNS within a Knative cluster using Google Cloud DNS to automate the process of publishing the Knative domain. Set up environtment variables \u00b6 Run the following command to configure the environment variables export PROJECT_NAME = <your-google-cloud-project-name> export CUSTOM_DOMAIN = <your-custom-domain-used-in-knative> export CLUSTER_NAME = <knative-cluster-name> export CLUSTER_ZONE = <knative-cluster-zone> Set up Kubernetes Engine cluster with CloudDNS read/write permissions \u00b6 There are two ways to set up a Kubernetes Engine cluster with CloudDNS read/write permissions. Cluster with Cloud DNS scope \u00b6 You can create a GKE cluster with Cloud DNS scope by entering the following command: gcloud container clusters create $CLUSTER_NAME \\ --zone = $CLUSTER_ZONE \\ --cluster-version = latest \\ --machine-type = n1-standard-4 \\ --enable-autoscaling --min-nodes = 1 --max-nodes = 10 \\ --enable-autorepair \\ --scopes = service-control,service-management,compute-rw,storage-ro,cloud-platform,logging-write,monitoring-write,pubsub,datastore, \"https://www.googleapis.com/auth/ndev.clouddns.readwrite\" \\ --num-nodes = 3 Note that by using this way, any pod within the cluster will have permissions to read/write CloudDNS. Cluster with Cloud DNS Admin Service Account credential \u00b6 Create a GKE cluster without Cloud DNS scope by entering the following command: gcloud container clusters create $CLUSTER_NAME \\ --zone = $CLUSTER_ZONE \\ --cluster-version = latest \\ --machine-type = n1-standard-4 \\ --enable-autoscaling --min-nodes = 1 --max-nodes = 10 \\ --enable-autorepair \\ --scopes = service-control,service-management,compute-rw,storage-ro,cloud-platform,logging-write,monitoring-write,pubsub,datastore \\ --num-nodes = 3 Create a new service account for Cloud DNS admin role. # Name of the service account you want to create. export CLOUD_DNS_SA = cloud-dns-admin gcloud --project $PROJECT_NAME iam service-accounts \\ create $CLOUD_DNS_SA \\ --display-name \"Service Account to support ACME DNS-01 challenge.\" Bind the role dns.admin to the newly created service account. # Fully-qualified service account name also has project-id information. export CLOUD_DNS_SA = $CLOUD_DNS_SA @ $PROJECT_NAME .iam.gserviceaccount.com gcloud projects add-iam-policy-binding $PROJECT_NAME \\ --member serviceAccount: $CLOUD_DNS_SA \\ --role roles/dns.admin Download the secret key file for your service account. gcloud iam service-accounts keys create ~/key.json \\ --iam-account = $CLOUD_DNS_SA Upload the service account credential to your cluster. This command uses the secret name cloud-dns-key , but you can choose a different name. kubectl create secret generic cloud-dns-key \\ --from-file = key.json = $HOME /key.json Delete the local secret rm ~/key.json Now your cluster has the credential of your CloudDNS admin service account. And it can be used to access your Cloud DNS. You can enforce the access of the credentail secret within your cluster, so that only the pods that have the permission to get the credential secret can access your Cloud DNS. Set up Knative \u00b6 Follow the instruction to install Knative on your cluster. Configure Knative to use your custom domain. kubectl edit cm config-domain --namespace knative-serving This command opens your default text editor and allows you to edit the config map. apiVersion: v1 data: example.com: \"\" kind: ConfigMap [...] Edit the file to replace example.com with your custom domain (the value of $CUSTOM_DOMAIN ) and save your changes. In this example, we use domain external-dns-test.my-org.do for all routes: apiVersion: v1 data: external-dns-test.my-org.do: \"\" kind: ConfigMap [...] Set up ExternalDNS \u00b6 This guide uses Google Cloud Platform as an example to show how to set up ExternalDNS. You can find detailed instructions for other cloud providers in the ExternalDNS documentation . Create a DNS zone for managing DNS records \u00b6 Skip this step if you already have a zone for managing the DNS records of your custom domain. A DNS zone which will contain the managed DNS records needs to be created. Use the following command to create a DNS zone with Google Cloud DNS : export DNS_ZONE_NAME = <dns-zone-name> gcloud dns managed-zones create $DNS_ZONE_NAME \\ --dns-name $CUSTOM_DOMAIN \\ --description \"Automatically managed zone by kubernetes.io/external-dns\" Make a note of the nameservers that were assigned to your new zone. gcloud dns record-sets list \\ --zone $DNS_ZONE_NAME \\ --name $CUSTOM_DOMAIN \\ --type NS You should see output similar to the following assuming your custom domain is external-dns-test.my-org.do : NAME TYPE TTL DATA external-dns-test.my-org.do. NS 21600 ns-cloud-e1.googledomains.com.,ns-cloud-e2.googledomains.com.,ns-cloud-e3.googledomains.com.,ns-cloud-e4.googledomains.com. In this case, the DNS nameservers are ns-cloud-{e1-e4}.googledomains.com . Yours could differ slightly, e.g. {a1-a4}, {b1-b4} etc. If this zone has the parent zone, you need to add NS records of this zone into the parent zone so that this zone can be found from the parent. Assuming the parent zone is my-org-do and the parent domain is my-org.do , and the parent zone is also hosted at Google Cloud DNS, you can follow these steps to add the NS records of this zone into the parent zone: gcloud dns record-sets transaction start --zone \"my-org-do\" gcloud dns record-sets transaction add ns-cloud-e { 1 ..4 } .googledomains.com. \\ --name \"external-dns-test.my-org.do.\" --ttl 300 --type NS --zone \"my-org-do\" gcloud dns record-sets transaction execute --zone \"my-org-do\" Deploy ExternalDNS \u00b6 Firstly, choose the manifest of ExternalDNS. Use below manifest if you set up your cluster with CloudDNS scope . apiVersion : v1 kind : ServiceAccount metadata : name : external-dns --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : external-dns rules : - apiGroups : [ \"\" ] resources : [ \"services\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"\" ] resources : [ \"pods\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"extensions\" ] resources : [ \"ingresses\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : external-dns-viewer roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : external-dns subjects : - kind : ServiceAccount name : external-dns namespace : default --- apiVersion : extensions/v1beta1 kind : Deployment metadata : name : external-dns spec : strategy : type : Recreate template : metadata : labels : app : external-dns spec : serviceAccountName : external-dns containers : - name : external-dns image : registry.opensource.zalan.do/teapot/external-dns:latest args : - --source=service - --domain-filter=$CUSTOM_DOMAIN # will make ExternalDNS see only the hosted zones matching provided domain, omit to process all available hosted zones - --provider=google - --google-project=$PROJECT_NAME # Use this to specify a project different from the one external-dns is running inside - --policy=sync # would prevent ExternalDNS from deleting any records, omit to enable full synchronization - --registry=txt - --txt-owner-id=my-identifier Or use below manifest if you set up your cluster with CloudDNS service account credential . apiVersion : v1 kind : ServiceAccount metadata : name : external-dns --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : external-dns rules : - apiGroups : [ \"\" ] resources : [ \"services\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"\" ] resources : [ \"pods,secrets\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"extensions\" ] resources : [ \"ingresses\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : external-dns-viewer roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : external-dns subjects : - kind : ServiceAccount name : external-dns namespace : default --- apiVersion : extensions/v1beta1 kind : Deployment metadata : name : external-dns spec : strategy : type : Recreate template : metadata : labels : app : external-dns spec : volumes : - name : google-cloud-key secret : secretName : cloud-dns-key serviceAccountName : external-dns containers : - name : external-dns image : registry.opensource.zalan.do/teapot/external-dns:latest volumeMounts : - name : google-cloud-key mountPath : /var/secrets/google env : - name : GOOGLE_APPLICATION_CREDENTIALS value : /var/secrets/google/key.json args : - --source=service - --domain-filter=$CUSTOM_DOMAIN # will make ExternalDNS see only the hosted zones matching provided domain, omit to process all available hosted zones - --provider=google - --google-project=$PROJECT_NAME # Use this to specify a project different from the one external-dns is running inside - --policy=sync # would prevent ExternalDNS from deleting any records, omit to enable full synchronization - --registry=txt - --txt-owner-id=my-identifier Then use the following command to apply the manifest you chose to install ExternalDNS cat <<EOF | kubectl apply --filename - <your-chosen-manifest> EOF You should see ExternalDNS is installed by running: kubectl get deployment external-dns Configuring Knative Gateway service \u00b6 In order to publish the Knative Gateway service, the annotation external-dns.alpha.kubernetes.io/hostname: '*.$CUSTOM_DOMAIN needs to be added into Knative gateway service: INGRESSGATEWAY = istio-ingressgateway kubectl edit svc $INGRESSGATEWAY --namespace istio-system This command opens your default text editor and allows you to add the annotation to istio-ingressgateway service. After you've added your annotation, your file may look similar to this (assuming your custom domain is external-dns-test.my-org.do ): apiVersion: v1 kind: Service metadata: annotations: external-dns.alpha.kubernetes.io/hostname: '*.external-dns-test.my-org.do' ... Verify ExternalDNS works \u00b6 After roughly two minutes, check that a corresponding DNS record for your service was created. gcloud dns record-sets list --zone $DNS_ZONE_NAME --name \"*. $CUSTOM_DOMAIN .\" You should see output similar to: NAME TYPE TTL DATA *.external-dns-test.my-org.do. A 300 35.231.248.30 *.external-dns-test.my-org.do. TXT 300 \"heritage=external-dns,external-dns/owner=my-identifier,external-dns/resource=service/istio-system/istio-ingressgateway\" Verify domain has been published \u00b6 You can check if the domain has been published to the Internet be entering the following command: host test.external-dns-test.my-org.do You should see the below result after the domain is published: test.external-dns-test.my-org.do has address 35.231.248.30 Note: The process of publishing the domain to the Internet can take several minutes.","title":"Using ExternalDNS on Google Cloud Platform to automate DNS setup"},{"location":"serving/using-external-dns-on-gcp/#using-externaldns-on-google-cloud-platform-to-automate-dns-setup","text":"ExternalDNS is a tool that synchronizes exposed Kubernetes Services and Ingresses with DNS providers. This doc explains how to set up ExternalDNS within a Knative cluster using Google Cloud DNS to automate the process of publishing the Knative domain.","title":"Using ExternalDNS on Google Cloud Platform to automate DNS setup"},{"location":"serving/using-external-dns-on-gcp/#set-up-environtment-variables","text":"Run the following command to configure the environment variables export PROJECT_NAME = <your-google-cloud-project-name> export CUSTOM_DOMAIN = <your-custom-domain-used-in-knative> export CLUSTER_NAME = <knative-cluster-name> export CLUSTER_ZONE = <knative-cluster-zone>","title":"Set up environtment variables"},{"location":"serving/using-external-dns-on-gcp/#set-up-kubernetes-engine-cluster-with-clouddns-readwrite-permissions","text":"There are two ways to set up a Kubernetes Engine cluster with CloudDNS read/write permissions.","title":"Set up Kubernetes Engine cluster with CloudDNS read/write permissions"},{"location":"serving/using-external-dns-on-gcp/#cluster-with-cloud-dns-scope","text":"You can create a GKE cluster with Cloud DNS scope by entering the following command: gcloud container clusters create $CLUSTER_NAME \\ --zone = $CLUSTER_ZONE \\ --cluster-version = latest \\ --machine-type = n1-standard-4 \\ --enable-autoscaling --min-nodes = 1 --max-nodes = 10 \\ --enable-autorepair \\ --scopes = service-control,service-management,compute-rw,storage-ro,cloud-platform,logging-write,monitoring-write,pubsub,datastore, \"https://www.googleapis.com/auth/ndev.clouddns.readwrite\" \\ --num-nodes = 3 Note that by using this way, any pod within the cluster will have permissions to read/write CloudDNS.","title":"Cluster with Cloud DNS scope"},{"location":"serving/using-external-dns-on-gcp/#cluster-with-cloud-dns-admin-service-account-credential","text":"Create a GKE cluster without Cloud DNS scope by entering the following command: gcloud container clusters create $CLUSTER_NAME \\ --zone = $CLUSTER_ZONE \\ --cluster-version = latest \\ --machine-type = n1-standard-4 \\ --enable-autoscaling --min-nodes = 1 --max-nodes = 10 \\ --enable-autorepair \\ --scopes = service-control,service-management,compute-rw,storage-ro,cloud-platform,logging-write,monitoring-write,pubsub,datastore \\ --num-nodes = 3 Create a new service account for Cloud DNS admin role. # Name of the service account you want to create. export CLOUD_DNS_SA = cloud-dns-admin gcloud --project $PROJECT_NAME iam service-accounts \\ create $CLOUD_DNS_SA \\ --display-name \"Service Account to support ACME DNS-01 challenge.\" Bind the role dns.admin to the newly created service account. # Fully-qualified service account name also has project-id information. export CLOUD_DNS_SA = $CLOUD_DNS_SA @ $PROJECT_NAME .iam.gserviceaccount.com gcloud projects add-iam-policy-binding $PROJECT_NAME \\ --member serviceAccount: $CLOUD_DNS_SA \\ --role roles/dns.admin Download the secret key file for your service account. gcloud iam service-accounts keys create ~/key.json \\ --iam-account = $CLOUD_DNS_SA Upload the service account credential to your cluster. This command uses the secret name cloud-dns-key , but you can choose a different name. kubectl create secret generic cloud-dns-key \\ --from-file = key.json = $HOME /key.json Delete the local secret rm ~/key.json Now your cluster has the credential of your CloudDNS admin service account. And it can be used to access your Cloud DNS. You can enforce the access of the credentail secret within your cluster, so that only the pods that have the permission to get the credential secret can access your Cloud DNS.","title":"Cluster with Cloud DNS Admin Service Account credential"},{"location":"serving/using-external-dns-on-gcp/#set-up-knative","text":"Follow the instruction to install Knative on your cluster. Configure Knative to use your custom domain. kubectl edit cm config-domain --namespace knative-serving This command opens your default text editor and allows you to edit the config map. apiVersion: v1 data: example.com: \"\" kind: ConfigMap [...] Edit the file to replace example.com with your custom domain (the value of $CUSTOM_DOMAIN ) and save your changes. In this example, we use domain external-dns-test.my-org.do for all routes: apiVersion: v1 data: external-dns-test.my-org.do: \"\" kind: ConfigMap [...]","title":"Set up Knative"},{"location":"serving/using-external-dns-on-gcp/#set-up-externaldns","text":"This guide uses Google Cloud Platform as an example to show how to set up ExternalDNS. You can find detailed instructions for other cloud providers in the ExternalDNS documentation .","title":"Set up ExternalDNS"},{"location":"serving/using-external-dns-on-gcp/#create-a-dns-zone-for-managing-dns-records","text":"Skip this step if you already have a zone for managing the DNS records of your custom domain. A DNS zone which will contain the managed DNS records needs to be created. Use the following command to create a DNS zone with Google Cloud DNS : export DNS_ZONE_NAME = <dns-zone-name> gcloud dns managed-zones create $DNS_ZONE_NAME \\ --dns-name $CUSTOM_DOMAIN \\ --description \"Automatically managed zone by kubernetes.io/external-dns\" Make a note of the nameservers that were assigned to your new zone. gcloud dns record-sets list \\ --zone $DNS_ZONE_NAME \\ --name $CUSTOM_DOMAIN \\ --type NS You should see output similar to the following assuming your custom domain is external-dns-test.my-org.do : NAME TYPE TTL DATA external-dns-test.my-org.do. NS 21600 ns-cloud-e1.googledomains.com.,ns-cloud-e2.googledomains.com.,ns-cloud-e3.googledomains.com.,ns-cloud-e4.googledomains.com. In this case, the DNS nameservers are ns-cloud-{e1-e4}.googledomains.com . Yours could differ slightly, e.g. {a1-a4}, {b1-b4} etc. If this zone has the parent zone, you need to add NS records of this zone into the parent zone so that this zone can be found from the parent. Assuming the parent zone is my-org-do and the parent domain is my-org.do , and the parent zone is also hosted at Google Cloud DNS, you can follow these steps to add the NS records of this zone into the parent zone: gcloud dns record-sets transaction start --zone \"my-org-do\" gcloud dns record-sets transaction add ns-cloud-e { 1 ..4 } .googledomains.com. \\ --name \"external-dns-test.my-org.do.\" --ttl 300 --type NS --zone \"my-org-do\" gcloud dns record-sets transaction execute --zone \"my-org-do\"","title":"Create a DNS zone for managing DNS records"},{"location":"serving/using-external-dns-on-gcp/#deploy-externaldns","text":"Firstly, choose the manifest of ExternalDNS. Use below manifest if you set up your cluster with CloudDNS scope . apiVersion : v1 kind : ServiceAccount metadata : name : external-dns --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : external-dns rules : - apiGroups : [ \"\" ] resources : [ \"services\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"\" ] resources : [ \"pods\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"extensions\" ] resources : [ \"ingresses\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : external-dns-viewer roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : external-dns subjects : - kind : ServiceAccount name : external-dns namespace : default --- apiVersion : extensions/v1beta1 kind : Deployment metadata : name : external-dns spec : strategy : type : Recreate template : metadata : labels : app : external-dns spec : serviceAccountName : external-dns containers : - name : external-dns image : registry.opensource.zalan.do/teapot/external-dns:latest args : - --source=service - --domain-filter=$CUSTOM_DOMAIN # will make ExternalDNS see only the hosted zones matching provided domain, omit to process all available hosted zones - --provider=google - --google-project=$PROJECT_NAME # Use this to specify a project different from the one external-dns is running inside - --policy=sync # would prevent ExternalDNS from deleting any records, omit to enable full synchronization - --registry=txt - --txt-owner-id=my-identifier Or use below manifest if you set up your cluster with CloudDNS service account credential . apiVersion : v1 kind : ServiceAccount metadata : name : external-dns --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRole metadata : name : external-dns rules : - apiGroups : [ \"\" ] resources : [ \"services\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"\" ] resources : [ \"pods,secrets\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"extensions\" ] resources : [ \"ingresses\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] - apiGroups : [ \"\" ] resources : [ \"nodes\" ] verbs : [ \"list\" ] --- apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : external-dns-viewer roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : external-dns subjects : - kind : ServiceAccount name : external-dns namespace : default --- apiVersion : extensions/v1beta1 kind : Deployment metadata : name : external-dns spec : strategy : type : Recreate template : metadata : labels : app : external-dns spec : volumes : - name : google-cloud-key secret : secretName : cloud-dns-key serviceAccountName : external-dns containers : - name : external-dns image : registry.opensource.zalan.do/teapot/external-dns:latest volumeMounts : - name : google-cloud-key mountPath : /var/secrets/google env : - name : GOOGLE_APPLICATION_CREDENTIALS value : /var/secrets/google/key.json args : - --source=service - --domain-filter=$CUSTOM_DOMAIN # will make ExternalDNS see only the hosted zones matching provided domain, omit to process all available hosted zones - --provider=google - --google-project=$PROJECT_NAME # Use this to specify a project different from the one external-dns is running inside - --policy=sync # would prevent ExternalDNS from deleting any records, omit to enable full synchronization - --registry=txt - --txt-owner-id=my-identifier Then use the following command to apply the manifest you chose to install ExternalDNS cat <<EOF | kubectl apply --filename - <your-chosen-manifest> EOF You should see ExternalDNS is installed by running: kubectl get deployment external-dns","title":"Deploy ExternalDNS"},{"location":"serving/using-external-dns-on-gcp/#configuring-knative-gateway-service","text":"In order to publish the Knative Gateway service, the annotation external-dns.alpha.kubernetes.io/hostname: '*.$CUSTOM_DOMAIN needs to be added into Knative gateway service: INGRESSGATEWAY = istio-ingressgateway kubectl edit svc $INGRESSGATEWAY --namespace istio-system This command opens your default text editor and allows you to add the annotation to istio-ingressgateway service. After you've added your annotation, your file may look similar to this (assuming your custom domain is external-dns-test.my-org.do ): apiVersion: v1 kind: Service metadata: annotations: external-dns.alpha.kubernetes.io/hostname: '*.external-dns-test.my-org.do' ...","title":"Configuring Knative Gateway service"},{"location":"serving/using-external-dns-on-gcp/#verify-externaldns-works","text":"After roughly two minutes, check that a corresponding DNS record for your service was created. gcloud dns record-sets list --zone $DNS_ZONE_NAME --name \"*. $CUSTOM_DOMAIN .\" You should see output similar to: NAME TYPE TTL DATA *.external-dns-test.my-org.do. A 300 35.231.248.30 *.external-dns-test.my-org.do. TXT 300 \"heritage=external-dns,external-dns/owner=my-identifier,external-dns/resource=service/istio-system/istio-ingressgateway\"","title":"Verify ExternalDNS works"},{"location":"serving/using-external-dns-on-gcp/#verify-domain-has-been-published","text":"You can check if the domain has been published to the Internet be entering the following command: host test.external-dns-test.my-org.do You should see the below result after the domain is published: test.external-dns-test.my-org.do has address 35.231.248.30 Note: The process of publishing the domain to the Internet can take several minutes.","title":"Verify domain has been published"},{"location":"serving/using-subroutes/","text":"Creating and using Subroutes \u00b6 Subroutes are most effective when used with multiple revisions. When defining a Knative service/route, the traffic section of the spec can split between the different revisions. For example: traffic : - percent : 0 revisionName : foo - percent : 40 revisionName : bar - percent : 60 revisionName : baz This allows anyone targeting the main route to have a 0% chance of hitting revision foo , 40% chance of hitting revision bar and 60% chance of hitting revision baz . Using tags to create target URLs \u00b6 The spec defines an attribute called tag . When a tag is applied to a route, an address for the specific traffic target is created. traffic : - percent : 0 revisionName : foo tag : staging - percent : 40 revisionName : bar - percent : 60 revisionName : baz In the above example, you can access the staging target by accessing staging-<route name>.<namespace>.<domain> . The targets for bar and baz can only be accessed using the main route, <route name>.<namespace>.<domain> . When a traffic target gets tagged, a new Kubernetes service is created for it so that other services can also access it within the cluster. From the above example, a new Kubernetes service called staging-<route name> will be created in the same namespace. This service has the ability to override the visibility of this specific route by applying the label networking.knative.dev/visibility with value cluster-local . See cluster local routes for more information about how to restrict visibility on the specific route.","title":"Creating and Using Sub-Routes"},{"location":"serving/using-subroutes/#creating-and-using-subroutes","text":"Subroutes are most effective when used with multiple revisions. When defining a Knative service/route, the traffic section of the spec can split between the different revisions. For example: traffic : - percent : 0 revisionName : foo - percent : 40 revisionName : bar - percent : 60 revisionName : baz This allows anyone targeting the main route to have a 0% chance of hitting revision foo , 40% chance of hitting revision bar and 60% chance of hitting revision baz .","title":"Creating and using Subroutes"},{"location":"serving/using-subroutes/#using-tags-to-create-target-urls","text":"The spec defines an attribute called tag . When a tag is applied to a route, an address for the specific traffic target is created. traffic : - percent : 0 revisionName : foo tag : staging - percent : 40 revisionName : bar - percent : 60 revisionName : baz In the above example, you can access the staging target by accessing staging-<route name>.<namespace>.<domain> . The targets for bar and baz can only be accessed using the main route, <route name>.<namespace>.<domain> . When a traffic target gets tagged, a new Kubernetes service is created for it so that other services can also access it within the cluster. From the above example, a new Kubernetes service called staging-<route name> will be created in the same namespace. This service has the ability to override the visibility of this specific route by applying the label networking.knative.dev/visibility with value cluster-local . See cluster local routes for more information about how to restrict visibility on the specific route.","title":"Using tags to create target URLs"},{"location":"serving/webhook-customizations/","text":"Exclude namespaces from the Knative webhook \u00b6 The Knative webhook examines resources that are created, read, updated, or deleted. This includes system namespaces, which can cause issues during an upgrade if the webhook becomes non-responsive. Cluster administrators may want to disable the Knative webhook on system namespaces to prevent issues during upgrades. You can configure the label webhooks.knative.dev/exclude to allow namespaces to bypass the Knative webhook. apiVersion : v1 kind : Namespace metadata : name : knative-dev labels : webhooks.knative.dev/exclude : \"true\"","title":"Exclude namespaces from the Knative webhook"},{"location":"serving/webhook-customizations/#exclude-namespaces-from-the-knative-webhook","text":"The Knative webhook examines resources that are created, read, updated, or deleted. This includes system namespaces, which can cause issues during an upgrade if the webhook becomes non-responsive. Cluster administrators may want to disable the Knative webhook on system namespaces to prevent issues during upgrades. You can configure the label webhooks.knative.dev/exclude to allow namespaces to bypass the Knative webhook. apiVersion : v1 kind : Namespace metadata : name : knative-dev labels : webhooks.knative.dev/exclude : \"true\"","title":"Exclude namespaces from the Knative webhook"},{"location":"serving/autoscaling/","text":"One of the main features of Knative is automatic scaling of replicas for an application to closely match incoming demand, including scaling applications to zero if no traffic is being received. Knative Serving enables this by default, using the Knative Pod Autoscaler (KPA). The Autoscaler component watches traffic flow to the application, and scales replicas up or down based on configured metrics. Knative services default to using autoscaling settings that are suitable for the majority of use cases. However, some workloads may require a custom, more finely-tuned configuration. This guide provides information about configuration options that you can modify to fit the requirements of your workload. For more information about how autoscaling for Knative works, see the Autoscaling concepts documentation. For more information about which metrics can be used to control the Autoscaler, see the metrics documentation. Optional autoscaling configuration tasks \u00b6 Configure your Knative deployment to use the Kubernetes Horizontal Pod Autoscaler (HPA) instead of the default KPA. For how to install HPA, see Install optional Eventing extensions . Disable scale to zero functionality for your cluster ( global configuration only ). Configure the type of metrics your Autoscaler consumes. Configure concurrency limits for applications. Try out the Go Autoscale Sample App .","title":"Overview"},{"location":"serving/autoscaling/#optional-autoscaling-configuration-tasks","text":"Configure your Knative deployment to use the Kubernetes Horizontal Pod Autoscaler (HPA) instead of the default KPA. For how to install HPA, see Install optional Eventing extensions . Disable scale to zero functionality for your cluster ( global configuration only ). Configure the type of metrics your Autoscaler consumes. Configure concurrency limits for applications. Try out the Go Autoscale Sample App .","title":"Optional autoscaling configuration tasks"},{"location":"serving/autoscaling/_index/","text":"Macro Rendering Error \u00b6 TemplateSyntaxError : unexpected '%' Traceback (most recent call last): File \"/usr/local/lib/python3.9/site-packages/mkdocs_macros/plugin.py\", line 441, in render md_template = self.env.from_string(markdown) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 941, in from_string return cls.from_code(self, self.compile(source), globals, None) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 638, in compile self.handle_exception(source=source_hint) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 832, in handle_exception reraise(*rewrite_traceback_stack(source=source)) File \"/usr/local/lib/python3.9/site-packages/jinja2/_compat.py\", line 28, in reraise raise value.with_traceback(tb) File \"<unknown>\", line 3, in template jinja2.exceptions.TemplateSyntaxError: unexpected '%'","title":"Autoscaling"},{"location":"serving/autoscaling/_index/#macro-rendering-error","text":"TemplateSyntaxError : unexpected '%' Traceback (most recent call last): File \"/usr/local/lib/python3.9/site-packages/mkdocs_macros/plugin.py\", line 441, in render md_template = self.env.from_string(markdown) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 941, in from_string return cls.from_code(self, self.compile(source), globals, None) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 638, in compile self.handle_exception(source=source_hint) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 832, in handle_exception reraise(*rewrite_traceback_stack(source=source)) File \"/usr/local/lib/python3.9/site-packages/jinja2/_compat.py\", line 28, in reraise raise value.with_traceback(tb) File \"<unknown>\", line 3, in template jinja2.exceptions.TemplateSyntaxError: unexpected '%'","title":"Macro Rendering Error"},{"location":"serving/autoscaling/autoscaling-concepts/","text":"Autoscaling concepts \u00b6 This section covers conceptual information about which Autoscaler types are supported, as well as fundamental information about how autoscaling is configured. Supported Autoscaler types \u00b6 Knative Serving supports the implementation of Knative Pod Autoscaler (KPA) and Kubernetes' Horizontal Pod Autoscaler (HPA). The features and limitations of each of these Autoscalers are listed below. IMPORTANT If you want to use Kubernetes Horizontal Pod Autoscaler (HPA), you must install it after you install Knative Serving. For how to install HPA, see Install optional Eventing extensions . Knative Pod Autoscaler (KPA) \u00b6 Part of the Knative Serving core and enabled by default once Knative Serving is installed. Supports scale to zero functionality. Does not support CPU-based autoscaling. Horizontal Pod Autoscaler (HPA) \u00b6 Not part of the Knative Serving core, and you must install Knative Serving first. Does not support scale to zero functionality. Supports CPU-based autoscaling. Configuring the Autoscaler implementation \u00b6 The type of Autoscaler implementation (KPA or HPA) can be configured by using the class annotation. Global settings key: pod-autoscaler-class Per-revision annotation key: autoscaling.knative.dev/class Possible values: \"kpa.autoscaling.knative.dev\" or \"hpa.autoscaling.knative.dev\" Default: \"kpa.autoscaling.knative.dev\" Example: Per Revision apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/class : \"kpa.autoscaling.knative.dev\" spec : containers : - image : gcr.io/knative-samples/helloworld-go Global (ConfigMap) apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : pod-autoscaler-class : \"kpa.autoscaling.knative.dev\" Global (Operator) apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : pod-autoscaler-class : \"kpa.autoscaling.knative.dev\" Global versus per-revision settings \u00b6 Configuring for autoscaling in Knative can be set using either global or per-revision settings. If no per-revision autoscaling settings are specified, the global settings will be used. If per-revision settings are specified, these will override the global settings when both types of settings exist. Global settings \u00b6 Global settings for autoscaling are configured using the config-autoscaler ConfigMap. If you installed Knative Serving using the Operator, you can set global configuration settings in the spec.config.autoscaler ConfigMap, located in the KnativeServing custom resource (CR). Example of the default autoscaling ConfigMap \u00b6 apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : container-concurrency-target-default : \"100\" container-concurrency-target-percentage : \"0.7\" enable-scale-to-zero : \"true\" max-scale-up-rate : \"1000\" max-scale-down-rate : \"2\" panic-window-percentage : \"10\" panic-threshold-percentage : \"200\" scale-to-zero-grace-period : \"30s\" scale-to-zero-pod-retention-period : \"0s\" stable-window : \"60s\" target-burst-capacity : \"200\" requests-per-second-target-default : \"200\" Per-revision settings \u00b6 Per-revision settings for autoscaling are configured by adding annotations to a revision. Example \u00b6 apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/target : \"70\" IMPORTANT If you are creating revisions by using a service or configuration, you must set the annotations in the revision template so that any modifications will be applied to each revision as they are created. Setting annotations in the top level metadata of a single revision will not propagate the changes to other revisions and will not apply changes to the autoscaling configuration for your application.","title":"Concepts"},{"location":"serving/autoscaling/autoscaling-concepts/#autoscaling-concepts","text":"This section covers conceptual information about which Autoscaler types are supported, as well as fundamental information about how autoscaling is configured.","title":"Autoscaling concepts"},{"location":"serving/autoscaling/autoscaling-concepts/#supported-autoscaler-types","text":"Knative Serving supports the implementation of Knative Pod Autoscaler (KPA) and Kubernetes' Horizontal Pod Autoscaler (HPA). The features and limitations of each of these Autoscalers are listed below. IMPORTANT If you want to use Kubernetes Horizontal Pod Autoscaler (HPA), you must install it after you install Knative Serving. For how to install HPA, see Install optional Eventing extensions .","title":"Supported Autoscaler types"},{"location":"serving/autoscaling/autoscaling-concepts/#knative-pod-autoscaler-kpa","text":"Part of the Knative Serving core and enabled by default once Knative Serving is installed. Supports scale to zero functionality. Does not support CPU-based autoscaling.","title":"Knative Pod Autoscaler (KPA)"},{"location":"serving/autoscaling/autoscaling-concepts/#horizontal-pod-autoscaler-hpa","text":"Not part of the Knative Serving core, and you must install Knative Serving first. Does not support scale to zero functionality. Supports CPU-based autoscaling.","title":"Horizontal Pod Autoscaler (HPA)"},{"location":"serving/autoscaling/autoscaling-concepts/#configuring-the-autoscaler-implementation","text":"The type of Autoscaler implementation (KPA or HPA) can be configured by using the class annotation. Global settings key: pod-autoscaler-class Per-revision annotation key: autoscaling.knative.dev/class Possible values: \"kpa.autoscaling.knative.dev\" or \"hpa.autoscaling.knative.dev\" Default: \"kpa.autoscaling.knative.dev\" Example: Per Revision apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/class : \"kpa.autoscaling.knative.dev\" spec : containers : - image : gcr.io/knative-samples/helloworld-go Global (ConfigMap) apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : pod-autoscaler-class : \"kpa.autoscaling.knative.dev\" Global (Operator) apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : pod-autoscaler-class : \"kpa.autoscaling.knative.dev\"","title":"Configuring the Autoscaler implementation"},{"location":"serving/autoscaling/autoscaling-concepts/#global-versus-per-revision-settings","text":"Configuring for autoscaling in Knative can be set using either global or per-revision settings. If no per-revision autoscaling settings are specified, the global settings will be used. If per-revision settings are specified, these will override the global settings when both types of settings exist.","title":"Global versus per-revision settings"},{"location":"serving/autoscaling/autoscaling-concepts/#global-settings","text":"Global settings for autoscaling are configured using the config-autoscaler ConfigMap. If you installed Knative Serving using the Operator, you can set global configuration settings in the spec.config.autoscaler ConfigMap, located in the KnativeServing custom resource (CR).","title":"Global settings"},{"location":"serving/autoscaling/autoscaling-concepts/#example-of-the-default-autoscaling-configmap","text":"apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : container-concurrency-target-default : \"100\" container-concurrency-target-percentage : \"0.7\" enable-scale-to-zero : \"true\" max-scale-up-rate : \"1000\" max-scale-down-rate : \"2\" panic-window-percentage : \"10\" panic-threshold-percentage : \"200\" scale-to-zero-grace-period : \"30s\" scale-to-zero-pod-retention-period : \"0s\" stable-window : \"60s\" target-burst-capacity : \"200\" requests-per-second-target-default : \"200\"","title":"Example of the default autoscaling ConfigMap"},{"location":"serving/autoscaling/autoscaling-concepts/#per-revision-settings","text":"Per-revision settings for autoscaling are configured by adding annotations to a revision.","title":"Per-revision settings"},{"location":"serving/autoscaling/autoscaling-concepts/#example","text":"apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/target : \"70\" IMPORTANT If you are creating revisions by using a service or configuration, you must set the annotations in the revision template so that any modifications will be applied to each revision as they are created. Setting annotations in the top level metadata of a single revision will not propagate the changes to other revisions and will not apply changes to the autoscaling configuration for your application.","title":"Example"},{"location":"serving/autoscaling/autoscaling-metrics/","text":"Metrics \u00b6 The metric configuration defines which metric type is watched by the Autoscaler. Setting metrics per revision \u00b6 For per-revision configuration, this is determined using the autoscaling.knative.dev/metric annotation. The possible metric types that can be configured per revision depend on the type of Autoscaler implementation you are using: The default KPA Autoscaler supports the concurrency and rps metrics. The HPA Autoscaler supports the cpu metric. For more information about KPA and HPA, see the documentation on Supported Autoscaler types . Per-revision annotation key: autoscaling.knative.dev/metric Possible values: \"concurrency\" , \"rps\" or \"cpu\" , depending on your Autoscaler type. The cpu metric is only supported on revisions with the HPA class. Default: \"concurrency\" Per-revision concurrency configuration apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/metric : \"concurrency\" Per-revision rps configuration apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/metric : \"rps\" Per-revision cpu configuration apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/metric : \"cpu\" Next steps \u00b6 Configure concurrency targets for applications Configure requests per second targets for replicas of an application","title":"Metrics"},{"location":"serving/autoscaling/autoscaling-metrics/#metrics","text":"The metric configuration defines which metric type is watched by the Autoscaler.","title":"Metrics"},{"location":"serving/autoscaling/autoscaling-metrics/#setting-metrics-per-revision","text":"For per-revision configuration, this is determined using the autoscaling.knative.dev/metric annotation. The possible metric types that can be configured per revision depend on the type of Autoscaler implementation you are using: The default KPA Autoscaler supports the concurrency and rps metrics. The HPA Autoscaler supports the cpu metric. For more information about KPA and HPA, see the documentation on Supported Autoscaler types . Per-revision annotation key: autoscaling.knative.dev/metric Possible values: \"concurrency\" , \"rps\" or \"cpu\" , depending on your Autoscaler type. The cpu metric is only supported on revisions with the HPA class. Default: \"concurrency\" Per-revision concurrency configuration apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/metric : \"concurrency\" Per-revision rps configuration apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/metric : \"rps\" Per-revision cpu configuration apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/metric : \"cpu\"","title":"Setting metrics per revision"},{"location":"serving/autoscaling/autoscaling-metrics/#next-steps","text":"Configure concurrency targets for applications Configure requests per second targets for replicas of an application","title":"Next steps"},{"location":"serving/autoscaling/autoscaling-targets/","text":"Targets \u00b6 Configuring a target provide the Autoscaler with a value that it tries to maintain for the configured metric for a revision. See the metrics documentation for more information about configurable metric types. The target annotation, used to configure per-revision targets, is metric agnostic . This means the target is simply an integer value, which can be applied for any metric type. Configuring targets \u00b6 Global settings key: container-concurrency-target-default for setting a concurrency target, and requests-per-second-target-default for setting a requests-per-second (RPS) target. For more information, see the documentation on metrics . Per-revision annotation key: autoscaling.knative.dev/target Possible values: An integer (metric agnostic). Default: \"100\" for container-concurrency-target-default , and \"200\" for requests-per-second-target-default . There is no default value set for the target annotation. Target annotation - Per-revision apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/target : \"50\" Concurrency target - Global (ConfigMap) apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : container-concurrency-target-default : \"200\" Concurrency target - Container Global (Operator) apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : container-concurrency-target-default : \"200\" Requests per second (RPS) target - Global (ConfigMap) apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : requests-per-second-target-default : \"150\" Requests per second (RPS) target - Global (Operator) apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : requests-per-second-target-default : \"150\"","title":"Targets"},{"location":"serving/autoscaling/autoscaling-targets/#targets","text":"Configuring a target provide the Autoscaler with a value that it tries to maintain for the configured metric for a revision. See the metrics documentation for more information about configurable metric types. The target annotation, used to configure per-revision targets, is metric agnostic . This means the target is simply an integer value, which can be applied for any metric type.","title":"Targets"},{"location":"serving/autoscaling/autoscaling-targets/#configuring-targets","text":"Global settings key: container-concurrency-target-default for setting a concurrency target, and requests-per-second-target-default for setting a requests-per-second (RPS) target. For more information, see the documentation on metrics . Per-revision annotation key: autoscaling.knative.dev/target Possible values: An integer (metric agnostic). Default: \"100\" for container-concurrency-target-default , and \"200\" for requests-per-second-target-default . There is no default value set for the target annotation. Target annotation - Per-revision apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/target : \"50\" Concurrency target - Global (ConfigMap) apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : container-concurrency-target-default : \"200\" Concurrency target - Container Global (Operator) apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : container-concurrency-target-default : \"200\" Requests per second (RPS) target - Global (ConfigMap) apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : requests-per-second-target-default : \"150\" Requests per second (RPS) target - Global (Operator) apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : requests-per-second-target-default : \"150\"","title":"Configuring targets"},{"location":"serving/autoscaling/concurrency/","text":"Configuring concurrency \u00b6 Concurrency determines the number of simultaneous requests that can be processed by each replica of an application at any given time. For per-revision concurrency, you must configure both autoscaling.knative.dev/metric and autoscaling.knative.dev/target for a soft limit , or containerConcurrency for a hard limit . For global concurrency, you can set the container-concurrency-target-default value. Soft versus hard concurrency limits \u00b6 It is possible to set either a soft or hard concurrency limit. NOTE: If both a soft and a hard limit are specified, the smaller of the two values will be used. This prevents the Autoscaler from having a target value that is not permitted by the hard limit value. The soft limit is a targeted limit rather than a strictly enforced bound. In some situations, particularly if there is a sudden burst of requests, this value can be exceeded. The hard limit is an enforced upper bound. If concurrency reaches the hard limit, surplus requests will be buffered and must wait until enough capacity is free to execute the requests. IMPORTANT: Using a hard limit configuration is only recommended if there is a clear use case for it with your application. Having a low hard limit specified may have a negative impact on the throughput and latency of an application, and may cause additional cold starts. Soft limit \u00b6 Global key: container-concurrency-target-default Per-revision annotation key: autoscaling.knative.dev/target Possible values: An integer. Default: \"100\" Example: Per Revision apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/target : \"200\" Global (ConfigMap) apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : container-concurrency-target-default : \"200\" Global (Operator) apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : container-concurrency-target-default : \"200\" Hard limit \u00b6 The hard limit is specified per Revision using the containerConcurrency field on the Revision spec. This setting is not an annotation. There is no global setting for the hard limit in the autoscaling ConfigMap, because containerConcurrency has implications outside of autoscaling, such as on buffering and queuing of requests. However, a default value can be set for the Revision's containerConcurrency field in config-defaults.yaml . The default value is 0 , meaning that there is no limit on the number of requests that are allowed to flow into the revision. A value greater than 0 specifies the exact number of requests that are allowed to flow to the replica at any one time. Global key: container-concurrency (in config-defaults.yaml ) Per-revision spec key: containerConcurrency Possible values: integer Default: 0 , meaning no limit Per Revision Example: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : spec : containerConcurrency : 50 Global (Defaults ConfigMap) apiVersion : v1 kind : ConfigMap metadata : name : config-defaults namespace : knative-serving data : container-concurrency : \"50\" Global (Operator) apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : defaults : container-concurrency : \"50\" Target utilization \u00b6 In addition to the literal settings explained previously, concurrency values can be further adjusted by using a target utilization value . This value specifies what percentage of the previously specified target should actually be targeted by the Autoscaler. This is also known as specifying the hotness at which a replica runs, which causes the Autoscaler to scale up before the defined hard limit is reached. For example, if containerConcurrency is set to 10, and the target utilization value is set to 70 (percent), the Autoscaler will create a new replica when the average number of concurrent requests across all existing replicas reaches 7. Requests numbered 7 to 10 will still be sent to the existing replicas, but this allows for additional replicas to be started in anticipation of being needed when the containerConcurrency limit is reached. Global key: container-concurrency-target-percentage Per-revision annotation key: autoscaling.knative.dev/targetUtilizationPercentage Possible values: float Default: 70 Example: Per Revision apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/targetUtilizationPercentage : \"80\" spec : containers : - image : gcr.io/knative-samples/helloworld-go Global (ConfigMap) apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : container-concurrency-target-percentage : \"80\" Global (Operator) apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : container-concurrency-target-percentage : \"80\"","title":"Configuring Concurrency"},{"location":"serving/autoscaling/concurrency/#configuring-concurrency","text":"Concurrency determines the number of simultaneous requests that can be processed by each replica of an application at any given time. For per-revision concurrency, you must configure both autoscaling.knative.dev/metric and autoscaling.knative.dev/target for a soft limit , or containerConcurrency for a hard limit . For global concurrency, you can set the container-concurrency-target-default value.","title":"Configuring concurrency"},{"location":"serving/autoscaling/concurrency/#soft-versus-hard-concurrency-limits","text":"It is possible to set either a soft or hard concurrency limit. NOTE: If both a soft and a hard limit are specified, the smaller of the two values will be used. This prevents the Autoscaler from having a target value that is not permitted by the hard limit value. The soft limit is a targeted limit rather than a strictly enforced bound. In some situations, particularly if there is a sudden burst of requests, this value can be exceeded. The hard limit is an enforced upper bound. If concurrency reaches the hard limit, surplus requests will be buffered and must wait until enough capacity is free to execute the requests. IMPORTANT: Using a hard limit configuration is only recommended if there is a clear use case for it with your application. Having a low hard limit specified may have a negative impact on the throughput and latency of an application, and may cause additional cold starts.","title":"Soft versus hard concurrency limits"},{"location":"serving/autoscaling/concurrency/#soft-limit","text":"Global key: container-concurrency-target-default Per-revision annotation key: autoscaling.knative.dev/target Possible values: An integer. Default: \"100\" Example: Per Revision apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/target : \"200\" Global (ConfigMap) apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : container-concurrency-target-default : \"200\" Global (Operator) apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : container-concurrency-target-default : \"200\"","title":"Soft limit"},{"location":"serving/autoscaling/concurrency/#hard-limit","text":"The hard limit is specified per Revision using the containerConcurrency field on the Revision spec. This setting is not an annotation. There is no global setting for the hard limit in the autoscaling ConfigMap, because containerConcurrency has implications outside of autoscaling, such as on buffering and queuing of requests. However, a default value can be set for the Revision's containerConcurrency field in config-defaults.yaml . The default value is 0 , meaning that there is no limit on the number of requests that are allowed to flow into the revision. A value greater than 0 specifies the exact number of requests that are allowed to flow to the replica at any one time. Global key: container-concurrency (in config-defaults.yaml ) Per-revision spec key: containerConcurrency Possible values: integer Default: 0 , meaning no limit Per Revision Example: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : spec : containerConcurrency : 50 Global (Defaults ConfigMap) apiVersion : v1 kind : ConfigMap metadata : name : config-defaults namespace : knative-serving data : container-concurrency : \"50\" Global (Operator) apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : defaults : container-concurrency : \"50\"","title":"Hard limit"},{"location":"serving/autoscaling/concurrency/#target-utilization","text":"In addition to the literal settings explained previously, concurrency values can be further adjusted by using a target utilization value . This value specifies what percentage of the previously specified target should actually be targeted by the Autoscaler. This is also known as specifying the hotness at which a replica runs, which causes the Autoscaler to scale up before the defined hard limit is reached. For example, if containerConcurrency is set to 10, and the target utilization value is set to 70 (percent), the Autoscaler will create a new replica when the average number of concurrent requests across all existing replicas reaches 7. Requests numbered 7 to 10 will still be sent to the existing replicas, but this allows for additional replicas to be started in anticipation of being needed when the containerConcurrency limit is reached. Global key: container-concurrency-target-percentage Per-revision annotation key: autoscaling.knative.dev/targetUtilizationPercentage Possible values: float Default: 70 Example: Per Revision apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/targetUtilizationPercentage : \"80\" spec : containers : - image : gcr.io/knative-samples/helloworld-go Global (ConfigMap) apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : container-concurrency-target-percentage : \"80\" Global (Operator) apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : container-concurrency-target-percentage : \"80\"","title":"Target utilization"},{"location":"serving/autoscaling/kpa-specific/","text":"Additional autoscaling configuration for Knative Pod Autoscaler \u00b6 The following settings are specific to the Knative Pod Autoscaler (KPA). Modes \u00b6 The KPA acts on metrics ( concurrency or rps ) aggregated over time-based windows. These windows define the amount of historical data that the Autoscaler takes into account, and are used to smooth the data over the specified amount of time. The shorter these windows are, the more quickly the Autoscaler will react. The KPA's implementation has two modes: stable and panic . Stable mode is used for general operation, while panic mode by default has a much shorter window, and will be used to quickly scale a revision up if a burst of traffic arrives. NOTE: When using panic mode, the revision will not scale down to avoid churn. The Autoscaler will leave panic mode if there has been no reason to react quickly during the stable window's timeframe. Stable window \u00b6 Global key: stable-window Per-revision annotation key: autoscaling.knative.dev/window Possible values: Duration, 6s <= value <= 1h Default: 60s NOTE: During scale down, the last replica will only be removed after there has not been any traffic to the revision for the entire duration of the stable window. Example: Per Revision apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/window : \"40s\" spec : containers : - image : gcr.io/knative-samples/helloworld-go Global (ConfigMap) apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : stable-window : \"40s\" Global (Operator) apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : stable-window : \"40s\" Panic window \u00b6 The panic window is defined as a percentage of the stable window to assure that both are relative to each other in a working way. This value indicates how the window over which historical data is evaluated will shrink upon entering panic mode. For example, a value of 10.0 means that in panic mode the window will be 10% of the stable window size. Global key: panic-window-percentage Per-revision annotation key: autoscaling.knative.dev/panicWindowPercentage Possible values: float, 1.0 <= value <= 100.0 Default: 10.0 Example: Per Revision apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/panicWindowPercentage : \"20.0\" spec : containers : - image : gcr.io/knative-samples/helloworld-go Global (ConfigMap) apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : panic-window-percentage : \"20.0\" Global (Operator) apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : panic-window-percentage : \"20.0\" Panic mode threshold \u00b6 This threshold defines when the Autoscaler will move from stable mode into panic mode. This value is a percentage of the traffic that the current amount of replicas can handle. NOTE: A value of 100.0 (100 percent) means that the Autoscaler is always in panic mode, therefore the minimum value should be higher than 100.0 . The default setting of 200.0 means that panic mode will be start if traffic is twice as high as the current replica population can handle. Global key: panic-threshold-percentage Per-revision annotation key: autoscaling.knative.dev/panicThresholdPercentage Possible values: float, 110.0 <= value <= 1000.0 Default: 200.0 Example: Per Revision apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/panicThresholdPercentage : \"150.0\" spec : containers : - image : gcr.io/knative-samples/helloworld-go Global (ConfigMap) apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : panic-threshold-percentage : \"150.0\" Global (Operator) apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : panic-threshold-percentage : \"150.0\" Scale rates \u00b6 These settings control by how much the replica population can scale up or down in a single evaluation cycle. A minimal change of one replica in each direction is always permitted, so the Autoscaler can scale to +/- 1 replica at any time, regardless of the scale rates set. Scale up rate \u00b6 This setting determines the maximum ratio of desired to existing pods. For example, with a value of 2.0 , the revision can only scale from N to 2*N pods in one evaluation cycle. Global key: max-scale-up-rate Per-revision annotation key: n/a Possible values: float Default: 1000.0 Example: Global (ConfigMap) apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : max-scale-up-rate : \"500.0\" Global (Operator) apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : max-scale-up-rate : \"500.0\" Scale down rate \u00b6 This setting determines the maximum ratio of existing to desired pods. For example, with a value of 2.0 , the revision can only scale from N to N/2 pods in one evaluation cycle. Global key: max-scale-down-rate Per-revision annotation key: n/a Possible values: float Default: 2.0 Example: Global (ConfigMap) apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : max-scale-down-rate : \"4.0\" Global (Operator) apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : max-scale-down-rate : \"4.0\"","title":"KPA-Specific Configuration"},{"location":"serving/autoscaling/kpa-specific/#additional-autoscaling-configuration-for-knative-pod-autoscaler","text":"The following settings are specific to the Knative Pod Autoscaler (KPA).","title":"Additional autoscaling configuration for Knative Pod Autoscaler"},{"location":"serving/autoscaling/kpa-specific/#modes","text":"The KPA acts on metrics ( concurrency or rps ) aggregated over time-based windows. These windows define the amount of historical data that the Autoscaler takes into account, and are used to smooth the data over the specified amount of time. The shorter these windows are, the more quickly the Autoscaler will react. The KPA's implementation has two modes: stable and panic . Stable mode is used for general operation, while panic mode by default has a much shorter window, and will be used to quickly scale a revision up if a burst of traffic arrives. NOTE: When using panic mode, the revision will not scale down to avoid churn. The Autoscaler will leave panic mode if there has been no reason to react quickly during the stable window's timeframe.","title":"Modes"},{"location":"serving/autoscaling/kpa-specific/#stable-window","text":"Global key: stable-window Per-revision annotation key: autoscaling.knative.dev/window Possible values: Duration, 6s <= value <= 1h Default: 60s NOTE: During scale down, the last replica will only be removed after there has not been any traffic to the revision for the entire duration of the stable window. Example: Per Revision apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/window : \"40s\" spec : containers : - image : gcr.io/knative-samples/helloworld-go Global (ConfigMap) apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : stable-window : \"40s\" Global (Operator) apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : stable-window : \"40s\"","title":"Stable window"},{"location":"serving/autoscaling/kpa-specific/#panic-window","text":"The panic window is defined as a percentage of the stable window to assure that both are relative to each other in a working way. This value indicates how the window over which historical data is evaluated will shrink upon entering panic mode. For example, a value of 10.0 means that in panic mode the window will be 10% of the stable window size. Global key: panic-window-percentage Per-revision annotation key: autoscaling.knative.dev/panicWindowPercentage Possible values: float, 1.0 <= value <= 100.0 Default: 10.0 Example: Per Revision apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/panicWindowPercentage : \"20.0\" spec : containers : - image : gcr.io/knative-samples/helloworld-go Global (ConfigMap) apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : panic-window-percentage : \"20.0\" Global (Operator) apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : panic-window-percentage : \"20.0\"","title":"Panic window"},{"location":"serving/autoscaling/kpa-specific/#panic-mode-threshold","text":"This threshold defines when the Autoscaler will move from stable mode into panic mode. This value is a percentage of the traffic that the current amount of replicas can handle. NOTE: A value of 100.0 (100 percent) means that the Autoscaler is always in panic mode, therefore the minimum value should be higher than 100.0 . The default setting of 200.0 means that panic mode will be start if traffic is twice as high as the current replica population can handle. Global key: panic-threshold-percentage Per-revision annotation key: autoscaling.knative.dev/panicThresholdPercentage Possible values: float, 110.0 <= value <= 1000.0 Default: 200.0 Example: Per Revision apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/panicThresholdPercentage : \"150.0\" spec : containers : - image : gcr.io/knative-samples/helloworld-go Global (ConfigMap) apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : panic-threshold-percentage : \"150.0\" Global (Operator) apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : panic-threshold-percentage : \"150.0\"","title":"Panic mode threshold"},{"location":"serving/autoscaling/kpa-specific/#scale-rates","text":"These settings control by how much the replica population can scale up or down in a single evaluation cycle. A minimal change of one replica in each direction is always permitted, so the Autoscaler can scale to +/- 1 replica at any time, regardless of the scale rates set.","title":"Scale rates"},{"location":"serving/autoscaling/kpa-specific/#scale-up-rate","text":"This setting determines the maximum ratio of desired to existing pods. For example, with a value of 2.0 , the revision can only scale from N to 2*N pods in one evaluation cycle. Global key: max-scale-up-rate Per-revision annotation key: n/a Possible values: float Default: 1000.0 Example: Global (ConfigMap) apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : max-scale-up-rate : \"500.0\" Global (Operator) apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : max-scale-up-rate : \"500.0\"","title":"Scale up rate"},{"location":"serving/autoscaling/kpa-specific/#scale-down-rate","text":"This setting determines the maximum ratio of existing to desired pods. For example, with a value of 2.0 , the revision can only scale from N to N/2 pods in one evaluation cycle. Global key: max-scale-down-rate Per-revision annotation key: n/a Possible values: float Default: 2.0 Example: Global (ConfigMap) apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : max-scale-down-rate : \"4.0\" Global (Operator) apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : max-scale-down-rate : \"4.0\"","title":"Scale down rate"},{"location":"serving/autoscaling/rps-target/","text":"Configuring the requests per second (RPS) target \u00b6 This setting specifies a target for requests-per-second per replica of an application. Global key: requests-per-second-target-default Per-revision annotation key: autoscaling.knative.dev/target (your revision must also be configured to use the rps metric annotation ) Possible values: An integer. Default: \"200\" Example: Per Revision apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/target : \"150\" autoscaling.knative.dev/metric : \"rps\" spec : containers : - image : gcr.io/knative-samples/helloworld-go Global (ConfigMap) apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : requests-per-second-target-default : \"150\" Global (Operator) apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : requests-per-second-target-default : \"150\"","title":"Configuring RPS Target"},{"location":"serving/autoscaling/rps-target/#configuring-the-requests-per-second-rps-target","text":"This setting specifies a target for requests-per-second per replica of an application. Global key: requests-per-second-target-default Per-revision annotation key: autoscaling.knative.dev/target (your revision must also be configured to use the rps metric annotation ) Possible values: An integer. Default: \"200\" Example: Per Revision apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/target : \"150\" autoscaling.knative.dev/metric : \"rps\" spec : containers : - image : gcr.io/knative-samples/helloworld-go Global (ConfigMap) apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : requests-per-second-target-default : \"150\" Global (Operator) apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : requests-per-second-target-default : \"150\"","title":"Configuring the requests per second (RPS) target"},{"location":"serving/autoscaling/scale-bounds/","text":"Configuring scale bounds \u00b6 You can configure upper and lower bounds to control autoscaling behavior. You can also specify the initial scale that a Revision is scaled to immediately after creation. This can be a default configuration for all Revisions, or for a specific Revision using an annotation. Lower bound \u00b6 This value controls the minimum number of replicas that each Revision should have. Knative will attempt to never have less than this number of replicas at any one point in time. Global key: n/a Per-revision annotation key: autoscaling.knative.dev/minScale Possible values: integer Default: 0 if scale-to-zero is enabled and class KPA is used, 1 otherwise NOTE: For more information about scale-to-zero configuration, see the documentation on Configuring scale to zero . Example: Per Revision apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/minScale : \"3\" spec : containers : - image : gcr.io/knative-samples/helloworld-go Upper bound \u00b6 This value controls the maximum number of replicas that each revision should have. Knative will attempt to never have more than this number of replicas running, or in the process of being created, at any one point in time. If the max-scale-limit global key is set, Knative ensures that neither the global max scale nor the per-revision max scale for new revisions exceed this value. When max-scale-limit is set to a positive value, a revision with a max scale above that value (including 0, which means unlimited) is disallowed. Global key: max-scale Per-revision annotation key: autoscaling.knative.dev/maxScale Possible values: integer Default: 0 which means unlimited Example: Per Revision apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/maxScale : \"3\" spec : containers : - image : gcr.io/knative-samples/helloworld-go Global (ConfigMap) apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : max-scale : \"3\" max-scale-limit : \"100\" Global (Operator) apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : max-scale : \"3\" max-scale-limit : \"100\" Initial scale \u00b6 This value controls the initial target scale a Revision must reach immediately after it is created before it is marked as Ready . After the Revision has reached this scale one time, this value is ignored. This means that the Revision will scale down after the initial target scale is reached if the actual traffic received only needs a smaller scale. When the Revision is created, the larger of initial scale and lower bound is automatically chosen as the initial target scale. Global key: initial-scale in combination with allow-zero-initial-scale Per-revision annotation key: autoscaling.knative.dev/initialScale Possible values: integer Default: 1 Example: Per Revision apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/initialScale : \"0\" spec : containers : - image : gcr.io/knative-samples/helloworld-go Global (ConfigMap) apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : initial-scale : \"0\" allow-zero-initial-scale : \"true\" Global (Operator) apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : initial-scale : \"0\" allow-zero-initial-scale : \"true\" Scale Down Delay \u00b6 Scale Down Delay specifies a time window which must pass at reduced concurrency before a scale-down decision is applied. This can be useful, for example, to keep containers around for a configurable duration to avoid a cold start penalty if new requests come in. Unlike setting a lower bound, the revision will eventually be scaled down if reduced concurrency is maintained for the delay period. Global key: scale-down-delay Per-revision annotation key: autoscaling.knative.dev/scaleDownDelay Possible values: Duration, 0s <= value <= 1h Default: 0s (no delay) Example: Per Revision apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/scaleDownDelay : \"15m\" spec : containers : - image : gcr.io/knative-samples/helloworld-go Global (ConfigMap) apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : scale-down-delay : \"15m\" Global (Operator) apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : scale-down-delay : \"15m\"","title":"Configuring Scale Bounds"},{"location":"serving/autoscaling/scale-bounds/#configuring-scale-bounds","text":"You can configure upper and lower bounds to control autoscaling behavior. You can also specify the initial scale that a Revision is scaled to immediately after creation. This can be a default configuration for all Revisions, or for a specific Revision using an annotation.","title":"Configuring scale bounds"},{"location":"serving/autoscaling/scale-bounds/#lower-bound","text":"This value controls the minimum number of replicas that each Revision should have. Knative will attempt to never have less than this number of replicas at any one point in time. Global key: n/a Per-revision annotation key: autoscaling.knative.dev/minScale Possible values: integer Default: 0 if scale-to-zero is enabled and class KPA is used, 1 otherwise NOTE: For more information about scale-to-zero configuration, see the documentation on Configuring scale to zero . Example: Per Revision apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/minScale : \"3\" spec : containers : - image : gcr.io/knative-samples/helloworld-go","title":"Lower bound"},{"location":"serving/autoscaling/scale-bounds/#upper-bound","text":"This value controls the maximum number of replicas that each revision should have. Knative will attempt to never have more than this number of replicas running, or in the process of being created, at any one point in time. If the max-scale-limit global key is set, Knative ensures that neither the global max scale nor the per-revision max scale for new revisions exceed this value. When max-scale-limit is set to a positive value, a revision with a max scale above that value (including 0, which means unlimited) is disallowed. Global key: max-scale Per-revision annotation key: autoscaling.knative.dev/maxScale Possible values: integer Default: 0 which means unlimited Example: Per Revision apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/maxScale : \"3\" spec : containers : - image : gcr.io/knative-samples/helloworld-go Global (ConfigMap) apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : max-scale : \"3\" max-scale-limit : \"100\" Global (Operator) apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : max-scale : \"3\" max-scale-limit : \"100\"","title":"Upper bound"},{"location":"serving/autoscaling/scale-bounds/#initial-scale","text":"This value controls the initial target scale a Revision must reach immediately after it is created before it is marked as Ready . After the Revision has reached this scale one time, this value is ignored. This means that the Revision will scale down after the initial target scale is reached if the actual traffic received only needs a smaller scale. When the Revision is created, the larger of initial scale and lower bound is automatically chosen as the initial target scale. Global key: initial-scale in combination with allow-zero-initial-scale Per-revision annotation key: autoscaling.knative.dev/initialScale Possible values: integer Default: 1 Example: Per Revision apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/initialScale : \"0\" spec : containers : - image : gcr.io/knative-samples/helloworld-go Global (ConfigMap) apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : initial-scale : \"0\" allow-zero-initial-scale : \"true\" Global (Operator) apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : initial-scale : \"0\" allow-zero-initial-scale : \"true\"","title":"Initial scale"},{"location":"serving/autoscaling/scale-bounds/#scale-down-delay","text":"Scale Down Delay specifies a time window which must pass at reduced concurrency before a scale-down decision is applied. This can be useful, for example, to keep containers around for a configurable duration to avoid a cold start penalty if new requests come in. Unlike setting a lower bound, the revision will eventually be scaled down if reduced concurrency is maintained for the delay period. Global key: scale-down-delay Per-revision annotation key: autoscaling.knative.dev/scaleDownDelay Possible values: Duration, 0s <= value <= 1h Default: 0s (no delay) Example: Per Revision apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/scaleDownDelay : \"15m\" spec : containers : - image : gcr.io/knative-samples/helloworld-go Global (ConfigMap) apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : scale-down-delay : \"15m\" Global (Operator) apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : scale-down-delay : \"15m\"","title":"Scale Down Delay"},{"location":"serving/autoscaling/scale-to-zero/","text":"Configuring scale to zero \u00b6 IMPORTANT: Scale to zero can only be enabled if you are using the Knative Pod Autoscaler (KPA), and can only be configured globally. For more information about using KPA or global configuration, see the documentation on Autoscaling concepts . Enable scale to zero \u00b6 The scale to zero value controls whether Knative allows replicas to scale down to zero (if set to true ), or stop at 1 replica if set to false . NOTE: For more information about scale bounds configuration per revision, see the documentation on Configuring scale bounds . Global key: enable-scale-to-zero Per-revision annotation key: No per-revision setting. Possible values: boolean Default: true Example: Global (ConfigMap) apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : enable-scale-to-zero : \"false\" Global (Operator) apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : enable-scale-to-zero : \"false\" Scale to zero grace period \u00b6 This setting specifies an upper bound time limit that the system will wait internally for scale-from-zero machinery to be in place before the last replica is removed. IMPORTANT: This is a value that controls how long internal network programming is allowed to take, and should only be adjusted if you have experienced issues with requests being dropped while a revision was scaling to zero replicas. This setting does not adjust how long the last replica will be kept after traffic ends, and it does not guarantee that the replica will actually be kept for this entire duration. Global key: scale-to-zero-grace-period Per-revision annotation key: n/a Possible values: Duration Default: 30s Example: Global (ConfigMap) apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : scale-to-zero-grace-period : \"40s\" Global (Operator) apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : scale-to-zero-grace-period : \"40s\" Scale to zero last pod retention period \u00b6 The scale-to-zero-pod-retention-period flag determines the minimum amount of time that the last pod will remain active after the Autoscaler decides to scale pods to zero. This contrasts with the scale-to-zero-grace-period flag, which determines the maximum amount of time that the last pod will remain active after the Autoscaler decides to scale pods to zero. Global key: scale-to-zero-pod-retention-period Per-revision annotation key: autoscaling.knative.dev/scaleToZeroPodRetentionPeriod Possible values: Non-negative duration string Default: 0s Example: Per Revision apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/scaleToZeroPodRetentionPeriod : \"1m5s\" spec : containers : - image : gcr.io/knative-samples/helloworld-go Global (ConfigMap) apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : scale-to-zero-pod-retention-period : \"42s\" Global (Operator) apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : scale-to-zero-pod-retention-period : \"42s\"","title":"Configuring Scale-to-zero"},{"location":"serving/autoscaling/scale-to-zero/#configuring-scale-to-zero","text":"IMPORTANT: Scale to zero can only be enabled if you are using the Knative Pod Autoscaler (KPA), and can only be configured globally. For more information about using KPA or global configuration, see the documentation on Autoscaling concepts .","title":"Configuring scale to zero"},{"location":"serving/autoscaling/scale-to-zero/#enable-scale-to-zero","text":"The scale to zero value controls whether Knative allows replicas to scale down to zero (if set to true ), or stop at 1 replica if set to false . NOTE: For more information about scale bounds configuration per revision, see the documentation on Configuring scale bounds . Global key: enable-scale-to-zero Per-revision annotation key: No per-revision setting. Possible values: boolean Default: true Example: Global (ConfigMap) apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : enable-scale-to-zero : \"false\" Global (Operator) apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : enable-scale-to-zero : \"false\"","title":"Enable scale to zero"},{"location":"serving/autoscaling/scale-to-zero/#scale-to-zero-grace-period","text":"This setting specifies an upper bound time limit that the system will wait internally for scale-from-zero machinery to be in place before the last replica is removed. IMPORTANT: This is a value that controls how long internal network programming is allowed to take, and should only be adjusted if you have experienced issues with requests being dropped while a revision was scaling to zero replicas. This setting does not adjust how long the last replica will be kept after traffic ends, and it does not guarantee that the replica will actually be kept for this entire duration. Global key: scale-to-zero-grace-period Per-revision annotation key: n/a Possible values: Duration Default: 30s Example: Global (ConfigMap) apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : scale-to-zero-grace-period : \"40s\" Global (Operator) apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : scale-to-zero-grace-period : \"40s\"","title":"Scale to zero grace period"},{"location":"serving/autoscaling/scale-to-zero/#scale-to-zero-last-pod-retention-period","text":"The scale-to-zero-pod-retention-period flag determines the minimum amount of time that the last pod will remain active after the Autoscaler decides to scale pods to zero. This contrasts with the scale-to-zero-grace-period flag, which determines the maximum amount of time that the last pod will remain active after the Autoscaler decides to scale pods to zero. Global key: scale-to-zero-pod-retention-period Per-revision annotation key: autoscaling.knative.dev/scaleToZeroPodRetentionPeriod Possible values: Non-negative duration string Default: 0s Example: Per Revision apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/scaleToZeroPodRetentionPeriod : \"1m5s\" spec : containers : - image : gcr.io/knative-samples/helloworld-go Global (ConfigMap) apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : scale-to-zero-pod-retention-period : \"42s\" Global (Operator) apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : scale-to-zero-pod-retention-period : \"42s\"","title":"Scale to zero last pod retention period"},{"location":"serving/autoscaling/autoscale-go/","text":"A demonstration of the autoscaling capabilities of a Knative Serving Revision. Prerequisites \u00b6 A Kubernetes cluster with Knative Serving installed. The hey load generator installed ( go get -u github.com/rakyll/hey ). Clone this repository, and move into the sample directory: git clone -b \"v0.21.x\" https://github.com/knative/docs knative-docs cd knative-docs Deploy the Service \u00b6 Deploy the sample Knative Service: kubectl apply --filename docs/serving/autoscaling/autoscale-go/service.yaml Obtain the URL of the service (once Ready ): $ kubectl get ksvc autoscale-go NAME URL LATESTCREATED LATESTREADY READY REASON autoscale-go http://autoscale-go.default.1.2.3.4.xip.io autoscale-go-96dtk autoscale-go-96dtk True Load the Service \u00b6 Make a request to the autoscale app to see it consume some resources. curl \"http://autoscale-go.default.1.2.3.4.xip.io?sleep=100&prime=10000&bloat=5\" Allocated 5 Mb of memory. The largest prime less than 10000 is 9973. Slept for 100.13 milliseconds. Send 30 seconds of traffic maintaining 50 in-flight requests. hey -z 30s -c 50 \\ \"http://autoscale-go.default.1.2.3.4.xip.io?sleep=100&prime=10000&bloat=5\" \\ && kubectl get pods Summary: Total: 30 .3379 secs Slowest: 0 .7433 secs Fastest: 0 .1672 secs Average: 0 .2778 secs Requests/sec: 178 .7861 Total data: 542038 bytes Size/request: 99 bytes Response time histogram: 0 .167 [ 1 ] | 0 .225 [ 1462 ] | \u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0 0 .282 [ 1303 ] | \u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0 0 .340 [ 1894 ] | \u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0 0 .398 [ 471 ] | \u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0 0 .455 [ 159 ] | \u25a0\u25a0\u25a0 0 .513 [ 68 ] | \u25a0 0 .570 [ 18 ] | 0 .628 [ 14 ] | 0 .686 [ 21 ] | 0 .743 [ 13 ] | Latency distribution: 10 % in 0 .1805 secs 25 % in 0 .2197 secs 50 % in 0 .2801 secs 75 % in 0 .3129 secs 90 % in 0 .3596 secs 95 % in 0 .4020 secs 99 % in 0 .5457 secs Details ( average, fastest, slowest ) : DNS+dialup: 0 .0007 secs, 0 .1672 secs, 0 .7433 secs DNS-lookup: 0 .0000 secs, 0 .0000 secs, 0 .0000 secs req write: 0 .0001 secs, 0 .0000 secs, 0 .0045 secs resp wait: 0 .2766 secs, 0 .1669 secs, 0 .6633 secs resp read: 0 .0002 secs, 0 .0000 secs, 0 .0065 secs Status code distribution: [ 200 ] 5424 responses NAME READY STATUS RESTARTS AGE autoscale-go-00001-deployment-78cdc67bf4-2w4sk 3 /3 Running 0 26s autoscale-go-00001-deployment-78cdc67bf4-dd2zb 3 /3 Running 0 24s autoscale-go-00001-deployment-78cdc67bf4-pg55p 3 /3 Running 0 18s autoscale-go-00001-deployment-78cdc67bf4-q8bf9 3 /3 Running 0 1m autoscale-go-00001-deployment-78cdc67bf4-thjbq 3 /3 Running 0 26s Analysis \u00b6 Algorithm \u00b6 Knative Serving autoscaling is based on the average number of in-flight requests per pod (concurrency). The system has a default target concurrency of 100 (Search for container-concurrency-target-default) but we used 10 for our service. We loaded the service with 50 concurrent requests so the autoscaler created 5 pods ( 50 concurrent requests / target of 10 = 5 pods ) Panic \u00b6 The autoscaler calculates average concurrency over a 60 second window so it takes a minute for the system to stablize at the desired level of concurrency. However the autoscaler also calculates a 6 second panic window and will enter panic mode if that window reached 2x the target concurrency. In panic mode the autoscaler operates on the shorter, more sensitive panic window. Once the panic conditions are no longer met for 60 seconds, the autoscaler will return to the initial 60 second stable window. | Panic Target---> +--| 20 | | | <------Panic Window | | Stable Target---> +-------------------------|--| 10 CONCURRENCY | | | | <-----------Stable Window | | | --------------------------+-------------------------+--+ 0 120 60 0 TIME Customization \u00b6 The autoscaler supports customization through annotations. There are two autoscaler classes built into Knative: kpa.autoscaling.knative.dev which is the concurrency-based autoscaler described above (the default), and hpa.autoscaling.knative.dev which delegates to the Kubernetes HPA which autoscales on CPU usage. Example of a Service scaled on CPU: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : autoscale-go namespace : default spec : template : metadata : annotations : # Standard Kubernetes CPU-based autoscaling. autoscaling.knative.dev/class : hpa.autoscaling.knative.dev autoscaling.knative.dev/metric : cpu spec : containers : - image : gcr.io/knative-samples/autoscale-go:0.1 Additionally the autoscaler targets and scaling bounds can be specified in annotations. Example of a Service with custom targets and scale bounds: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : autoscale-go namespace : default spec : template : metadata : annotations : # Knative concurrency-based autoscaling (default). autoscaling.knative.dev/class : kpa.autoscaling.knative.dev autoscaling.knative.dev/metric : concurrency # Target 10 requests in-flight per pod. autoscaling.knative.dev/target : \"10\" # Disable scale to zero with a minScale of 1. autoscaling.knative.dev/minScale : \"1\" # Limit scaling to 100 pods. autoscaling.knative.dev/maxScale : \"100\" spec : containers : - image : gcr.io/knative-samples/autoscale-go:0.1 Note: for an hpa.autoscaling.knative.dev class service, the autoscaling.knative.dev/target specifies the CPU percentage target (default \"80\" ). Demo \u00b6 View the Kubecon Demo of Knative autoscaler customization (32 minutes). Other Experiments \u00b6 Send 60 seconds of traffic maintaining 100 concurrent requests. hey -z 60s -c 100 \\ \"http://autoscale-go.default.1.2.3.4.xip.io?sleep=100&prime=10000&bloat=5\" Send 60 seconds of traffic maintaining 100 qps with short requests (10 ms). hey -z 60s -q 100 \\ \"http://autoscale-go.default.1.2.3.4.xip.io?sleep=10\" Send 60 seconds of traffic maintaining 100 qps with long requests (1 sec). hey -z 60s -q 100 \\ \"http://autoscale-go.default.1.2.3.4.xip.io?sleep=1000\" Send 60 seconds of traffic with heavy CPU usage (~1 cpu/sec/request, total 100 cpus). hey -z 60s -q 100 \\ \"http://autoscale-go.default.1.2.3.4.xip.io?prime=40000000\" Send 60 seconds of traffic with heavy memory usage (1 gb/request, total 5 gb). hey -z 60s -c 5 \\ \"http://autoscale-go.default.1.2.3.4.xip.io?bloat=1000\" Cleanup \u00b6 kubectl delete --filename docs/serving/autoscaling/autoscale-go/service.yaml Further reading \u00b6 Autoscaling Developer Documentation","title":"Autoscaling Sample App"},{"location":"serving/autoscaling/autoscale-go/#prerequisites","text":"A Kubernetes cluster with Knative Serving installed. The hey load generator installed ( go get -u github.com/rakyll/hey ). Clone this repository, and move into the sample directory: git clone -b \"v0.21.x\" https://github.com/knative/docs knative-docs cd knative-docs","title":"Prerequisites"},{"location":"serving/autoscaling/autoscale-go/#deploy-the-service","text":"Deploy the sample Knative Service: kubectl apply --filename docs/serving/autoscaling/autoscale-go/service.yaml Obtain the URL of the service (once Ready ): $ kubectl get ksvc autoscale-go NAME URL LATESTCREATED LATESTREADY READY REASON autoscale-go http://autoscale-go.default.1.2.3.4.xip.io autoscale-go-96dtk autoscale-go-96dtk True","title":"Deploy the Service"},{"location":"serving/autoscaling/autoscale-go/#load-the-service","text":"Make a request to the autoscale app to see it consume some resources. curl \"http://autoscale-go.default.1.2.3.4.xip.io?sleep=100&prime=10000&bloat=5\" Allocated 5 Mb of memory. The largest prime less than 10000 is 9973. Slept for 100.13 milliseconds. Send 30 seconds of traffic maintaining 50 in-flight requests. hey -z 30s -c 50 \\ \"http://autoscale-go.default.1.2.3.4.xip.io?sleep=100&prime=10000&bloat=5\" \\ && kubectl get pods Summary: Total: 30 .3379 secs Slowest: 0 .7433 secs Fastest: 0 .1672 secs Average: 0 .2778 secs Requests/sec: 178 .7861 Total data: 542038 bytes Size/request: 99 bytes Response time histogram: 0 .167 [ 1 ] | 0 .225 [ 1462 ] | \u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0 0 .282 [ 1303 ] | \u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0 0 .340 [ 1894 ] | \u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0 0 .398 [ 471 ] | \u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0 0 .455 [ 159 ] | \u25a0\u25a0\u25a0 0 .513 [ 68 ] | \u25a0 0 .570 [ 18 ] | 0 .628 [ 14 ] | 0 .686 [ 21 ] | 0 .743 [ 13 ] | Latency distribution: 10 % in 0 .1805 secs 25 % in 0 .2197 secs 50 % in 0 .2801 secs 75 % in 0 .3129 secs 90 % in 0 .3596 secs 95 % in 0 .4020 secs 99 % in 0 .5457 secs Details ( average, fastest, slowest ) : DNS+dialup: 0 .0007 secs, 0 .1672 secs, 0 .7433 secs DNS-lookup: 0 .0000 secs, 0 .0000 secs, 0 .0000 secs req write: 0 .0001 secs, 0 .0000 secs, 0 .0045 secs resp wait: 0 .2766 secs, 0 .1669 secs, 0 .6633 secs resp read: 0 .0002 secs, 0 .0000 secs, 0 .0065 secs Status code distribution: [ 200 ] 5424 responses NAME READY STATUS RESTARTS AGE autoscale-go-00001-deployment-78cdc67bf4-2w4sk 3 /3 Running 0 26s autoscale-go-00001-deployment-78cdc67bf4-dd2zb 3 /3 Running 0 24s autoscale-go-00001-deployment-78cdc67bf4-pg55p 3 /3 Running 0 18s autoscale-go-00001-deployment-78cdc67bf4-q8bf9 3 /3 Running 0 1m autoscale-go-00001-deployment-78cdc67bf4-thjbq 3 /3 Running 0 26s","title":"Load the Service"},{"location":"serving/autoscaling/autoscale-go/#analysis","text":"","title":"Analysis"},{"location":"serving/autoscaling/autoscale-go/#algorithm","text":"Knative Serving autoscaling is based on the average number of in-flight requests per pod (concurrency). The system has a default target concurrency of 100 (Search for container-concurrency-target-default) but we used 10 for our service. We loaded the service with 50 concurrent requests so the autoscaler created 5 pods ( 50 concurrent requests / target of 10 = 5 pods )","title":"Algorithm"},{"location":"serving/autoscaling/autoscale-go/#panic","text":"The autoscaler calculates average concurrency over a 60 second window so it takes a minute for the system to stablize at the desired level of concurrency. However the autoscaler also calculates a 6 second panic window and will enter panic mode if that window reached 2x the target concurrency. In panic mode the autoscaler operates on the shorter, more sensitive panic window. Once the panic conditions are no longer met for 60 seconds, the autoscaler will return to the initial 60 second stable window. | Panic Target---> +--| 20 | | | <------Panic Window | | Stable Target---> +-------------------------|--| 10 CONCURRENCY | | | | <-----------Stable Window | | | --------------------------+-------------------------+--+ 0 120 60 0 TIME","title":"Panic"},{"location":"serving/autoscaling/autoscale-go/#customization","text":"The autoscaler supports customization through annotations. There are two autoscaler classes built into Knative: kpa.autoscaling.knative.dev which is the concurrency-based autoscaler described above (the default), and hpa.autoscaling.knative.dev which delegates to the Kubernetes HPA which autoscales on CPU usage. Example of a Service scaled on CPU: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : autoscale-go namespace : default spec : template : metadata : annotations : # Standard Kubernetes CPU-based autoscaling. autoscaling.knative.dev/class : hpa.autoscaling.knative.dev autoscaling.knative.dev/metric : cpu spec : containers : - image : gcr.io/knative-samples/autoscale-go:0.1 Additionally the autoscaler targets and scaling bounds can be specified in annotations. Example of a Service with custom targets and scale bounds: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : autoscale-go namespace : default spec : template : metadata : annotations : # Knative concurrency-based autoscaling (default). autoscaling.knative.dev/class : kpa.autoscaling.knative.dev autoscaling.knative.dev/metric : concurrency # Target 10 requests in-flight per pod. autoscaling.knative.dev/target : \"10\" # Disable scale to zero with a minScale of 1. autoscaling.knative.dev/minScale : \"1\" # Limit scaling to 100 pods. autoscaling.knative.dev/maxScale : \"100\" spec : containers : - image : gcr.io/knative-samples/autoscale-go:0.1 Note: for an hpa.autoscaling.knative.dev class service, the autoscaling.knative.dev/target specifies the CPU percentage target (default \"80\" ).","title":"Customization"},{"location":"serving/autoscaling/autoscale-go/#demo","text":"View the Kubecon Demo of Knative autoscaler customization (32 minutes).","title":"Demo"},{"location":"serving/autoscaling/autoscale-go/#other-experiments","text":"Send 60 seconds of traffic maintaining 100 concurrent requests. hey -z 60s -c 100 \\ \"http://autoscale-go.default.1.2.3.4.xip.io?sleep=100&prime=10000&bloat=5\" Send 60 seconds of traffic maintaining 100 qps with short requests (10 ms). hey -z 60s -q 100 \\ \"http://autoscale-go.default.1.2.3.4.xip.io?sleep=10\" Send 60 seconds of traffic maintaining 100 qps with long requests (1 sec). hey -z 60s -q 100 \\ \"http://autoscale-go.default.1.2.3.4.xip.io?sleep=1000\" Send 60 seconds of traffic with heavy CPU usage (~1 cpu/sec/request, total 100 cpus). hey -z 60s -q 100 \\ \"http://autoscale-go.default.1.2.3.4.xip.io?prime=40000000\" Send 60 seconds of traffic with heavy memory usage (1 gb/request, total 5 gb). hey -z 60s -c 5 \\ \"http://autoscale-go.default.1.2.3.4.xip.io?bloat=1000\"","title":"Other Experiments"},{"location":"serving/autoscaling/autoscale-go/#cleanup","text":"kubectl delete --filename docs/serving/autoscaling/autoscale-go/service.yaml","title":"Cleanup"},{"location":"serving/autoscaling/autoscale-go/#further-reading","text":"Autoscaling Developer Documentation","title":"Further reading"},{"location":"serving/deploying/private-registry/","text":"Deploying images from a private container registry \u00b6 Learn how to configure your Knative cluster to deploy images from a private container registry. To share access to your private container images across multiple services and revisions, you create a list of Kubernetes secrets ( imagePullSecrets ) using your registry credentials, add that imagePullSecrets to your default service account , and then deploy those configurations to your Knative cluster. Before you begin \u00b6 You need: A Kubernetes cluster with Knative Serving installed . The credentials to the private container registry where your container images are stored. Configuring your credentials in Knative \u00b6 Create a imagePullSecrets that contains your credentials as a list of secrets: kubectl create secret docker-registry [ REGISTRY-CRED-SECRETS ] \\ --docker-server =[ PRIVATE_REGISTRY_SERVER_URL ] \\ --docker-email =[ PRIVATE_REGISTRY_EMAIL ] \\ --docker-username =[ PRIVATE_REGISTRY_USER ] \\ --docker-password =[ PRIVATE_REGISTRY_PASSWORD ] Where - [REGISTRY-CRED-SECRETS] is the name that you want for your secrets ( imagePullSecrets object). For example, container-registry . [PRIVATE_REGISTRY_SERVER_URL] is the URL to the private registry where your container images are stored. Examples: - Google Container Registry: https://gcr.io/ - DockerHub https://docker.io/ [PRIVATE_REGISTRY_EMAIL] is your email address that is associated with the private registry. [PRIVATE_REGISTRY_USER] is the username that you use to access the private container registry. [PRIVATE_REGISTRY_PASSWORD] is the password that you use to access the private container registry. Example: kubectl create secret ` container-registry ` \\ --docker-server = https://gcr.io/ \\ --docker-email = my-account-email@address.com \\ --docker-username = my-grc-username \\ --docker-password = my-gcr-password Tip: After creating the imagePullSecrets , you can view those secret's by running: kubectl get secret [ REGISTRY-CRED-SECRETS ] --output = yaml Add the imagePullSecrets to your default service account in the default namespace. Note: By default, the default service account in each of the namespaces of your Knative cluster are use by your revisions unless serviceAccountName is specified. Run the following command to modify your default service account, assuming you named your secrets container-registry : ```shell kubectl patch serviceaccount default -p \"{\\\"imagePullSecrets\\\": [{\\\"name\\\": \\\"container-registry\\\"}]}\" ``` Now, all the new pods that are created in the default namespace will include your credentials and have access to your container images in the private registry. What's next \u00b6 You can now create a service that uses your container images from the private registry. Learn how to create a Knative service .","title":"Using a Private Registry"},{"location":"serving/deploying/private-registry/#deploying-images-from-a-private-container-registry","text":"Learn how to configure your Knative cluster to deploy images from a private container registry. To share access to your private container images across multiple services and revisions, you create a list of Kubernetes secrets ( imagePullSecrets ) using your registry credentials, add that imagePullSecrets to your default service account , and then deploy those configurations to your Knative cluster.","title":"Deploying images from a private container registry"},{"location":"serving/deploying/private-registry/#before-you-begin","text":"You need: A Kubernetes cluster with Knative Serving installed . The credentials to the private container registry where your container images are stored.","title":"Before you begin"},{"location":"serving/deploying/private-registry/#configuring-your-credentials-in-knative","text":"Create a imagePullSecrets that contains your credentials as a list of secrets: kubectl create secret docker-registry [ REGISTRY-CRED-SECRETS ] \\ --docker-server =[ PRIVATE_REGISTRY_SERVER_URL ] \\ --docker-email =[ PRIVATE_REGISTRY_EMAIL ] \\ --docker-username =[ PRIVATE_REGISTRY_USER ] \\ --docker-password =[ PRIVATE_REGISTRY_PASSWORD ] Where - [REGISTRY-CRED-SECRETS] is the name that you want for your secrets ( imagePullSecrets object). For example, container-registry . [PRIVATE_REGISTRY_SERVER_URL] is the URL to the private registry where your container images are stored. Examples: - Google Container Registry: https://gcr.io/ - DockerHub https://docker.io/ [PRIVATE_REGISTRY_EMAIL] is your email address that is associated with the private registry. [PRIVATE_REGISTRY_USER] is the username that you use to access the private container registry. [PRIVATE_REGISTRY_PASSWORD] is the password that you use to access the private container registry. Example: kubectl create secret ` container-registry ` \\ --docker-server = https://gcr.io/ \\ --docker-email = my-account-email@address.com \\ --docker-username = my-grc-username \\ --docker-password = my-gcr-password Tip: After creating the imagePullSecrets , you can view those secret's by running: kubectl get secret [ REGISTRY-CRED-SECRETS ] --output = yaml Add the imagePullSecrets to your default service account in the default namespace. Note: By default, the default service account in each of the namespaces of your Knative cluster are use by your revisions unless serviceAccountName is specified. Run the following command to modify your default service account, assuming you named your secrets container-registry : ```shell kubectl patch serviceaccount default -p \"{\\\"imagePullSecrets\\\": [{\\\"name\\\": \\\"container-registry\\\"}]}\" ``` Now, all the new pods that are created in the default namespace will include your credentials and have access to your container images in the private registry.","title":"Configuring your credentials in Knative"},{"location":"serving/deploying/private-registry/#whats-next","text":"You can now create a service that uses your container images from the private registry. Learn how to create a Knative service .","title":"What's next"},{"location":"serving/load-balancing/","text":"Load balancing \u00b6 You can turn on Knative load balancing, by placing the Activator service in the request path to act as a load balancer. NOTE: To do this, you must first ensure that individual pod addressability is enabled. Activator pod selection \u00b6 Activator pods are scaled horizontally, so there may be multiple Activators in a deployment. In general, the system will perform best if the number of revision pods is larger than the number of Activator pods, and those numbers divide equally. Knative assigns a subset of Activators for each revision, depending on the revision size. More revision pods will mean a greater number of Activators for that revision. The Activator load balancing algorithm works as follows: If concurrency is unlimited, the request is sent to the better of two random choices. If concurrency is set to a value less or equal than 3, the Activator will send the request to the first pod that has capacity. Otherwise, requests will be balanced in a round robin fashion, with respect to container concurrency. For more information, see the documentation on concurrency . Configuring target burst capacity \u00b6 Target burst capacity is mainly responsible for determining whether the Activator is in the request path outside of scale-from-zero scenarios. Target burst capacity can be configured using a combination of the following parameters: Setting the targeted concurrency limits for the revision. See concurrency . Setting the target utilization parameters. See target utilization . Setting the target burst capacity. You can configure target burst capacity using the autoscaling.knative.dev/targetBurstCapacity annotation key in the config-autoscaler ConfigMap. See Setting the target burst capacity .","title":"Overview"},{"location":"serving/load-balancing/#load-balancing","text":"You can turn on Knative load balancing, by placing the Activator service in the request path to act as a load balancer. NOTE: To do this, you must first ensure that individual pod addressability is enabled.","title":"Load balancing"},{"location":"serving/load-balancing/#activator-pod-selection","text":"Activator pods are scaled horizontally, so there may be multiple Activators in a deployment. In general, the system will perform best if the number of revision pods is larger than the number of Activator pods, and those numbers divide equally. Knative assigns a subset of Activators for each revision, depending on the revision size. More revision pods will mean a greater number of Activators for that revision. The Activator load balancing algorithm works as follows: If concurrency is unlimited, the request is sent to the better of two random choices. If concurrency is set to a value less or equal than 3, the Activator will send the request to the first pod that has capacity. Otherwise, requests will be balanced in a round robin fashion, with respect to container concurrency. For more information, see the documentation on concurrency .","title":"Activator pod selection"},{"location":"serving/load-balancing/#configuring-target-burst-capacity","text":"Target burst capacity is mainly responsible for determining whether the Activator is in the request path outside of scale-from-zero scenarios. Target burst capacity can be configured using a combination of the following parameters: Setting the targeted concurrency limits for the revision. See concurrency . Setting the target utilization parameters. See target utilization . Setting the target burst capacity. You can configure target burst capacity using the autoscaling.knative.dev/targetBurstCapacity annotation key in the config-autoscaler ConfigMap. See Setting the target burst capacity .","title":"Configuring target burst capacity"},{"location":"serving/load-balancing/target-burst-capacity/","text":"Configuring target burst capacity \u00b6 Target burst capacity is a global and per-revision integer setting that determines the size of traffic burst a Knative application can handle without buffering. If a traffic burst is too large for the application to handle, the Activator service will be placed in the request path to protect the revision and optimize request load balancing. The Activator service is responsible for receiving and buffering requests for inactive revisions, or for revisions where a traffic burst is larger than the limits of what can be handled without buffering for that revision. It can also quickly spin up additional pods for capacity, and throttle how quickly requests are sent to pods. Target burst capacity can be configured using a combination of the following parameters: Setting the targeted concurrency limits for the revision. See concurrency . Setting the target utilization parameters. See target utilization . Setting the target burst capacity. You can configure target burst capacity using the autoscaling.knative.dev/targetBurstCapacity annotation key in the config-autoscaler ConfigMap. See Setting the target burst capacity . Setting the target burst capacity \u00b6 Global key: target-burst-capacity Per-revision annotation key: autoscaling.knative.dev/targetBurstCapacity Possible values: float ( 0 means the Activator is only in path when scaled to 0, -1 means the Activator is always in path) Default: 200 Example: Per Revision apiVersion : serving.knative.dev/v1 kind : Service metadata : annotations : name : <service_name> namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/targetBurstCapacity : \"200\" Global (ConfigMap) apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : target-burst-capacity : \"200\" Global (Operator) apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : target-burst-capacity : \"200\" If autoscaling.knative.dev/targetBurstCapacity is set to 0 , the Activator is only added to the request path during scale from zero scenarios, and ingress load balancing will be applied. NOTE: Ingress gateway load balancing requires additional configuration. For more information about load balancing using an ingress gateway, see the Serving API documentation. If autoscaling.knative.dev/targetBurstCapacity is set to -1 , the Activator is always in the request path, regardless of the revision size. If autoscaling.knative.dev/targetBurstCapacity is set to another integer, the Activator may be in the path, depending on the revision scale and load.","title":"Target Burst Capacity"},{"location":"serving/load-balancing/target-burst-capacity/#configuring-target-burst-capacity","text":"Target burst capacity is a global and per-revision integer setting that determines the size of traffic burst a Knative application can handle without buffering. If a traffic burst is too large for the application to handle, the Activator service will be placed in the request path to protect the revision and optimize request load balancing. The Activator service is responsible for receiving and buffering requests for inactive revisions, or for revisions where a traffic burst is larger than the limits of what can be handled without buffering for that revision. It can also quickly spin up additional pods for capacity, and throttle how quickly requests are sent to pods. Target burst capacity can be configured using a combination of the following parameters: Setting the targeted concurrency limits for the revision. See concurrency . Setting the target utilization parameters. See target utilization . Setting the target burst capacity. You can configure target burst capacity using the autoscaling.knative.dev/targetBurstCapacity annotation key in the config-autoscaler ConfigMap. See Setting the target burst capacity .","title":"Configuring target burst capacity"},{"location":"serving/load-balancing/target-burst-capacity/#setting-the-target-burst-capacity","text":"Global key: target-burst-capacity Per-revision annotation key: autoscaling.knative.dev/targetBurstCapacity Possible values: float ( 0 means the Activator is only in path when scaled to 0, -1 means the Activator is always in path) Default: 200 Example: Per Revision apiVersion : serving.knative.dev/v1 kind : Service metadata : annotations : name : <service_name> namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/targetBurstCapacity : \"200\" Global (ConfigMap) apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : target-burst-capacity : \"200\" Global (Operator) apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : target-burst-capacity : \"200\" If autoscaling.knative.dev/targetBurstCapacity is set to 0 , the Activator is only added to the request path during scale from zero scenarios, and ingress load balancing will be applied. NOTE: Ingress gateway load balancing requires additional configuration. For more information about load balancing using an ingress gateway, see the Serving API documentation. If autoscaling.knative.dev/targetBurstCapacity is set to -1 , the Activator is always in the request path, regardless of the revision size. If autoscaling.knative.dev/targetBurstCapacity is set to another integer, the Activator may be in the path, depending on the revision scale and load.","title":"Setting the target burst capacity"},{"location":"serving/samples/","text":"Use the following code samples to help you understand the various Knative Serving resources and how they can be applied across common use cases. Learn more about Knative Serving resources . See all Knative code samples Name Description Languages Hello World A quick introduction that highlights how to deploy an app using Knative Serving. C# , Go , Java (Spark) , Java (Spring) , Kotlin , Node.js , PHP , Python , Ruby , Scala , Shell Cloud Events A quick introduction that highlights how to send and receive Cloud Events. C# , Go , Node.js , Rust , Java (Vert.x) Advanced Deployment Simple blue/green-like application deployment pattern illustrating the process of updating a live application without dropping any traffic. YAML Autoscale A demonstration of the autoscaling capabilities of Knative. Go Github Webhook A simple webhook handler that demonstrates interacting with Github. Go gRPC A simple gRPC server. Go Knative Routing An example of mapping multiple Knative services to different paths under a single domain name using the Istio VirtualService concept. Go Knative Secrets A simple app that demonstrates how to use a Kubernetes secret as a Volume in Knative. Go REST API A simple Restful service that exposes an endpoint defined by an environment variable described in the Knative Configuration. Go Traffic Splitting This samples builds off the Creating a RESTful Service sample to illustrate applying a revision, then using that revision for manual traffic splitting. YAML Multi Container A quick introduction that highlights how to build and deploy an app using Knative Serving for multiple containers. Go","title":"Overview"},{"location":"serving/samples/_index/","text":"Macro Rendering Error \u00b6 TemplateSyntaxError : unexpected '%' Traceback (most recent call last): File \"/usr/local/lib/python3.9/site-packages/mkdocs_macros/plugin.py\", line 441, in render md_template = self.env.from_string(markdown) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 941, in from_string return cls.from_code(self, self.compile(source), globals, None) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 638, in compile self.handle_exception(source=source_hint) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 832, in handle_exception reraise(*rewrite_traceback_stack(source=source)) File \"/usr/local/lib/python3.9/site-packages/jinja2/_compat.py\", line 28, in reraise raise value.with_traceback(tb) File \"<unknown>\", line 3, in template jinja2.exceptions.TemplateSyntaxError: unexpected '%'","title":"Knative Serving code samples"},{"location":"serving/samples/_index/#macro-rendering-error","text":"TemplateSyntaxError : unexpected '%' Traceback (most recent call last): File \"/usr/local/lib/python3.9/site-packages/mkdocs_macros/plugin.py\", line 441, in render md_template = self.env.from_string(markdown) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 941, in from_string return cls.from_code(self, self.compile(source), globals, None) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 638, in compile self.handle_exception(source=source_hint) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 832, in handle_exception reraise(*rewrite_traceback_stack(source=source)) File \"/usr/local/lib/python3.9/site-packages/jinja2/_compat.py\", line 28, in reraise raise value.with_traceback(tb) File \"<unknown>\", line 3, in template jinja2.exceptions.TemplateSyntaxError: unexpected '%'","title":"Macro Rendering Error"},{"location":"serving/samples/blue-green-deployment/","text":"Routing and managing traffic with blue/green deployment \u00b6 This sample demonstrates updating an application to a new version using a blue/green traffic routing pattern. With Knative, you can safely reroute traffic from a live version of an application to a new version by changing the routing configuration. Before you begin \u00b6 You need: A Kubernetes cluster with Knative installed . (Optional) A custom domain configured for use with Knative. Note: The source code for the gcr.io/knative-samples/knative-route-demo image that is used in this sample, is located at https://github.com/mchmarny/knative-route-demo. Deploying Revision 1 (Blue) \u00b6 We'll be deploying an image of a sample application that displays the text \"App v1\" on a blue background. First, create a new file called blue-green-demo-config.yaml and copy this into it: apiVersion : serving.knative.dev/v1 kind : Configuration metadata : name : blue-green-demo namespace : default spec : template : spec : containers : - image : gcr.io/knative-samples/knative-route-demo:blue # The URL to the sample app docker image env : - name : T_VERSION value : \"blue\" Save the file, then deploy the configuration to your cluster: kubectl apply --filename blue-green-demo-config.yaml configuration \"blue-green-demo\" configured This will deploy the initial revision of the sample application. Before we can route traffic to this application we need to know the name of the initial revision which was just created. Using kubectl you can get it with the following command: kubectl get configurations blue-green-demo -o = jsonpath = '{.status.latestCreatedRevisionName}' The command above will return the name of the revision, it will be similar to blue-green-demo-lcfrd . In the rest of this document we will use this revision name, but yours will be different. To route inbound traffic to it, we need to define a route. Create a new file called blue-green-demo-route.yaml and copy the following YAML manifest into it (do not forget to edit the revision name): apiVersion : serving.knative.dev/v1 kind : Route metadata : name : blue-green-demo # The name of our route; appears in the URL to access the app namespace : default # The namespace we're working in; also appears in the URL to access the app spec : traffic : - revisionName : blue-green-demo-lcfrd percent : 100 # All traffic goes to this revision Save the file, then apply the route to your cluster: kubectl apply --filename blue-green-demo-route.yaml route \"blue-green-demo\" configured You'll now be able to view the sample app at the URL shown by: kubectl get route blue-green-demo Deploying Revision 2 (Green) \u00b6 Revision 2 of the sample application will display the text \"App v2\" on a green background. To create the new revision, we'll edit our existing configuration in blue-green-demo-config.yaml with an updated image and environment variables: apiVersion : serving.knative.dev/v1 kind : Configuration metadata : name : blue-green-demo # Configuration name is unchanged, since we're updating an existing Configuration namespace : default spec : template : spec : containers : - image : gcr.io/knative-samples/knative-route-demo:green # URL to the new version of the sample app docker image env : - name : T_VERSION value : \"green\" # Updated value for the T_VERSION environment variable Save the file, then apply the updated configuration to your cluster: kubectl apply --filename blue-green-demo-config.yaml configuration \"blue-green-demo\" configured Find the name of the second revision with the following command: kubectl get configurations blue-green-demo -o = jsonpath = '{.status.latestCreatedRevisionName}' In the rest of this document we will assume that the second revision is called blue-green-demo-m9548 , however yours will differ. Make sure to use the correct name of the second revision in the manifests that follow. At this point, the first revision ( blue-green-demo-lcfrd ) and the second revision ( blue-green-demo-m9548 ) will both be deployed and running. We can update our existing route to create a new (test) endpoint for the second revision while still sending all other traffic to the first revision. Edit blue-green-demo-route.yaml : apiVersion : serving.knative.dev/v1 kind : Route metadata : name : blue-green-demo # Route name is unchanged, since we're updating an existing Route namespace : default spec : traffic : - revisionName : blue-green-demo-lcfrd percent : 100 # All traffic still going to the first revision - revisionName : blue-green-demo-m9548 percent : 0 # 0% of traffic routed to the second revision tag : v2 # A named route Save the file, then apply the updated route to your cluster: kubectl apply --filename blue-green-demo-route.yaml route \"blue-green-demo\" configured Revision 2 of the app is staged at this point. That means: No traffic will be routed to revision 2 at the main URL, http://blue-green-demo.default.[YOUR_CUSTOM_DOMAIN].com Knative creates a new route named v2 for testing the newly deployed version. The URL of this can be seen in the status section of your Route. kubectl get route blue-green-demo --output jsonpath = \"{.status.traffic[*].url}\" This allows you to validate that the new version of the app is behaving as expected before switching any traffic over to it. Migrating traffic to the new revision \u00b6 We'll once again update our existing route to begin shifting traffic away from the first revision and toward the second. Edit blue-green-demo-route.yaml : apiVersion : serving.knative.dev/v1 kind : Route metadata : name : blue-green-demo # Updating our existing route namespace : default spec : traffic : - revisionName : blue-green-demo-lcfrd percent : 50 # Updating the percentage from 100 to 50 - revisionName : blue-green-demo-m9548 percent : 50 # Updating the percentage from 0 to 50 tag : v2 Save the file, then apply the updated route to your cluster: kubectl apply --filename blue-green-demo-route.yaml route \"blue-green-demo\" configured Refresh the original route ( http://blue-green-demo.default.[YOUR_CUSTOM_DOMAIN].com ) a few times to see that some traffic now goes to version 2 of the app. Note: This sample shows a 50/50 split to assure you don't have to refresh too much, but it's recommended to start with 1-2% of traffic in a production environment Rerouting all traffic to the new version \u00b6 Lastly, we'll update our existing route to finally shift all traffic to the second revision. Edit blue-green-demo-route.yaml : apiVersion : serving.knative.dev/v1 kind : Route metadata : name : blue-green-demo # Updating our existing route namespace : default spec : traffic : - revisionName : blue-green-demo-lcfrd percent : 0 tag : v1 # Adding a new named route for v1 - revisionName : blue-green-demo-m9548 percent : 100 # Named route for v2 has been removed, since we don't need it anymore Note: You can remove the first revision blue-green-demo-lcfrd instead of 0% of traffic when you will not roll back the revision anymore. Then the non-routeable revision object will be garbage collected. Save the file, then apply the updated route to your cluster: kubectl apply --filename blue-green-demo-route.yaml route \"blue-green-demo\" configured Refresh the original route ( http://blue-green-demo.default.[YOUR_CUSTOM_DOMAIN].com ) a few times to verify that no traffic is being routed to v1 of the app. We added a named route to v1 of the app, so you can now access it at the URL listed in the traffic block of the status section. To get the URL, enter the following command: kubectl get route blue-green-demo --output jsonpath = \"{.status.traffic[*].url}\" With all inbound traffic being directed to the second revision of the application, Knative will soon scale the first revision down to 0 running pods and the blue/green deployment can be considered complete. Using the named v1 route will reactivate a pod to serve any occasional requests intended specifically for the initial revision. Cleaning up \u00b6 To delete the sample app, enter the following commands: kubectl delete route blue-green-demo kubectl delete configuration blue-green-demo","title":"Blue Green Deployment"},{"location":"serving/samples/blue-green-deployment/#routing-and-managing-traffic-with-bluegreen-deployment","text":"This sample demonstrates updating an application to a new version using a blue/green traffic routing pattern. With Knative, you can safely reroute traffic from a live version of an application to a new version by changing the routing configuration.","title":"Routing and managing traffic with blue/green deployment"},{"location":"serving/samples/blue-green-deployment/#before-you-begin","text":"You need: A Kubernetes cluster with Knative installed . (Optional) A custom domain configured for use with Knative. Note: The source code for the gcr.io/knative-samples/knative-route-demo image that is used in this sample, is located at https://github.com/mchmarny/knative-route-demo.","title":"Before you begin"},{"location":"serving/samples/blue-green-deployment/#deploying-revision-1-blue","text":"We'll be deploying an image of a sample application that displays the text \"App v1\" on a blue background. First, create a new file called blue-green-demo-config.yaml and copy this into it: apiVersion : serving.knative.dev/v1 kind : Configuration metadata : name : blue-green-demo namespace : default spec : template : spec : containers : - image : gcr.io/knative-samples/knative-route-demo:blue # The URL to the sample app docker image env : - name : T_VERSION value : \"blue\" Save the file, then deploy the configuration to your cluster: kubectl apply --filename blue-green-demo-config.yaml configuration \"blue-green-demo\" configured This will deploy the initial revision of the sample application. Before we can route traffic to this application we need to know the name of the initial revision which was just created. Using kubectl you can get it with the following command: kubectl get configurations blue-green-demo -o = jsonpath = '{.status.latestCreatedRevisionName}' The command above will return the name of the revision, it will be similar to blue-green-demo-lcfrd . In the rest of this document we will use this revision name, but yours will be different. To route inbound traffic to it, we need to define a route. Create a new file called blue-green-demo-route.yaml and copy the following YAML manifest into it (do not forget to edit the revision name): apiVersion : serving.knative.dev/v1 kind : Route metadata : name : blue-green-demo # The name of our route; appears in the URL to access the app namespace : default # The namespace we're working in; also appears in the URL to access the app spec : traffic : - revisionName : blue-green-demo-lcfrd percent : 100 # All traffic goes to this revision Save the file, then apply the route to your cluster: kubectl apply --filename blue-green-demo-route.yaml route \"blue-green-demo\" configured You'll now be able to view the sample app at the URL shown by: kubectl get route blue-green-demo","title":"Deploying Revision 1 (Blue)"},{"location":"serving/samples/blue-green-deployment/#deploying-revision-2-green","text":"Revision 2 of the sample application will display the text \"App v2\" on a green background. To create the new revision, we'll edit our existing configuration in blue-green-demo-config.yaml with an updated image and environment variables: apiVersion : serving.knative.dev/v1 kind : Configuration metadata : name : blue-green-demo # Configuration name is unchanged, since we're updating an existing Configuration namespace : default spec : template : spec : containers : - image : gcr.io/knative-samples/knative-route-demo:green # URL to the new version of the sample app docker image env : - name : T_VERSION value : \"green\" # Updated value for the T_VERSION environment variable Save the file, then apply the updated configuration to your cluster: kubectl apply --filename blue-green-demo-config.yaml configuration \"blue-green-demo\" configured Find the name of the second revision with the following command: kubectl get configurations blue-green-demo -o = jsonpath = '{.status.latestCreatedRevisionName}' In the rest of this document we will assume that the second revision is called blue-green-demo-m9548 , however yours will differ. Make sure to use the correct name of the second revision in the manifests that follow. At this point, the first revision ( blue-green-demo-lcfrd ) and the second revision ( blue-green-demo-m9548 ) will both be deployed and running. We can update our existing route to create a new (test) endpoint for the second revision while still sending all other traffic to the first revision. Edit blue-green-demo-route.yaml : apiVersion : serving.knative.dev/v1 kind : Route metadata : name : blue-green-demo # Route name is unchanged, since we're updating an existing Route namespace : default spec : traffic : - revisionName : blue-green-demo-lcfrd percent : 100 # All traffic still going to the first revision - revisionName : blue-green-demo-m9548 percent : 0 # 0% of traffic routed to the second revision tag : v2 # A named route Save the file, then apply the updated route to your cluster: kubectl apply --filename blue-green-demo-route.yaml route \"blue-green-demo\" configured Revision 2 of the app is staged at this point. That means: No traffic will be routed to revision 2 at the main URL, http://blue-green-demo.default.[YOUR_CUSTOM_DOMAIN].com Knative creates a new route named v2 for testing the newly deployed version. The URL of this can be seen in the status section of your Route. kubectl get route blue-green-demo --output jsonpath = \"{.status.traffic[*].url}\" This allows you to validate that the new version of the app is behaving as expected before switching any traffic over to it.","title":"Deploying Revision 2 (Green)"},{"location":"serving/samples/blue-green-deployment/#migrating-traffic-to-the-new-revision","text":"We'll once again update our existing route to begin shifting traffic away from the first revision and toward the second. Edit blue-green-demo-route.yaml : apiVersion : serving.knative.dev/v1 kind : Route metadata : name : blue-green-demo # Updating our existing route namespace : default spec : traffic : - revisionName : blue-green-demo-lcfrd percent : 50 # Updating the percentage from 100 to 50 - revisionName : blue-green-demo-m9548 percent : 50 # Updating the percentage from 0 to 50 tag : v2 Save the file, then apply the updated route to your cluster: kubectl apply --filename blue-green-demo-route.yaml route \"blue-green-demo\" configured Refresh the original route ( http://blue-green-demo.default.[YOUR_CUSTOM_DOMAIN].com ) a few times to see that some traffic now goes to version 2 of the app. Note: This sample shows a 50/50 split to assure you don't have to refresh too much, but it's recommended to start with 1-2% of traffic in a production environment","title":"Migrating traffic to the new revision"},{"location":"serving/samples/blue-green-deployment/#rerouting-all-traffic-to-the-new-version","text":"Lastly, we'll update our existing route to finally shift all traffic to the second revision. Edit blue-green-demo-route.yaml : apiVersion : serving.knative.dev/v1 kind : Route metadata : name : blue-green-demo # Updating our existing route namespace : default spec : traffic : - revisionName : blue-green-demo-lcfrd percent : 0 tag : v1 # Adding a new named route for v1 - revisionName : blue-green-demo-m9548 percent : 100 # Named route for v2 has been removed, since we don't need it anymore Note: You can remove the first revision blue-green-demo-lcfrd instead of 0% of traffic when you will not roll back the revision anymore. Then the non-routeable revision object will be garbage collected. Save the file, then apply the updated route to your cluster: kubectl apply --filename blue-green-demo-route.yaml route \"blue-green-demo\" configured Refresh the original route ( http://blue-green-demo.default.[YOUR_CUSTOM_DOMAIN].com ) a few times to verify that no traffic is being routed to v1 of the app. We added a named route to v1 of the app, so you can now access it at the URL listed in the traffic block of the status section. To get the URL, enter the following command: kubectl get route blue-green-demo --output jsonpath = \"{.status.traffic[*].url}\" With all inbound traffic being directed to the second revision of the application, Knative will soon scale the first revision down to 0 running pods and the blue/green deployment can be considered complete. Using the named v1 route will reactivate a pod to serve any occasional requests intended specifically for the initial revision.","title":"Rerouting all traffic to the new version"},{"location":"serving/samples/blue-green-deployment/#cleaning-up","text":"To delete the sample app, enter the following commands: kubectl delete route blue-green-demo kubectl delete configuration blue-green-demo","title":"Cleaning up"},{"location":"serving/samples/cloudevents/_index/","text":"","title":"Knative Serving 'Cloud Events' samples"},{"location":"serving/samples/cloudevents/cloudevents-dotnet/","text":"Macro Rendering Error \u00b6 TemplateSyntaxError : unexpected '%' Traceback (most recent call last): File \"/usr/local/lib/python3.9/site-packages/mkdocs_macros/plugin.py\", line 441, in render md_template = self.env.from_string(markdown) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 941, in from_string return cls.from_code(self, self.compile(source), globals, None) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 638, in compile self.handle_exception(source=source_hint) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 832, in handle_exception reraise(*rewrite_traceback_stack(source=source)) File \"/usr/local/lib/python3.9/site-packages/jinja2/_compat.py\", line 28, in reraise raise value.with_traceback(tb) File \"<unknown>\", line 3, in template jinja2.exceptions.TemplateSyntaxError: unexpected '%'","title":"Cloud Events - .NET Core"},{"location":"serving/samples/cloudevents/cloudevents-dotnet/#macro-rendering-error","text":"TemplateSyntaxError : unexpected '%' Traceback (most recent call last): File \"/usr/local/lib/python3.9/site-packages/mkdocs_macros/plugin.py\", line 441, in render md_template = self.env.from_string(markdown) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 941, in from_string return cls.from_code(self, self.compile(source), globals, None) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 638, in compile self.handle_exception(source=source_hint) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 832, in handle_exception reraise(*rewrite_traceback_stack(source=source)) File \"/usr/local/lib/python3.9/site-packages/jinja2/_compat.py\", line 28, in reraise raise value.with_traceback(tb) File \"<unknown>\", line 3, in template jinja2.exceptions.TemplateSyntaxError: unexpected '%'","title":"Macro Rendering Error"},{"location":"serving/samples/cloudevents/cloudevents-go/","text":"Macro Rendering Error \u00b6 TemplateSyntaxError : unexpected '<' Traceback (most recent call last): File \"/usr/local/lib/python3.9/site-packages/mkdocs_macros/plugin.py\", line 441, in render md_template = self.env.from_string(markdown) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 941, in from_string return cls.from_code(self, self.compile(source), globals, None) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 638, in compile self.handle_exception(source=source_hint) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 832, in handle_exception reraise(*rewrite_traceback_stack(source=source)) File \"/usr/local/lib/python3.9/site-packages/jinja2/_compat.py\", line 28, in reraise raise value.with_traceback(tb) File \"<unknown>\", line 23, in template jinja2.exceptions.TemplateSyntaxError: unexpected '<'","title":"Cloud Events - Go"},{"location":"serving/samples/cloudevents/cloudevents-go/#macro-rendering-error","text":"TemplateSyntaxError : unexpected '<' Traceback (most recent call last): File \"/usr/local/lib/python3.9/site-packages/mkdocs_macros/plugin.py\", line 441, in render md_template = self.env.from_string(markdown) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 941, in from_string return cls.from_code(self, self.compile(source), globals, None) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 638, in compile self.handle_exception(source=source_hint) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 832, in handle_exception reraise(*rewrite_traceback_stack(source=source)) File \"/usr/local/lib/python3.9/site-packages/jinja2/_compat.py\", line 28, in reraise raise value.with_traceback(tb) File \"<unknown>\", line 23, in template jinja2.exceptions.TemplateSyntaxError: unexpected '<'","title":"Macro Rendering Error"},{"location":"serving/samples/cloudevents/cloudevents-nodejs/","text":"A simple web app written in Node.js that can receive and send Cloud Events that you can use for testing. It supports running in two modes: The default mode has the app reply to your input events with the output event, which is simplest for demonstrating things working in isolation, but is also the model for working for the Knative Eventing Broker concept. K_SINK mode has the app send events to the destination encoded in $K_SINK , which is useful to demonstrate how folks can synthesize events to send to a Service or Broker when not initiated by a Broker invocation (e.g. implementing an event source) The application will use $K_SINK -mode whenever the environment variable is specified. Follow the steps below to create the sample code and then deploy the app to your cluster. You can also download a working copy of the sample, by running the following commands: git clone -b \"v0.21.x\" https://github.com/knative/docs knative-docs cd knative-docs/docs/serving/samples/cloudevents/cloudevents-nodejs Before you begin \u00b6 A Kubernetes cluster with Knative installed and DNS configured. Follow the installation instructions if you need to create one. Docker installed and running on your local machine, and a Docker Hub account configured (we'll use it for a container registry). The Sample Code \u00b6 In the index.js file, you will see two key functions for the different modes of operation: const receiveAndSend = ( cloudEvent , res ) => { // This is called whenever an event is received if $K_SINK is set, and sends a new event // to the url in $K_SINK. } const receiveAndReply = ( cloudEvent , res ) => { // This is called whenever an event is received if $K_SINK is NOT set, and it replies with // the new event instead. } Build and Deploy the Application \u00b6 In the Dockerfile , you can see how the dependencies are installed using npm. You can build and push this to your registry of choice via: docker build -t <image> . docker push <image> YAML To deploy the Knative service, edit the service.yaml file and replace <registry/repository/image:tag> with the image you have just created. kubectl apply -f service.yaml Kn To deploy using the kn CLI: kn service create cloudevents-nodejs --image = <image> Testing the sample \u00b6 Get the URL for your Service with: $ kubectl get ksvc NAME URL LATESTCREATED LATESTREADY READY REASON cloudevents-nodejs http://cloudevents-nodejs.default.1.2.3.4.xip.io cloudevents-nodejs-ss5pj cloudevents-nodejs-ss5pj True Then send a cloud event to it with: $ curl -X POST \\ -H \"content-type: application/json\" \\ -H \"ce-specversion: 1.0\" \\ -H \"ce-source: curl-command\" \\ -H \"ce-type: curl.demo\" \\ -H \"ce-id: 123-abc\" \\ -d '{\"name\":\"Dave\"}' \\ http://cloudevents-nodejs.default.1.2.3.4.xip.io You will get back: { \"message\" : \"Hello, Dave\" } Removing the sample app deployment \u00b6 To remove the sample app from your cluster, delete the service. YAML Run: kubectl delete --filename service.yaml Kn Run: kn service delete cloudevents-nodejs","title":"NodeJS"},{"location":"serving/samples/cloudevents/cloudevents-nodejs/#before-you-begin","text":"A Kubernetes cluster with Knative installed and DNS configured. Follow the installation instructions if you need to create one. Docker installed and running on your local machine, and a Docker Hub account configured (we'll use it for a container registry).","title":"Before you begin"},{"location":"serving/samples/cloudevents/cloudevents-nodejs/#the-sample-code","text":"In the index.js file, you will see two key functions for the different modes of operation: const receiveAndSend = ( cloudEvent , res ) => { // This is called whenever an event is received if $K_SINK is set, and sends a new event // to the url in $K_SINK. } const receiveAndReply = ( cloudEvent , res ) => { // This is called whenever an event is received if $K_SINK is NOT set, and it replies with // the new event instead. }","title":"The Sample Code"},{"location":"serving/samples/cloudevents/cloudevents-nodejs/#build-and-deploy-the-application","text":"In the Dockerfile , you can see how the dependencies are installed using npm. You can build and push this to your registry of choice via: docker build -t <image> . docker push <image> YAML To deploy the Knative service, edit the service.yaml file and replace <registry/repository/image:tag> with the image you have just created. kubectl apply -f service.yaml Kn To deploy using the kn CLI: kn service create cloudevents-nodejs --image = <image>","title":"Build and Deploy the Application"},{"location":"serving/samples/cloudevents/cloudevents-nodejs/#testing-the-sample","text":"Get the URL for your Service with: $ kubectl get ksvc NAME URL LATESTCREATED LATESTREADY READY REASON cloudevents-nodejs http://cloudevents-nodejs.default.1.2.3.4.xip.io cloudevents-nodejs-ss5pj cloudevents-nodejs-ss5pj True Then send a cloud event to it with: $ curl -X POST \\ -H \"content-type: application/json\" \\ -H \"ce-specversion: 1.0\" \\ -H \"ce-source: curl-command\" \\ -H \"ce-type: curl.demo\" \\ -H \"ce-id: 123-abc\" \\ -d '{\"name\":\"Dave\"}' \\ http://cloudevents-nodejs.default.1.2.3.4.xip.io You will get back: { \"message\" : \"Hello, Dave\" }","title":"Testing the sample"},{"location":"serving/samples/cloudevents/cloudevents-nodejs/#removing-the-sample-app-deployment","text":"To remove the sample app from your cluster, delete the service. YAML Run: kubectl delete --filename service.yaml Kn Run: kn service delete cloudevents-nodejs","title":"Removing the sample app deployment"},{"location":"serving/samples/cloudevents/cloudevents-rust/","text":"Cloud Events - Rust \u00b6 A simple web app written in Rust using Actix web that can receive CloudEvents. It supports running in two modes: The default mode has the app reply to your input events with the output event, which is simplest for demonstrating things working in isolation, but is also the model for working for the Knative Eventing Broker concept. The input event is modified assigning a new source and type attribute. K_SINK mode has the app send events to the destination encoded in $K_SINK , which is useful to demonstrate how folks can synthesize events to send to a Service or Broker when not initiated by a Broker invocation (e.g. implementing an event source). The input event is modified assigning a new source and type attribute. The application will use $K_SINK -mode whenever the environment variable is specified. Follow the steps below to create the sample code and then deploy the app to your cluster. You can also download a working copy of the sample by running the following commands: git clone -b \"v0.21.x\" https://github.com/knative/docs knative-docs cd knative-docs/docs/serving/samples/cloudevents/cloudevents-rust Before you begin \u00b6 A Kubernetes cluster with Knative installed and DNS configured. Follow the installation instructions if you need to create one. Docker installed and running on your local machine, and a Docker Hub account configured (we'll use it for a container registry). This guide uses Rust musl toolchain to build the image in order to create really small docker images. To install the Rust toolchain: rustup . To install musl support: MUSL support for fully static binaries . Build and deploy the sample \u00b6 To build the binary, run: cargo build --target x86_64-unknown-linux-musl --release This will build a statically linked binary, in order to create an image from scratch. Now build the docker image: docker build -t <image> . YAML To deploy the Knative Service, look in the service.yaml and replace <image> with the deployed image name. Then run: kubectl apply -f service.yaml Kn If using kn to deploy: kn service create cloudevents-rust --image = <image> Testing the sample \u00b6 Get the URL for your Service with: $ kubectl get ksvc NAME URL LATESTCREATED LATESTREADY READY REASON cloudevents-rust http://cloudevents-rust.xip.io cloudevents-rust-vl8fq cloudevents-rust-vl8fq True Then send a CloudEvent to it with: $ curl \\ -X POST -v \\ -H \"content-type: application/json\" \\ -H \"ce-specversion: 1.0\" \\ -H \"ce-source: http://curl-command\" \\ -H \"ce-type: curl.demo\" \\ -H \"ce-id: 123-abc\" \\ -d '{\"name\":\"Dave\"}' \\ http://cloudevents-rust.xip.io You can also send CloudEvents spawning a temporary curl pod in your cluster with: $ kubectl run curl \\ --image = curlimages/curl --rm = true --restart = Never -ti -- \\ -X POST -v \\ -H \"content-type: application/json\" \\ -H \"ce-specversion: 1.0\" \\ -H \"ce-source: http://curl-command\" \\ -H \"ce-type: curl.demo\" \\ -H \"ce-id: 123-abc\" \\ -d '{\"name\":\"Dave\"}' \\ http://cloudevents-rust.default.svc You'll get as result: > POST / HTTP/1.1 > Host: localhost:8080 > User-Agent: curl/7.69.1 > Accept: */* > content-type: application/json > ce-specversion: 1 .0 > ce-source: http://curl-command > ce-type: curl.demo > ce-id: 123 -abc > Content-Length: 15 > < HTTP/1.1 200 OK < content-length: 15 < content-type: application/json < ce-specversion: 1 .0 < ce-id: 123 -abc < ce-type: dev.knative.docs.sample < ce-source: https://github.com/knative/docs/docs/serving/samples/cloudevents/cloudevents-rust < date: Sat, 23 May 2020 09 :00:01 GMT < { \"name\" : \"Dave\" } Removing the sample app deployment \u00b6 To remove the sample app from your cluster, delete the service. YAML Run: kubectl delete --filename service.yaml Kn Run: kn service delete cloudevents-rust","title":"Rust"},{"location":"serving/samples/cloudevents/cloudevents-rust/#cloud-events-rust","text":"A simple web app written in Rust using Actix web that can receive CloudEvents. It supports running in two modes: The default mode has the app reply to your input events with the output event, which is simplest for demonstrating things working in isolation, but is also the model for working for the Knative Eventing Broker concept. The input event is modified assigning a new source and type attribute. K_SINK mode has the app send events to the destination encoded in $K_SINK , which is useful to demonstrate how folks can synthesize events to send to a Service or Broker when not initiated by a Broker invocation (e.g. implementing an event source). The input event is modified assigning a new source and type attribute. The application will use $K_SINK -mode whenever the environment variable is specified. Follow the steps below to create the sample code and then deploy the app to your cluster. You can also download a working copy of the sample by running the following commands: git clone -b \"v0.21.x\" https://github.com/knative/docs knative-docs cd knative-docs/docs/serving/samples/cloudevents/cloudevents-rust","title":"Cloud Events - Rust"},{"location":"serving/samples/cloudevents/cloudevents-rust/#before-you-begin","text":"A Kubernetes cluster with Knative installed and DNS configured. Follow the installation instructions if you need to create one. Docker installed and running on your local machine, and a Docker Hub account configured (we'll use it for a container registry). This guide uses Rust musl toolchain to build the image in order to create really small docker images. To install the Rust toolchain: rustup . To install musl support: MUSL support for fully static binaries .","title":"Before you begin"},{"location":"serving/samples/cloudevents/cloudevents-rust/#build-and-deploy-the-sample","text":"To build the binary, run: cargo build --target x86_64-unknown-linux-musl --release This will build a statically linked binary, in order to create an image from scratch. Now build the docker image: docker build -t <image> . YAML To deploy the Knative Service, look in the service.yaml and replace <image> with the deployed image name. Then run: kubectl apply -f service.yaml Kn If using kn to deploy: kn service create cloudevents-rust --image = <image>","title":"Build and deploy the sample"},{"location":"serving/samples/cloudevents/cloudevents-rust/#testing-the-sample","text":"Get the URL for your Service with: $ kubectl get ksvc NAME URL LATESTCREATED LATESTREADY READY REASON cloudevents-rust http://cloudevents-rust.xip.io cloudevents-rust-vl8fq cloudevents-rust-vl8fq True Then send a CloudEvent to it with: $ curl \\ -X POST -v \\ -H \"content-type: application/json\" \\ -H \"ce-specversion: 1.0\" \\ -H \"ce-source: http://curl-command\" \\ -H \"ce-type: curl.demo\" \\ -H \"ce-id: 123-abc\" \\ -d '{\"name\":\"Dave\"}' \\ http://cloudevents-rust.xip.io You can also send CloudEvents spawning a temporary curl pod in your cluster with: $ kubectl run curl \\ --image = curlimages/curl --rm = true --restart = Never -ti -- \\ -X POST -v \\ -H \"content-type: application/json\" \\ -H \"ce-specversion: 1.0\" \\ -H \"ce-source: http://curl-command\" \\ -H \"ce-type: curl.demo\" \\ -H \"ce-id: 123-abc\" \\ -d '{\"name\":\"Dave\"}' \\ http://cloudevents-rust.default.svc You'll get as result: > POST / HTTP/1.1 > Host: localhost:8080 > User-Agent: curl/7.69.1 > Accept: */* > content-type: application/json > ce-specversion: 1 .0 > ce-source: http://curl-command > ce-type: curl.demo > ce-id: 123 -abc > Content-Length: 15 > < HTTP/1.1 200 OK < content-length: 15 < content-type: application/json < ce-specversion: 1 .0 < ce-id: 123 -abc < ce-type: dev.knative.docs.sample < ce-source: https://github.com/knative/docs/docs/serving/samples/cloudevents/cloudevents-rust < date: Sat, 23 May 2020 09 :00:01 GMT < { \"name\" : \"Dave\" }","title":"Testing the sample"},{"location":"serving/samples/cloudevents/cloudevents-rust/#removing-the-sample-app-deployment","text":"To remove the sample app from your cluster, delete the service. YAML Run: kubectl delete --filename service.yaml Kn Run: kn service delete cloudevents-rust","title":"Removing the sample app deployment"},{"location":"serving/samples/cloudevents/cloudevents-spring/","text":"Macro Rendering Error \u00b6 TemplateSyntaxError : unexpected '<' Traceback (most recent call last): File \"/usr/local/lib/python3.9/site-packages/mkdocs_macros/plugin.py\", line 441, in render md_template = self.env.from_string(markdown) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 941, in from_string return cls.from_code(self, self.compile(source), globals, None) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 638, in compile self.handle_exception(source=source_hint) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 832, in handle_exception reraise(*rewrite_traceback_stack(source=source)) File \"/usr/local/lib/python3.9/site-packages/jinja2/_compat.py\", line 28, in reraise raise value.with_traceback(tb) File \"<unknown>\", line 25, in template jinja2.exceptions.TemplateSyntaxError: unexpected '<'","title":"Cloud Events - Java and Spring"},{"location":"serving/samples/cloudevents/cloudevents-spring/#macro-rendering-error","text":"TemplateSyntaxError : unexpected '<' Traceback (most recent call last): File \"/usr/local/lib/python3.9/site-packages/mkdocs_macros/plugin.py\", line 441, in render md_template = self.env.from_string(markdown) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 941, in from_string return cls.from_code(self, self.compile(source), globals, None) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 638, in compile self.handle_exception(source=source_hint) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 832, in handle_exception reraise(*rewrite_traceback_stack(source=source)) File \"/usr/local/lib/python3.9/site-packages/jinja2/_compat.py\", line 28, in reraise raise value.with_traceback(tb) File \"<unknown>\", line 25, in template jinja2.exceptions.TemplateSyntaxError: unexpected '<'","title":"Macro Rendering Error"},{"location":"serving/samples/cloudevents/cloudevents-vertx/","text":"Vert.x + CloudEvents + Knative \u00b6 A simple web app written in Java using Vert.x that can receive CloudEvents. It supports running in two modes: The default mode has the app reply to your input events with the output event, which is simplest for demonstrating things working in isolation, but is also the model for working for the Knative Eventing Broker concept. The input event is modified assigning a new source and type attribute. K_SINK mode has the app send events to the destination encoded in $K_SINK , which is useful to demonstrate how folks can synthesize events to send to a Service or Broker when not initiated by a Broker invocation (e.g. implementing an event source). The input event is modified assigning a new source and type attribute. The application will use $K_SINK -mode whenever the environment variable is specified. Follow the steps below to create the sample code and then deploy the app to your cluster. You can also download a working copy of the sample, by running the following commands: git clone -b \"v0.21.x\" https://github.com/knative/docs knative-docs cd knative-docs/docs/serving/samples/cloudevents/cloudevents-vertx Before you begin \u00b6 A Kubernetes cluster with Knative installed and DNS configured. Follow the installation instructions if you need to create one. Docker installed and running on your local machine, and a Docker Hub account configured (we'll use it for a container registry). Build and deploy the sample \u00b6 To build the image, run: mvn compile jib:build -Dimage = <image_name> To deploy the Knative Service, look in the service.yaml and replace <image> with the deployed image name. Then run: kubectl apply -f service.yaml Testing the sample \u00b6 Get the URL for your Service with: $ kubectl get ksvc NAME URL LATESTCREATED LATESTREADY READY REASON cloudevents-vertx http://cloudevents-java.xip.io cloudevents-vertx-86h28 cloudevents-vertx-86h28 True Then send a CloudEvent to it with: $ curl \\ -X POST -v \\ -H \"content-type: application/json\" \\ -H \"ce-specversion: 1.0\" \\ -H \"ce-source: http://curl-command\" \\ -H \"ce-type: curl.demo\" \\ -H \"ce-id: 123-abc\" \\ -d '{\"name\":\"Dave\"}' \\ http://cloudevents-java.xip.io You can also send CloudEvents spawning a temporary curl pod in your cluster with: $ kubectl run curl \\ --image = curlimages/curl --rm = true --restart = Never -ti -- \\ -X POST -v \\ -H \"content-type: application/json\" \\ -H \"ce-specversion: 1.0\" \\ -H \"ce-source: http://curl-command\" \\ -H \"ce-type: curl.demo\" \\ -H \"ce-id: 123-abc\" \\ -d '{\"name\":\"Dave\"}' \\ http://cloudevents-java.default.svc You'll see on the console: > POST / HTTP/1.1 > Host: localhost:8080 > User-Agent: curl/7.69.1 > Accept: */* > content-type: application/json > ce-specversion: 1 .0 > ce-source: http://curl-command > ce-type: curl.demo > ce-id: 123 -abc > Content-Length: 15 > < HTTP/1.1 202 Accepted < ce-specversion: 1 .0 < ce-id: 123 -abc < ce-source: https://github.com/knative/docs/docs/serving/samples/cloudevents/cloudevents-vertx < ce-type: curl.demo < content-type: application/json < content-length: 15 < { \"name\" : \"Dave\" } Removing the sample app deployment \u00b6 To remove the sample app from your cluster, delete the service record: kubectl delete --filename service.yaml","title":"Java (Vert.x)"},{"location":"serving/samples/cloudevents/cloudevents-vertx/#vertx-cloudevents-knative","text":"A simple web app written in Java using Vert.x that can receive CloudEvents. It supports running in two modes: The default mode has the app reply to your input events with the output event, which is simplest for demonstrating things working in isolation, but is also the model for working for the Knative Eventing Broker concept. The input event is modified assigning a new source and type attribute. K_SINK mode has the app send events to the destination encoded in $K_SINK , which is useful to demonstrate how folks can synthesize events to send to a Service or Broker when not initiated by a Broker invocation (e.g. implementing an event source). The input event is modified assigning a new source and type attribute. The application will use $K_SINK -mode whenever the environment variable is specified. Follow the steps below to create the sample code and then deploy the app to your cluster. You can also download a working copy of the sample, by running the following commands: git clone -b \"v0.21.x\" https://github.com/knative/docs knative-docs cd knative-docs/docs/serving/samples/cloudevents/cloudevents-vertx","title":"Vert.x + CloudEvents + Knative"},{"location":"serving/samples/cloudevents/cloudevents-vertx/#before-you-begin","text":"A Kubernetes cluster with Knative installed and DNS configured. Follow the installation instructions if you need to create one. Docker installed and running on your local machine, and a Docker Hub account configured (we'll use it for a container registry).","title":"Before you begin"},{"location":"serving/samples/cloudevents/cloudevents-vertx/#build-and-deploy-the-sample","text":"To build the image, run: mvn compile jib:build -Dimage = <image_name> To deploy the Knative Service, look in the service.yaml and replace <image> with the deployed image name. Then run: kubectl apply -f service.yaml","title":"Build and deploy the sample"},{"location":"serving/samples/cloudevents/cloudevents-vertx/#testing-the-sample","text":"Get the URL for your Service with: $ kubectl get ksvc NAME URL LATESTCREATED LATESTREADY READY REASON cloudevents-vertx http://cloudevents-java.xip.io cloudevents-vertx-86h28 cloudevents-vertx-86h28 True Then send a CloudEvent to it with: $ curl \\ -X POST -v \\ -H \"content-type: application/json\" \\ -H \"ce-specversion: 1.0\" \\ -H \"ce-source: http://curl-command\" \\ -H \"ce-type: curl.demo\" \\ -H \"ce-id: 123-abc\" \\ -d '{\"name\":\"Dave\"}' \\ http://cloudevents-java.xip.io You can also send CloudEvents spawning a temporary curl pod in your cluster with: $ kubectl run curl \\ --image = curlimages/curl --rm = true --restart = Never -ti -- \\ -X POST -v \\ -H \"content-type: application/json\" \\ -H \"ce-specversion: 1.0\" \\ -H \"ce-source: http://curl-command\" \\ -H \"ce-type: curl.demo\" \\ -H \"ce-id: 123-abc\" \\ -d '{\"name\":\"Dave\"}' \\ http://cloudevents-java.default.svc You'll see on the console: > POST / HTTP/1.1 > Host: localhost:8080 > User-Agent: curl/7.69.1 > Accept: */* > content-type: application/json > ce-specversion: 1 .0 > ce-source: http://curl-command > ce-type: curl.demo > ce-id: 123 -abc > Content-Length: 15 > < HTTP/1.1 202 Accepted < ce-specversion: 1 .0 < ce-id: 123 -abc < ce-source: https://github.com/knative/docs/docs/serving/samples/cloudevents/cloudevents-vertx < ce-type: curl.demo < content-type: application/json < content-length: 15 < { \"name\" : \"Dave\" }","title":"Testing the sample"},{"location":"serving/samples/cloudevents/cloudevents-vertx/#removing-the-sample-app-deployment","text":"To remove the sample app from your cluster, delete the service record: kubectl delete --filename service.yaml","title":"Removing the sample app deployment"},{"location":"serving/samples/gitwebhook-go/","text":"Macro Rendering Error \u00b6 TemplateSyntaxError : unexpected '%' Traceback (most recent call last): File \"/usr/local/lib/python3.9/site-packages/mkdocs_macros/plugin.py\", line 441, in render md_template = self.env.from_string(markdown) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 941, in from_string return cls.from_code(self, self.compile(source), globals, None) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 638, in compile self.handle_exception(source=source_hint) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 832, in handle_exception reraise(*rewrite_traceback_stack(source=source)) File \"/usr/local/lib/python3.9/site-packages/jinja2/_compat.py\", line 28, in reraise raise value.with_traceback(tb) File \"<unknown>\", line 3, in template jinja2.exceptions.TemplateSyntaxError: unexpected '%'","title":"GitHub webhook sample - Go"},{"location":"serving/samples/gitwebhook-go/#macro-rendering-error","text":"TemplateSyntaxError : unexpected '%' Traceback (most recent call last): File \"/usr/local/lib/python3.9/site-packages/mkdocs_macros/plugin.py\", line 441, in render md_template = self.env.from_string(markdown) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 941, in from_string return cls.from_code(self, self.compile(source), globals, None) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 638, in compile self.handle_exception(source=source_hint) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 832, in handle_exception reraise(*rewrite_traceback_stack(source=source)) File \"/usr/local/lib/python3.9/site-packages/jinja2/_compat.py\", line 28, in reraise raise value.with_traceback(tb) File \"<unknown>\", line 3, in template jinja2.exceptions.TemplateSyntaxError: unexpected '%'","title":"Macro Rendering Error"},{"location":"serving/samples/grpc-ping-go/","text":"Macro Rendering Error \u00b6 TemplateSyntaxError : unexpected '%' Traceback (most recent call last): File \"/usr/local/lib/python3.9/site-packages/mkdocs_macros/plugin.py\", line 441, in render md_template = self.env.from_string(markdown) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 941, in from_string return cls.from_code(self, self.compile(source), globals, None) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 638, in compile self.handle_exception(source=source_hint) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 832, in handle_exception reraise(*rewrite_traceback_stack(source=source)) File \"/usr/local/lib/python3.9/site-packages/jinja2/_compat.py\", line 28, in reraise raise value.with_traceback(tb) File \"<unknown>\", line 3, in template jinja2.exceptions.TemplateSyntaxError: unexpected '%'","title":"gRPC Server - Go"},{"location":"serving/samples/grpc-ping-go/#macro-rendering-error","text":"TemplateSyntaxError : unexpected '%' Traceback (most recent call last): File \"/usr/local/lib/python3.9/site-packages/mkdocs_macros/plugin.py\", line 441, in render md_template = self.env.from_string(markdown) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 941, in from_string return cls.from_code(self, self.compile(source), globals, None) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 638, in compile self.handle_exception(source=source_hint) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 832, in handle_exception reraise(*rewrite_traceback_stack(source=source)) File \"/usr/local/lib/python3.9/site-packages/jinja2/_compat.py\", line 28, in reraise raise value.with_traceback(tb) File \"<unknown>\", line 3, in template jinja2.exceptions.TemplateSyntaxError: unexpected '%'","title":"Macro Rendering Error"},{"location":"serving/samples/hello-world/_index/","text":"","title":"Knative 'Hello World' samples"},{"location":"serving/samples/hello-world/helloworld-csharp/","text":"Macro Rendering Error \u00b6 TemplateSyntaxError : unexpected '%' Traceback (most recent call last): File \"/usr/local/lib/python3.9/site-packages/mkdocs_macros/plugin.py\", line 441, in render md_template = self.env.from_string(markdown) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 941, in from_string return cls.from_code(self, self.compile(source), globals, None) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 638, in compile self.handle_exception(source=source_hint) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 832, in handle_exception reraise(*rewrite_traceback_stack(source=source)) File \"/usr/local/lib/python3.9/site-packages/jinja2/_compat.py\", line 28, in reraise raise value.with_traceback(tb) File \"<unknown>\", line 3, in template jinja2.exceptions.TemplateSyntaxError: unexpected '%'","title":"Hello world - .NET Core"},{"location":"serving/samples/hello-world/helloworld-csharp/#macro-rendering-error","text":"TemplateSyntaxError : unexpected '%' Traceback (most recent call last): File \"/usr/local/lib/python3.9/site-packages/mkdocs_macros/plugin.py\", line 441, in render md_template = self.env.from_string(markdown) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 941, in from_string return cls.from_code(self, self.compile(source), globals, None) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 638, in compile self.handle_exception(source=source_hint) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 832, in handle_exception reraise(*rewrite_traceback_stack(source=source)) File \"/usr/local/lib/python3.9/site-packages/jinja2/_compat.py\", line 28, in reraise raise value.with_traceback(tb) File \"<unknown>\", line 3, in template jinja2.exceptions.TemplateSyntaxError: unexpected '%'","title":"Macro Rendering Error"},{"location":"serving/samples/hello-world/helloworld-go/","text":"Hello World - Go \u00b6 This guide describes the steps required to to create the helloworld-go sample app and deploy it to your cluster. The sample app reads a TARGET environment variable, and prints Hello ${TARGET}! . If TARGET is not specified, World is used as the default value. Prerequisites \u00b6 You will need: - A Kubernetes cluster with Knative installed and DNS configured . - Docker installed and running on your local machine, and a Docker Hub account configured. - Optional: You can use the Knative CLI client kn to simplify resource creation and deployment. Alternatively, you can use kubectl to apply resource files directly. Building \u00b6 Create a basic web server which listens on port 8080, by copying the following code into a new file named helloworld.go : package main import ( \"fmt\" \"log\" \"net/http\" \"os\" ) func handler ( w http . ResponseWriter , r * http . Request ) { log . Print ( \"helloworld: received a request\" ) target := os . Getenv ( \"TARGET\" ) if target == \"\" { target = \"World\" } fmt . Fprintf ( w , \"Hello %s!\\n\" , target ) } func main () { log . Print ( \"helloworld: starting server...\" ) http . HandleFunc ( \"/\" , handler ) port := os . Getenv ( \"PORT\" ) if port == \"\" { port = \"8080\" } log . Printf ( \"helloworld: listening on port %s\" , port ) log . Fatal ( http . ListenAndServe ( fmt . Sprintf ( \":%s\" , port ), nil )) } You can also download a working copy of the sample, by running the following commands: git clone -b \"v0.21.x\" https://github.com/knative/docs knative-docs cd knative-docs/docs/serving/samples/hello-world/helloworld-go Navigate to your project directory and copy the following code into a new file named Dockerfile : # Use the official Golang image to create a build artifact. # This is based on Debian and sets the GOPATH to /go. FROM golang:1.13 as builder # Create and change to the app directory. WORKDIR /app # Retrieve application dependencies using go modules. # Allows container builds to reuse downloaded dependencies. COPY go.* ./ RUN go mod download # Copy local code to the container image. COPY . ./ # Build the binary. # -mod=readonly ensures immutable go.mod and go.sum in container builds. RUN CGO_ENABLED = 0 GOOS = linux go build -mod = readonly -v -o server # Use the official Alpine image for a lean production container. # https://hub.docker.com/_/alpine # https://docs.docker.com/develop/develop-images/multistage-build/#use-multi-stage-builds FROM alpine:3 RUN apk add --no-cache ca-certificates # Copy the binary to the production image from the builder stage. COPY --from = builder /app/server /server # Run the web service on container startup. CMD [ \"/server\" ] Use the Go tool to create a go.mod manifest. go mod init github.com/knative/docs/docs/serving/samples/hello-world/helloworld-go Deploying \u00b6 To build the sample code into a container, and push using Docker Hub, enter the following commands and replace {username} with your Docker Hub username: # Build the container on your local machine docker build -t { username } /helloworld-go . # Push the container to docker registry docker push { username } /helloworld-go After the build has completed and the container is pushed to docker hub, you can deploy the app into your cluster. Choose one of the following methods: YAML Create a new file, service.yaml and copy the following service definition into the file. Make sure to replace {username} with your Docker Hub username. apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : spec : containers : - image : docker.io/{username}/helloworld-go env : - name : TARGET value : \"Go Sample v1\" Check that the container image value in the service.yaml file matches the container you built in the previous step. Apply the configuration using kubectl : kubectl apply --filename service.yaml After your service is created, Knative will perform the following steps: Create a new immutable revision for this version of the app. Network programming to create a route, ingress, service, and load balance for your app. Automatically scale your pods up and down (including to zero active pods). Run the following command to find the domain URL for your service: kubectl get ksvc helloworld-go --output = custom-columns = NAME:.metadata.name,URL:.status.url Example: NAME URL helloworld-go http://helloworld-go.default.1.2.3.4.xip.io Kn Use kn to deploy the service: kn service create helloworld-go --image = docker.io/ { username } /helloworld-go --env TARGET = \"Go Sample v1\" You should see output like this: Creating service 'helloworld-go' in namespace 'default' : 0 .031s The Configuration is still working to reflect the latest desired specification. 0 .051s The Route is still working to reflect the latest desired specification. 0 .076s Configuration \"helloworld-go\" is waiting for a Revision to become ready. 15 .694s ... 15 .738s Ingress has not yet been reconciled. 15 .784s Waiting for Envoys to receive Endpoints data. 16 .066s Waiting for load balancer to be ready 16 .237s Ready to serve. Service 'helloworld-go' created to latest revision 'helloworld-go-jjzgd-1' is available at URL: http://helloworld-go.default.1.2.3.4.xip.io You can then access your service through the resulting URL. Verifying \u00b6 Now you can make a request to your app and see the result. Replace the URL below with the URL returned in the previous command. curl http://helloworld-go.default.1.2.3.4.xip.io Hello Go Sample v1! Note: Add -v option to get more detail if the curl command failed. Removing \u00b6 To remove the sample app from your cluster, delete the service record: YAML kubectl delete --filename service.yaml Kn kn service delete helloworld-go","title":"Hello World Go"},{"location":"serving/samples/hello-world/helloworld-go/#hello-world-go","text":"This guide describes the steps required to to create the helloworld-go sample app and deploy it to your cluster. The sample app reads a TARGET environment variable, and prints Hello ${TARGET}! . If TARGET is not specified, World is used as the default value.","title":"Hello World - Go"},{"location":"serving/samples/hello-world/helloworld-go/#prerequisites","text":"You will need: - A Kubernetes cluster with Knative installed and DNS configured . - Docker installed and running on your local machine, and a Docker Hub account configured. - Optional: You can use the Knative CLI client kn to simplify resource creation and deployment. Alternatively, you can use kubectl to apply resource files directly.","title":"Prerequisites"},{"location":"serving/samples/hello-world/helloworld-go/#building","text":"Create a basic web server which listens on port 8080, by copying the following code into a new file named helloworld.go : package main import ( \"fmt\" \"log\" \"net/http\" \"os\" ) func handler ( w http . ResponseWriter , r * http . Request ) { log . Print ( \"helloworld: received a request\" ) target := os . Getenv ( \"TARGET\" ) if target == \"\" { target = \"World\" } fmt . Fprintf ( w , \"Hello %s!\\n\" , target ) } func main () { log . Print ( \"helloworld: starting server...\" ) http . HandleFunc ( \"/\" , handler ) port := os . Getenv ( \"PORT\" ) if port == \"\" { port = \"8080\" } log . Printf ( \"helloworld: listening on port %s\" , port ) log . Fatal ( http . ListenAndServe ( fmt . Sprintf ( \":%s\" , port ), nil )) } You can also download a working copy of the sample, by running the following commands: git clone -b \"v0.21.x\" https://github.com/knative/docs knative-docs cd knative-docs/docs/serving/samples/hello-world/helloworld-go Navigate to your project directory and copy the following code into a new file named Dockerfile : # Use the official Golang image to create a build artifact. # This is based on Debian and sets the GOPATH to /go. FROM golang:1.13 as builder # Create and change to the app directory. WORKDIR /app # Retrieve application dependencies using go modules. # Allows container builds to reuse downloaded dependencies. COPY go.* ./ RUN go mod download # Copy local code to the container image. COPY . ./ # Build the binary. # -mod=readonly ensures immutable go.mod and go.sum in container builds. RUN CGO_ENABLED = 0 GOOS = linux go build -mod = readonly -v -o server # Use the official Alpine image for a lean production container. # https://hub.docker.com/_/alpine # https://docs.docker.com/develop/develop-images/multistage-build/#use-multi-stage-builds FROM alpine:3 RUN apk add --no-cache ca-certificates # Copy the binary to the production image from the builder stage. COPY --from = builder /app/server /server # Run the web service on container startup. CMD [ \"/server\" ] Use the Go tool to create a go.mod manifest. go mod init github.com/knative/docs/docs/serving/samples/hello-world/helloworld-go","title":"Building"},{"location":"serving/samples/hello-world/helloworld-go/#deploying","text":"To build the sample code into a container, and push using Docker Hub, enter the following commands and replace {username} with your Docker Hub username: # Build the container on your local machine docker build -t { username } /helloworld-go . # Push the container to docker registry docker push { username } /helloworld-go After the build has completed and the container is pushed to docker hub, you can deploy the app into your cluster. Choose one of the following methods: YAML Create a new file, service.yaml and copy the following service definition into the file. Make sure to replace {username} with your Docker Hub username. apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : spec : containers : - image : docker.io/{username}/helloworld-go env : - name : TARGET value : \"Go Sample v1\" Check that the container image value in the service.yaml file matches the container you built in the previous step. Apply the configuration using kubectl : kubectl apply --filename service.yaml After your service is created, Knative will perform the following steps: Create a new immutable revision for this version of the app. Network programming to create a route, ingress, service, and load balance for your app. Automatically scale your pods up and down (including to zero active pods). Run the following command to find the domain URL for your service: kubectl get ksvc helloworld-go --output = custom-columns = NAME:.metadata.name,URL:.status.url Example: NAME URL helloworld-go http://helloworld-go.default.1.2.3.4.xip.io Kn Use kn to deploy the service: kn service create helloworld-go --image = docker.io/ { username } /helloworld-go --env TARGET = \"Go Sample v1\" You should see output like this: Creating service 'helloworld-go' in namespace 'default' : 0 .031s The Configuration is still working to reflect the latest desired specification. 0 .051s The Route is still working to reflect the latest desired specification. 0 .076s Configuration \"helloworld-go\" is waiting for a Revision to become ready. 15 .694s ... 15 .738s Ingress has not yet been reconciled. 15 .784s Waiting for Envoys to receive Endpoints data. 16 .066s Waiting for load balancer to be ready 16 .237s Ready to serve. Service 'helloworld-go' created to latest revision 'helloworld-go-jjzgd-1' is available at URL: http://helloworld-go.default.1.2.3.4.xip.io You can then access your service through the resulting URL.","title":"Deploying"},{"location":"serving/samples/hello-world/helloworld-go/#verifying","text":"Now you can make a request to your app and see the result. Replace the URL below with the URL returned in the previous command. curl http://helloworld-go.default.1.2.3.4.xip.io Hello Go Sample v1! Note: Add -v option to get more detail if the curl command failed.","title":"Verifying"},{"location":"serving/samples/hello-world/helloworld-go/#removing","text":"To remove the sample app from your cluster, delete the service record: YAML kubectl delete --filename service.yaml Kn kn service delete helloworld-go","title":"Removing"},{"location":"serving/samples/hello-world/helloworld-java-spark/","text":"Macro Rendering Error \u00b6 TemplateSyntaxError : unexpected '<' Traceback (most recent call last): File \"/usr/local/lib/python3.9/site-packages/mkdocs_macros/plugin.py\", line 441, in render md_template = self.env.from_string(markdown) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 941, in from_string return cls.from_code(self, self.compile(source), globals, None) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 638, in compile self.handle_exception(source=source_hint) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 832, in handle_exception reraise(*rewrite_traceback_stack(source=source)) File \"/usr/local/lib/python3.9/site-packages/jinja2/_compat.py\", line 28, in reraise raise value.with_traceback(tb) File \"<unknown>\", line 23, in template jinja2.exceptions.TemplateSyntaxError: unexpected '<'","title":"Hello World - Spark Java Framework"},{"location":"serving/samples/hello-world/helloworld-java-spark/#macro-rendering-error","text":"TemplateSyntaxError : unexpected '<' Traceback (most recent call last): File \"/usr/local/lib/python3.9/site-packages/mkdocs_macros/plugin.py\", line 441, in render md_template = self.env.from_string(markdown) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 941, in from_string return cls.from_code(self, self.compile(source), globals, None) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 638, in compile self.handle_exception(source=source_hint) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 832, in handle_exception reraise(*rewrite_traceback_stack(source=source)) File \"/usr/local/lib/python3.9/site-packages/jinja2/_compat.py\", line 28, in reraise raise value.with_traceback(tb) File \"<unknown>\", line 23, in template jinja2.exceptions.TemplateSyntaxError: unexpected '<'","title":"Macro Rendering Error"},{"location":"serving/samples/hello-world/helloworld-java-spring/","text":"Macro Rendering Error \u00b6 TemplateSyntaxError : unexpected '<' Traceback (most recent call last): File \"/usr/local/lib/python3.9/site-packages/mkdocs_macros/plugin.py\", line 441, in render md_template = self.env.from_string(markdown) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 941, in from_string return cls.from_code(self, self.compile(source), globals, None) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 638, in compile self.handle_exception(source=source_hint) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 832, in handle_exception reraise(*rewrite_traceback_stack(source=source)) File \"/usr/local/lib/python3.9/site-packages/jinja2/_compat.py\", line 28, in reraise raise value.with_traceback(tb) File \"<unknown>\", line 10, in template jinja2.exceptions.TemplateSyntaxError: unexpected '<'","title":"Hello World - Spring Boot Java"},{"location":"serving/samples/hello-world/helloworld-java-spring/#macro-rendering-error","text":"TemplateSyntaxError : unexpected '<' Traceback (most recent call last): File \"/usr/local/lib/python3.9/site-packages/mkdocs_macros/plugin.py\", line 441, in render md_template = self.env.from_string(markdown) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 941, in from_string return cls.from_code(self, self.compile(source), globals, None) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 638, in compile self.handle_exception(source=source_hint) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 832, in handle_exception reraise(*rewrite_traceback_stack(source=source)) File \"/usr/local/lib/python3.9/site-packages/jinja2/_compat.py\", line 28, in reraise raise value.with_traceback(tb) File \"<unknown>\", line 10, in template jinja2.exceptions.TemplateSyntaxError: unexpected '<'","title":"Macro Rendering Error"},{"location":"serving/samples/hello-world/helloworld-kotlin/","text":"Macro Rendering Error \u00b6 TemplateSyntaxError : unexpected '%' Traceback (most recent call last): File \"/usr/local/lib/python3.9/site-packages/mkdocs_macros/plugin.py\", line 441, in render md_template = self.env.from_string(markdown) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 941, in from_string return cls.from_code(self, self.compile(source), globals, None) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 638, in compile self.handle_exception(source=source_hint) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 832, in handle_exception reraise(*rewrite_traceback_stack(source=source)) File \"/usr/local/lib/python3.9/site-packages/jinja2/_compat.py\", line 28, in reraise raise value.with_traceback(tb) File \"<unknown>\", line 3, in template jinja2.exceptions.TemplateSyntaxError: unexpected '%'","title":"Hello World - Kotlin"},{"location":"serving/samples/hello-world/helloworld-kotlin/#macro-rendering-error","text":"TemplateSyntaxError : unexpected '%' Traceback (most recent call last): File \"/usr/local/lib/python3.9/site-packages/mkdocs_macros/plugin.py\", line 441, in render md_template = self.env.from_string(markdown) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 941, in from_string return cls.from_code(self, self.compile(source), globals, None) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 638, in compile self.handle_exception(source=source_hint) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 832, in handle_exception reraise(*rewrite_traceback_stack(source=source)) File \"/usr/local/lib/python3.9/site-packages/jinja2/_compat.py\", line 28, in reraise raise value.with_traceback(tb) File \"<unknown>\", line 3, in template jinja2.exceptions.TemplateSyntaxError: unexpected '%'","title":"Macro Rendering Error"},{"location":"serving/samples/hello-world/helloworld-nodejs/","text":"Macro Rendering Error \u00b6 TemplateSyntaxError : unexpected '%' Traceback (most recent call last): File \"/usr/local/lib/python3.9/site-packages/mkdocs_macros/plugin.py\", line 441, in render md_template = self.env.from_string(markdown) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 941, in from_string return cls.from_code(self, self.compile(source), globals, None) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 638, in compile self.handle_exception(source=source_hint) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 832, in handle_exception reraise(*rewrite_traceback_stack(source=source)) File \"/usr/local/lib/python3.9/site-packages/jinja2/_compat.py\", line 28, in reraise raise value.with_traceback(tb) File \"<unknown>\", line 3, in template jinja2.exceptions.TemplateSyntaxError: unexpected '%'","title":"Hello World - Node.js"},{"location":"serving/samples/hello-world/helloworld-nodejs/#macro-rendering-error","text":"TemplateSyntaxError : unexpected '%' Traceback (most recent call last): File \"/usr/local/lib/python3.9/site-packages/mkdocs_macros/plugin.py\", line 441, in render md_template = self.env.from_string(markdown) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 941, in from_string return cls.from_code(self, self.compile(source), globals, None) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 638, in compile self.handle_exception(source=source_hint) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 832, in handle_exception reraise(*rewrite_traceback_stack(source=source)) File \"/usr/local/lib/python3.9/site-packages/jinja2/_compat.py\", line 28, in reraise raise value.with_traceback(tb) File \"<unknown>\", line 3, in template jinja2.exceptions.TemplateSyntaxError: unexpected '%'","title":"Macro Rendering Error"},{"location":"serving/samples/hello-world/helloworld-php/","text":"Macro Rendering Error \u00b6 TemplateSyntaxError : unexpected '<' Traceback (most recent call last): File \"/usr/local/lib/python3.9/site-packages/mkdocs_macros/plugin.py\", line 441, in render md_template = self.env.from_string(markdown) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 941, in from_string return cls.from_code(self, self.compile(source), globals, None) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 638, in compile self.handle_exception(source=source_hint) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 832, in handle_exception reraise(*rewrite_traceback_stack(source=source)) File \"/usr/local/lib/python3.9/site-packages/jinja2/_compat.py\", line 28, in reraise raise value.with_traceback(tb) File \"<unknown>\", line 12, in template jinja2.exceptions.TemplateSyntaxError: unexpected '<'","title":"Hello World - PHP"},{"location":"serving/samples/hello-world/helloworld-php/#macro-rendering-error","text":"TemplateSyntaxError : unexpected '<' Traceback (most recent call last): File \"/usr/local/lib/python3.9/site-packages/mkdocs_macros/plugin.py\", line 441, in render md_template = self.env.from_string(markdown) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 941, in from_string return cls.from_code(self, self.compile(source), globals, None) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 638, in compile self.handle_exception(source=source_hint) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 832, in handle_exception reraise(*rewrite_traceback_stack(source=source)) File \"/usr/local/lib/python3.9/site-packages/jinja2/_compat.py\", line 28, in reraise raise value.with_traceback(tb) File \"<unknown>\", line 12, in template jinja2.exceptions.TemplateSyntaxError: unexpected '<'","title":"Macro Rendering Error"},{"location":"serving/samples/hello-world/helloworld-python/","text":"Macro Rendering Error \u00b6 TemplateSyntaxError : unexpected '<' Traceback (most recent call last): File \"/usr/local/lib/python3.9/site-packages/mkdocs_macros/plugin.py\", line 441, in render md_template = self.env.from_string(markdown) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 941, in from_string return cls.from_code(self, self.compile(source), globals, None) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 638, in compile self.handle_exception(source=source_hint) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 832, in handle_exception reraise(*rewrite_traceback_stack(source=source)) File \"/usr/local/lib/python3.9/site-packages/jinja2/_compat.py\", line 28, in reraise raise value.with_traceback(tb) File \"<unknown>\", line 14, in template jinja2.exceptions.TemplateSyntaxError: unexpected '<'","title":"Hello World - Python"},{"location":"serving/samples/hello-world/helloworld-python/#macro-rendering-error","text":"TemplateSyntaxError : unexpected '<' Traceback (most recent call last): File \"/usr/local/lib/python3.9/site-packages/mkdocs_macros/plugin.py\", line 441, in render md_template = self.env.from_string(markdown) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 941, in from_string return cls.from_code(self, self.compile(source), globals, None) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 638, in compile self.handle_exception(source=source_hint) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 832, in handle_exception reraise(*rewrite_traceback_stack(source=source)) File \"/usr/local/lib/python3.9/site-packages/jinja2/_compat.py\", line 28, in reraise raise value.with_traceback(tb) File \"<unknown>\", line 14, in template jinja2.exceptions.TemplateSyntaxError: unexpected '<'","title":"Macro Rendering Error"},{"location":"serving/samples/hello-world/helloworld-ruby/","text":"Macro Rendering Error \u00b6 TemplateSyntaxError : unexpected '<' Traceback (most recent call last): File \"/usr/local/lib/python3.9/site-packages/mkdocs_macros/plugin.py\", line 441, in render md_template = self.env.from_string(markdown) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 941, in from_string return cls.from_code(self, self.compile(source), globals, None) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 638, in compile self.handle_exception(source=source_hint) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 832, in handle_exception reraise(*rewrite_traceback_stack(source=source)) File \"/usr/local/lib/python3.9/site-packages/jinja2/_compat.py\", line 28, in reraise raise value.with_traceback(tb) File \"<unknown>\", line 12, in template jinja2.exceptions.TemplateSyntaxError: unexpected '<'","title":"Hello World - Ruby"},{"location":"serving/samples/hello-world/helloworld-ruby/#macro-rendering-error","text":"TemplateSyntaxError : unexpected '<' Traceback (most recent call last): File \"/usr/local/lib/python3.9/site-packages/mkdocs_macros/plugin.py\", line 441, in render md_template = self.env.from_string(markdown) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 941, in from_string return cls.from_code(self, self.compile(source), globals, None) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 638, in compile self.handle_exception(source=source_hint) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 832, in handle_exception reraise(*rewrite_traceback_stack(source=source)) File \"/usr/local/lib/python3.9/site-packages/jinja2/_compat.py\", line 28, in reraise raise value.with_traceback(tb) File \"<unknown>\", line 12, in template jinja2.exceptions.TemplateSyntaxError: unexpected '<'","title":"Macro Rendering Error"},{"location":"serving/samples/hello-world/helloworld-scala/","text":"Macro Rendering Error \u00b6 TemplateSyntaxError : unexpected '<' Traceback (most recent call last): File \"/usr/local/lib/python3.9/site-packages/mkdocs_macros/plugin.py\", line 441, in render md_template = self.env.from_string(markdown) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 941, in from_string return cls.from_code(self, self.compile(source), globals, None) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 638, in compile self.handle_exception(source=source_hint) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 832, in handle_exception reraise(*rewrite_traceback_stack(source=source)) File \"/usr/local/lib/python3.9/site-packages/jinja2/_compat.py\", line 28, in reraise raise value.with_traceback(tb) File \"<unknown>\", line 14, in template jinja2.exceptions.TemplateSyntaxError: unexpected '<'","title":"Hello World - Scala using Akka HTTP"},{"location":"serving/samples/hello-world/helloworld-scala/#macro-rendering-error","text":"TemplateSyntaxError : unexpected '<' Traceback (most recent call last): File \"/usr/local/lib/python3.9/site-packages/mkdocs_macros/plugin.py\", line 441, in render md_template = self.env.from_string(markdown) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 941, in from_string return cls.from_code(self, self.compile(source), globals, None) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 638, in compile self.handle_exception(source=source_hint) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 832, in handle_exception reraise(*rewrite_traceback_stack(source=source)) File \"/usr/local/lib/python3.9/site-packages/jinja2/_compat.py\", line 28, in reraise raise value.with_traceback(tb) File \"<unknown>\", line 14, in template jinja2.exceptions.TemplateSyntaxError: unexpected '<'","title":"Macro Rendering Error"},{"location":"serving/samples/hello-world/helloworld-shell/","text":"Macro Rendering Error \u00b6 TemplateSyntaxError : unexpected '<' Traceback (most recent call last): File \"/usr/local/lib/python3.9/site-packages/mkdocs_macros/plugin.py\", line 441, in render md_template = self.env.from_string(markdown) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 941, in from_string return cls.from_code(self, self.compile(source), globals, None) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 638, in compile self.handle_exception(source=source_hint) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 832, in handle_exception reraise(*rewrite_traceback_stack(source=source)) File \"/usr/local/lib/python3.9/site-packages/jinja2/_compat.py\", line 28, in reraise raise value.with_traceback(tb) File \"<unknown>\", line 13, in template jinja2.exceptions.TemplateSyntaxError: unexpected '<'","title":"Hello World - Shell"},{"location":"serving/samples/hello-world/helloworld-shell/#macro-rendering-error","text":"TemplateSyntaxError : unexpected '<' Traceback (most recent call last): File \"/usr/local/lib/python3.9/site-packages/mkdocs_macros/plugin.py\", line 441, in render md_template = self.env.from_string(markdown) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 941, in from_string return cls.from_code(self, self.compile(source), globals, None) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 638, in compile self.handle_exception(source=source_hint) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 832, in handle_exception reraise(*rewrite_traceback_stack(source=source)) File \"/usr/local/lib/python3.9/site-packages/jinja2/_compat.py\", line 28, in reraise raise value.with_traceback(tb) File \"<unknown>\", line 13, in template jinja2.exceptions.TemplateSyntaxError: unexpected '<'","title":"Macro Rendering Error"},{"location":"serving/samples/knative-routing-go/","text":"Macro Rendering Error \u00b6 TemplateSyntaxError : unexpected '%' Traceback (most recent call last): File \"/usr/local/lib/python3.9/site-packages/mkdocs_macros/plugin.py\", line 441, in render md_template = self.env.from_string(markdown) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 941, in from_string return cls.from_code(self, self.compile(source), globals, None) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 638, in compile self.handle_exception(source=source_hint) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 832, in handle_exception reraise(*rewrite_traceback_stack(source=source)) File \"/usr/local/lib/python3.9/site-packages/jinja2/_compat.py\", line 28, in reraise raise value.with_traceback(tb) File \"<unknown>\", line 3, in template jinja2.exceptions.TemplateSyntaxError: unexpected '%'","title":"Routing across multiple Knative services - Go"},{"location":"serving/samples/knative-routing-go/#macro-rendering-error","text":"TemplateSyntaxError : unexpected '%' Traceback (most recent call last): File \"/usr/local/lib/python3.9/site-packages/mkdocs_macros/plugin.py\", line 441, in render md_template = self.env.from_string(markdown) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 941, in from_string return cls.from_code(self, self.compile(source), globals, None) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 638, in compile self.handle_exception(source=source_hint) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 832, in handle_exception reraise(*rewrite_traceback_stack(source=source)) File \"/usr/local/lib/python3.9/site-packages/jinja2/_compat.py\", line 28, in reraise raise value.with_traceback(tb) File \"<unknown>\", line 3, in template jinja2.exceptions.TemplateSyntaxError: unexpected '%'","title":"Macro Rendering Error"},{"location":"serving/samples/multi-container/","text":"Macro Rendering Error \u00b6 TemplateSyntaxError : unexpected '%' Traceback (most recent call last): File \"/usr/local/lib/python3.9/site-packages/mkdocs_macros/plugin.py\", line 441, in render md_template = self.env.from_string(markdown) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 941, in from_string return cls.from_code(self, self.compile(source), globals, None) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 638, in compile self.handle_exception(source=source_hint) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 832, in handle_exception reraise(*rewrite_traceback_stack(source=source)) File \"/usr/local/lib/python3.9/site-packages/jinja2/_compat.py\", line 28, in reraise raise value.with_traceback(tb) File \"<unknown>\", line 3, in template jinja2.exceptions.TemplateSyntaxError: unexpected '%'","title":"Knative multi-container samples"},{"location":"serving/samples/multi-container/#macro-rendering-error","text":"TemplateSyntaxError : unexpected '%' Traceback (most recent call last): File \"/usr/local/lib/python3.9/site-packages/mkdocs_macros/plugin.py\", line 441, in render md_template = self.env.from_string(markdown) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 941, in from_string return cls.from_code(self, self.compile(source), globals, None) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 638, in compile self.handle_exception(source=source_hint) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 832, in handle_exception reraise(*rewrite_traceback_stack(source=source)) File \"/usr/local/lib/python3.9/site-packages/jinja2/_compat.py\", line 28, in reraise raise value.with_traceback(tb) File \"<unknown>\", line 3, in template jinja2.exceptions.TemplateSyntaxError: unexpected '%'","title":"Macro Rendering Error"},{"location":"serving/samples/rest-api-go/","text":"This \"stock ticker\" sample demonstrates how to create and run a simple RESTful service on Knative Serving. The exposed endpoint outputs the stock price for a given \" stock symbol \", like AAPL , AMZN , GOOG , MSFT , etc. Prerequisites \u00b6 A Kubernetes cluster with Knative Serving installed and DNS configured. Docker installed locally. envsubst installed locally. This is installed by the gettext package. If not installed it can be installed by a Linux package manager, or by Homebrew on OS X. Download a copy of the code: git clone -b \"v0.21.x\" https://github.com/knative/docs knative-docs cd knative-docs Setup \u00b6 In order to run an application on Knative Serving a container image must be available to fetch from a container registry. This sample uses Docker for both building and pushing. To build and push to a container registry using Docker: From the knative-docs directory, run the following command to set your container registry endpoint as an environment variable. This sample uses Google Container Registry (GCR) : ```shell export REPO=\"gcr.io/<YOUR_PROJECT_ID>\" ``` Set up your container registry to make sure you are ready to push. To push to GCR, you need to: Create a Google Cloud Platform project . Enable the Google Container Registry API . Setup an auth helper to give the Docker client the permissions it needs to push. If you are using a different container registry, you will want to follow the registry specific instructions for both setup and authorizing the image push. Use Docker to build your application container: docker build \\ --tag \" ${ REPO } /rest-api-go\" \\ --file docs/serving/samples/rest-api-go/Dockerfile . Push your container to a container registry: docker push \" ${ REPO } /rest-api-go\" Substitute the image reference path in the template with our published image path. The command below substitutes using the \\${REPO} variable into a new file called docs/serving/samples/rest-api-go/sample.yaml . envsubst < docs/serving/samples/rest-api-go/sample-template.yaml > \\ docs/serving/samples/rest-api-go/sample.yaml Deploy the Service \u00b6 Now that our image is available from the container registry, we can deploy the Knative Serving sample: kubectl apply --filename docs/serving/samples/rest-api-go/sample.yaml The above command creates a Knative Service within your Kubernetes cluster in the default namespace. Explore the Service \u00b6 The Knative Service creates the following child resources: Knative Route Knative Configuration Knative Revision Kubernetes Deployment Kubernetes Service You can inspect the created resources with the following kubectl commands: View the created Service resource: kubectl get ksvc stock-service-example --output yaml View the created Route resource: kubectl get route -l \\ \"serving.knative.dev/service=stock-service-example\" --output yaml View the Kubernetes Service created by the Route kubectl get service -l \\ \"serving.knative.dev/service=stock-service-example\" --output yaml View the created Configuration resource: kubectl get configuration -l \\ \"serving.knative.dev/service=stock-service-example\" --output yaml View the Revision that was created by our Configuration: kubectl get revision -l \\ \"serving.knative.dev/service=stock-service-example\" --output yaml View the Deployment created by our Revision kubectl get deployment -l \\ \"serving.knative.dev/service=stock-service-example\" --output yaml Access the Service \u00b6 To access this service and run the stock ticker, you first obtain the service URL, and then you run curl commands to send request with your stock symbol. Get the URL of the service: kubectl get ksvc stock-service-example --output = custom-columns = NAME:.metadata.name,URL:.status.url NAME URL stock-service-example http://stock-service-example.default.1.2.3.4.xip.io Send requests to the service using curl : Send a request to the index endpoint: curl http://stock-service-example.default.1.2.3.4.xip.io Response body: Welcome to the stock app! Send a request to the /stock endpoint: curl http://stock-service-example.default.1.2.3.4.xip.io/stock Response body: stock ticker not found!, require /stock/{ticker} Send a request to the /stock endpoint with your \" stock symbol \": curl http://stock-service-example.default.1.2.3.4.xip.io/stock/<SYMBOL> where <SYMBOL> is your \"stock symbol\". Response body: stock price for ticker <SYMBOL> is <PRICE> Example Request: curl http://stock-service-example.default.1.2.3.4.xip.io/stock/FAKE Response: stock price for ticker FAKE is 0.00 Next Steps \u00b6 The traffic splitting example continues from here to walk you through how to create new Revisions and then use traffic splitting between those Revisions. Clean Up \u00b6 To clean up the sample Service: kubectl delete --filename docs/serving/samples/rest-api-go/sample.yaml","title":"Rest API"},{"location":"serving/samples/rest-api-go/#prerequisites","text":"A Kubernetes cluster with Knative Serving installed and DNS configured. Docker installed locally. envsubst installed locally. This is installed by the gettext package. If not installed it can be installed by a Linux package manager, or by Homebrew on OS X. Download a copy of the code: git clone -b \"v0.21.x\" https://github.com/knative/docs knative-docs cd knative-docs","title":"Prerequisites"},{"location":"serving/samples/rest-api-go/#setup","text":"In order to run an application on Knative Serving a container image must be available to fetch from a container registry. This sample uses Docker for both building and pushing. To build and push to a container registry using Docker: From the knative-docs directory, run the following command to set your container registry endpoint as an environment variable. This sample uses Google Container Registry (GCR) : ```shell export REPO=\"gcr.io/<YOUR_PROJECT_ID>\" ``` Set up your container registry to make sure you are ready to push. To push to GCR, you need to: Create a Google Cloud Platform project . Enable the Google Container Registry API . Setup an auth helper to give the Docker client the permissions it needs to push. If you are using a different container registry, you will want to follow the registry specific instructions for both setup and authorizing the image push. Use Docker to build your application container: docker build \\ --tag \" ${ REPO } /rest-api-go\" \\ --file docs/serving/samples/rest-api-go/Dockerfile . Push your container to a container registry: docker push \" ${ REPO } /rest-api-go\" Substitute the image reference path in the template with our published image path. The command below substitutes using the \\${REPO} variable into a new file called docs/serving/samples/rest-api-go/sample.yaml . envsubst < docs/serving/samples/rest-api-go/sample-template.yaml > \\ docs/serving/samples/rest-api-go/sample.yaml","title":"Setup"},{"location":"serving/samples/rest-api-go/#deploy-the-service","text":"Now that our image is available from the container registry, we can deploy the Knative Serving sample: kubectl apply --filename docs/serving/samples/rest-api-go/sample.yaml The above command creates a Knative Service within your Kubernetes cluster in the default namespace.","title":"Deploy the Service"},{"location":"serving/samples/rest-api-go/#explore-the-service","text":"The Knative Service creates the following child resources: Knative Route Knative Configuration Knative Revision Kubernetes Deployment Kubernetes Service You can inspect the created resources with the following kubectl commands: View the created Service resource: kubectl get ksvc stock-service-example --output yaml View the created Route resource: kubectl get route -l \\ \"serving.knative.dev/service=stock-service-example\" --output yaml View the Kubernetes Service created by the Route kubectl get service -l \\ \"serving.knative.dev/service=stock-service-example\" --output yaml View the created Configuration resource: kubectl get configuration -l \\ \"serving.knative.dev/service=stock-service-example\" --output yaml View the Revision that was created by our Configuration: kubectl get revision -l \\ \"serving.knative.dev/service=stock-service-example\" --output yaml View the Deployment created by our Revision kubectl get deployment -l \\ \"serving.knative.dev/service=stock-service-example\" --output yaml","title":"Explore the Service"},{"location":"serving/samples/rest-api-go/#access-the-service","text":"To access this service and run the stock ticker, you first obtain the service URL, and then you run curl commands to send request with your stock symbol. Get the URL of the service: kubectl get ksvc stock-service-example --output = custom-columns = NAME:.metadata.name,URL:.status.url NAME URL stock-service-example http://stock-service-example.default.1.2.3.4.xip.io Send requests to the service using curl : Send a request to the index endpoint: curl http://stock-service-example.default.1.2.3.4.xip.io Response body: Welcome to the stock app! Send a request to the /stock endpoint: curl http://stock-service-example.default.1.2.3.4.xip.io/stock Response body: stock ticker not found!, require /stock/{ticker} Send a request to the /stock endpoint with your \" stock symbol \": curl http://stock-service-example.default.1.2.3.4.xip.io/stock/<SYMBOL> where <SYMBOL> is your \"stock symbol\". Response body: stock price for ticker <SYMBOL> is <PRICE> Example Request: curl http://stock-service-example.default.1.2.3.4.xip.io/stock/FAKE Response: stock price for ticker FAKE is 0.00","title":"Access the Service"},{"location":"serving/samples/rest-api-go/#next-steps","text":"The traffic splitting example continues from here to walk you through how to create new Revisions and then use traffic splitting between those Revisions.","title":"Next Steps"},{"location":"serving/samples/rest-api-go/#clean-up","text":"To clean up the sample Service: kubectl delete --filename docs/serving/samples/rest-api-go/sample.yaml","title":"Clean Up"},{"location":"serving/samples/secrets-go/","text":"Macro Rendering Error \u00b6 TemplateSyntaxError : unexpected '%' Traceback (most recent call last): File \"/usr/local/lib/python3.9/site-packages/mkdocs_macros/plugin.py\", line 441, in render md_template = self.env.from_string(markdown) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 941, in from_string return cls.from_code(self, self.compile(source), globals, None) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 638, in compile self.handle_exception(source=source_hint) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 832, in handle_exception reraise(*rewrite_traceback_stack(source=source)) File \"/usr/local/lib/python3.9/site-packages/jinja2/_compat.py\", line 28, in reraise raise value.with_traceback(tb) File \"<unknown>\", line 3, in template jinja2.exceptions.TemplateSyntaxError: unexpected '%'","title":"Knative Secrets - Go"},{"location":"serving/samples/secrets-go/#macro-rendering-error","text":"TemplateSyntaxError : unexpected '%' Traceback (most recent call last): File \"/usr/local/lib/python3.9/site-packages/mkdocs_macros/plugin.py\", line 441, in render md_template = self.env.from_string(markdown) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 941, in from_string return cls.from_code(self, self.compile(source), globals, None) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 638, in compile self.handle_exception(source=source_hint) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 832, in handle_exception reraise(*rewrite_traceback_stack(source=source)) File \"/usr/local/lib/python3.9/site-packages/jinja2/_compat.py\", line 28, in reraise raise value.with_traceback(tb) File \"<unknown>\", line 3, in template jinja2.exceptions.TemplateSyntaxError: unexpected '%'","title":"Macro Rendering Error"},{"location":"serving/samples/tag-header-based-routing/","text":"Macro Rendering Error \u00b6 TemplateSyntaxError : unexpected '%' Traceback (most recent call last): File \"/usr/local/lib/python3.9/site-packages/mkdocs_macros/plugin.py\", line 441, in render md_template = self.env.from_string(markdown) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 941, in from_string return cls.from_code(self, self.compile(source), globals, None) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 638, in compile self.handle_exception(source=source_hint) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 832, in handle_exception reraise(*rewrite_traceback_stack(source=source)) File \"/usr/local/lib/python3.9/site-packages/jinja2/_compat.py\", line 28, in reraise raise value.with_traceback(tb) File \"<unknown>\", line 3, in template jinja2.exceptions.TemplateSyntaxError: unexpected '%'","title":"Tag Header Based Routing"},{"location":"serving/samples/tag-header-based-routing/#macro-rendering-error","text":"TemplateSyntaxError : unexpected '%' Traceback (most recent call last): File \"/usr/local/lib/python3.9/site-packages/mkdocs_macros/plugin.py\", line 441, in render md_template = self.env.from_string(markdown) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 941, in from_string return cls.from_code(self, self.compile(source), globals, None) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 638, in compile self.handle_exception(source=source_hint) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 832, in handle_exception reraise(*rewrite_traceback_stack(source=source)) File \"/usr/local/lib/python3.9/site-packages/jinja2/_compat.py\", line 28, in reraise raise value.with_traceback(tb) File \"<unknown>\", line 3, in template jinja2.exceptions.TemplateSyntaxError: unexpected '%'","title":"Macro Rendering Error"},{"location":"serving/samples/traffic-splitting/","text":"Macro Rendering Error \u00b6 TemplateSyntaxError : unexpected '%' Traceback (most recent call last): File \"/usr/local/lib/python3.9/site-packages/mkdocs_macros/plugin.py\", line 441, in render md_template = self.env.from_string(markdown) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 941, in from_string return cls.from_code(self, self.compile(source), globals, None) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 638, in compile self.handle_exception(source=source_hint) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 832, in handle_exception reraise(*rewrite_traceback_stack(source=source)) File \"/usr/local/lib/python3.9/site-packages/jinja2/_compat.py\", line 28, in reraise raise value.with_traceback(tb) File \"<unknown>\", line 3, in template jinja2.exceptions.TemplateSyntaxError: unexpected '%'","title":"Simple Traffic Splitting Between Revisions"},{"location":"serving/samples/traffic-splitting/#macro-rendering-error","text":"TemplateSyntaxError : unexpected '%' Traceback (most recent call last): File \"/usr/local/lib/python3.9/site-packages/mkdocs_macros/plugin.py\", line 441, in render md_template = self.env.from_string(markdown) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 941, in from_string return cls.from_code(self, self.compile(source), globals, None) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 638, in compile self.handle_exception(source=source_hint) File \"/usr/local/lib/python3.9/site-packages/jinja2/environment.py\", line 832, in handle_exception reraise(*rewrite_traceback_stack(source=source)) File \"/usr/local/lib/python3.9/site-packages/jinja2/_compat.py\", line 28, in reraise raise value.with_traceback(tb) File \"<unknown>\", line 3, in template jinja2.exceptions.TemplateSyntaxError: unexpected '%'","title":"Macro Rendering Error"},{"location":"serving/services/_index/","text":"","title":"Knative Services"},{"location":"serving/services/creating-services/","text":"Creating Knative Services \u00b6 Knative Services are used to deploy an application. Each Knative Service is defined by a Route and a Configuration, which have the same name as the Service, contained in a YAML file. Every time the Configuration is updated, a new Revision is created. Knative Service Revisions are each backed by a deployment with 2 Kubernetes Services. For more information about Kubernetes Services, see the Kubernetes documentation . Before you begin \u00b6 To create a Knative Service, you will need: A Kubernetes cluster with Knative installed . Custom domains set up for Knative Services. Creating a Knative Service \u00b6 To create an application, you need to create a YAML file that defines a Knative Service. This YAML file specifies metadata about the application, points to the hosted image of the app and allows the Service to be configured. This guide uses the Hello World sample app in Go to demonstrate the structure of a Service YAML file and the basic workflow for deploying an app. These steps can be adapted for your own application if you have an image of it available on Docker Hub, Google Container Registry, or another container image registry. The Hello World sample app does the following: 1. Reads an environment variable TARGET . 2. Prints Hello ${TARGET}! . If TARGET is not defined, it will use World as the TARGET . Procedure \u00b6 Create a new file named service.yaml containing the following information. apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : spec : containers : - image : gcr.io/knative-samples/helloworld-go env : - name : TARGET value : \"Go Sample v1\" apiVersion : The current Knative version. name (metadata): The name of the application. namespace : The namespace that the application will use. image : The URL to the image of the application. name (env): The environment variable printed out by the sample application. Note: If you\u2019re deploying an image of your own app, update the name of the app and the URL of the image accordingly. From the directory where the new service.yaml file was created, deploy the application by applying the service.yaml file. kubectl apply --filename service.yaml Now that your app has been deployed, Knative will perform the following steps: Create a new immutable revision for this version of the app. Perform network programming to create a route, ingress, service, and load balancer for your app. Automatically scale your pods up and down based on traffic, including to zero active pods. Modifying Knative Services \u00b6 Any changes to specifications, metadata labels, or metadata annotations for a Service must be copied to the Route and Configuration owned by that Service. The serving.knative.dev/service label on the Route and Configuration must also be set to the name of the Service. Any additional labels or annotations on the Route and Configuration not specified above must be removed. The Service updates its status fields based on the corresponding status value for the owned Route and Configuration. The Service must include conditions of RoutesReady and ConfigurationsReady in addition to the generic Ready condition. Other conditions can also be present. What's next? \u00b6 For more information about the Knative Service object, see the Resource Types documentation. For more information about getting started with deploying a Knative application, see the Getting Started with App Deployment documentation.","title":"Creating Knative Services"},{"location":"serving/services/creating-services/#creating-knative-services","text":"Knative Services are used to deploy an application. Each Knative Service is defined by a Route and a Configuration, which have the same name as the Service, contained in a YAML file. Every time the Configuration is updated, a new Revision is created. Knative Service Revisions are each backed by a deployment with 2 Kubernetes Services. For more information about Kubernetes Services, see the Kubernetes documentation .","title":"Creating Knative Services"},{"location":"serving/services/creating-services/#before-you-begin","text":"To create a Knative Service, you will need: A Kubernetes cluster with Knative installed . Custom domains set up for Knative Services.","title":"Before you begin"},{"location":"serving/services/creating-services/#creating-a-knative-service","text":"To create an application, you need to create a YAML file that defines a Knative Service. This YAML file specifies metadata about the application, points to the hosted image of the app and allows the Service to be configured. This guide uses the Hello World sample app in Go to demonstrate the structure of a Service YAML file and the basic workflow for deploying an app. These steps can be adapted for your own application if you have an image of it available on Docker Hub, Google Container Registry, or another container image registry. The Hello World sample app does the following: 1. Reads an environment variable TARGET . 2. Prints Hello ${TARGET}! . If TARGET is not defined, it will use World as the TARGET .","title":"Creating a Knative Service"},{"location":"serving/services/creating-services/#procedure","text":"Create a new file named service.yaml containing the following information. apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : spec : containers : - image : gcr.io/knative-samples/helloworld-go env : - name : TARGET value : \"Go Sample v1\" apiVersion : The current Knative version. name (metadata): The name of the application. namespace : The namespace that the application will use. image : The URL to the image of the application. name (env): The environment variable printed out by the sample application. Note: If you\u2019re deploying an image of your own app, update the name of the app and the URL of the image accordingly. From the directory where the new service.yaml file was created, deploy the application by applying the service.yaml file. kubectl apply --filename service.yaml Now that your app has been deployed, Knative will perform the following steps: Create a new immutable revision for this version of the app. Perform network programming to create a route, ingress, service, and load balancer for your app. Automatically scale your pods up and down based on traffic, including to zero active pods.","title":"Procedure"},{"location":"serving/services/creating-services/#modifying-knative-services","text":"Any changes to specifications, metadata labels, or metadata annotations for a Service must be copied to the Route and Configuration owned by that Service. The serving.knative.dev/service label on the Route and Configuration must also be set to the name of the Service. Any additional labels or annotations on the Route and Configuration not specified above must be removed. The Service updates its status fields based on the corresponding status value for the owned Route and Configuration. The Service must include conditions of RoutesReady and ConfigurationsReady in addition to the generic Ready condition. Other conditions can also be present.","title":"Modifying Knative Services"},{"location":"serving/services/creating-services/#whats-next","text":"For more information about the Knative Service object, see the Resource Types documentation. For more information about getting started with deploying a Knative application, see the Getting Started with App Deployment documentation.","title":"What's next?"},{"location":"serving/services/deployment/","text":"Modifying the Deployment Config Map \u00b6 The config-deployment ConfigMap is located in the knative-serving namespace. This ConfigMap, known as the Deployment ConfigMap, contains settings that determine how Kubernetes Deployment resources, that back Knative services, are configured. Accessing the Deployment ConfigMap \u00b6 To view the current Deployment ConfigMap: kubectl get configmap -n knative-serving config-deployment -oyaml Configuring progress deadlines \u00b6 Configuring progress deadline settings allows you to specify the maximum time, either in seconds or minutes, that you will wait for your Deployment to progress before the system reports back that the Deployment has failed progressing for the Knative Revision. By default, this value is set to 120 seconds. The value is expressed as a Go time.Duration string representation, but must be rounded to a second precision. The Knative Autoscaler component scales the revision to 0, and the Knative service enters a terminal Failed state, if the initial scale cannot be achieved within the time limit defined by this setting. You may want to configure this setting as a higher value if any of the following issues occur in your Knative deployment: It takes a long time to pull the Service image, due to the size of the image. It takes a long time for the Service to become READY , due to priming of the initial cache state. The cluster is relies on cluster autoscaling to allocate resources for new pods. See the Kubernetes documentation for more information. The following example shows a snippet of an example Deployment Config Map that sets this value to 10 minutes: apiVersion : v1 kind : ConfigMap metadata : name : config-deployment namespace : knative-serving data : progressDeadline : \"10m\"","title":"Configuring the Knative Deployment"},{"location":"serving/services/deployment/#modifying-the-deployment-config-map","text":"The config-deployment ConfigMap is located in the knative-serving namespace. This ConfigMap, known as the Deployment ConfigMap, contains settings that determine how Kubernetes Deployment resources, that back Knative services, are configured.","title":"Modifying the Deployment Config Map"},{"location":"serving/services/deployment/#accessing-the-deployment-configmap","text":"To view the current Deployment ConfigMap: kubectl get configmap -n knative-serving config-deployment -oyaml","title":"Accessing the Deployment ConfigMap"},{"location":"serving/services/deployment/#configuring-progress-deadlines","text":"Configuring progress deadline settings allows you to specify the maximum time, either in seconds or minutes, that you will wait for your Deployment to progress before the system reports back that the Deployment has failed progressing for the Knative Revision. By default, this value is set to 120 seconds. The value is expressed as a Go time.Duration string representation, but must be rounded to a second precision. The Knative Autoscaler component scales the revision to 0, and the Knative service enters a terminal Failed state, if the initial scale cannot be achieved within the time limit defined by this setting. You may want to configure this setting as a higher value if any of the following issues occur in your Knative deployment: It takes a long time to pull the Service image, due to the size of the image. It takes a long time for the Service to become READY , due to priming of the initial cache state. The cluster is relies on cluster autoscaling to allocate resources for new pods. See the Kubernetes documentation for more information. The following example shows a snippet of an example Deployment Config Map that sets this value to 10 minutes: apiVersion : v1 kind : ConfigMap metadata : name : config-deployment namespace : knative-serving data : progressDeadline : \"10m\"","title":"Configuring progress deadlines"},{"location":"serving/spec/knative-api-specification-1.0/","text":"Knative Serving API Specification \u00b6 This file has been moved to the Knative Specs Repository","title":"Knative Serving API Specification"},{"location":"serving/spec/knative-api-specification-1.0/#knative-serving-api-specification","text":"This file has been moved to the Knative Specs Repository","title":"Knative Serving API Specification"}]}